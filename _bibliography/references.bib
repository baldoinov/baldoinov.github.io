---
---
References
==========

@book{__,
  type = {book}
}

@book{333PaginasPara_2016_Gomes,
  title = {333 Páginas para Tirar seu Projeto do Papel},
  author = {Gomes, Gabriel},
  namea = {Larusso, Daniel},
  nameatype = {collaborator},
  date = {2016-08-29},
  publisher = {Não definido},
  isbn = {978-85-920301-1-7},
  langid = {brazilian},
  keywords = {Ciências sociais}
}

@article{50YearsData_2017_Donoho,
  title = {50 {{Years}} of {{Data Science}}},
  author = {Donoho, David},
  date = {2017-10-02},
  journaltitle = {Journal of Computational and Graphical Statistics},
  volume = {26},
  number = {4},
  pages = {745--766},
  publisher = {Taylor \& Francis},
  issn = {1061-8600},
  doi = {10.1080/10618600.2017.1384734},
  url = {https://doi.org/10.1080/10618600.2017.1384734},
  urldate = {2023-04-13},
  abstract = {More than 50 years ago, John Tukey called for a reformation of academic statistics. In “The Future of Data Analysis,” he pointed to the existence of an as-yet unrecognized science, whose subject of interest was learning from data, or “data analysis.” Ten to 20 years ago, John Chambers, Jeff Wu, Bill Cleveland, and Leo Breiman independently once again urged academic statistics to expand its boundaries beyond the classical domain of theoretical statistics; Chambers called for more emphasis on data preparation and presentation rather than statistical modeling; and Breiman called for emphasis on prediction rather than inference. Cleveland and Wu even suggested the catchy name “data science” for this envisioned field. A recent and growing phenomenon has been the emergence of “data science” programs at major universities, including UC Berkeley, NYU, MIT, and most prominently, the University of Michigan, which in September 2015 announced a \$100M “Data Science Initiative” that aims to hire 35 new faculty. Teaching in these new programs has significant overlap in curricular subject matter with traditional statistics courses; yet many academic statisticians perceive the new programs as “cultural appropriation.” This article reviews some ingredients of the current “data science moment,” including recent commentary about data science in the popular media, and about how/whether data science is really different from statistics. The now-contemplated field of data science amounts to a superset of the fields of statistics and machine learning, which adds some technology for “scaling up” to “big data.” This chosen superset is motivated by commercial rather than intellectual developments. Choosing in this way is likely to miss out on the really important intellectual event of the next 50 years. Because all of science itself will soon become data that can be mined, the imminent revolution in data science is not about mere “scaling up,” but instead the emergence of scientific studies of data analysis science-wide. In the future, we will be able to predict how a proposal to change data analysis workflows would impact the validity of data analysis across all of science, even predicting the impacts field-by-field. Drawing on work by Tukey, Cleveland, Chambers, and Breiman, I present a vision of data science based on the activities of people who are “learning from data,” and I describe an academic field dedicated to improving that activity in an evidence-based manner. This new field is a better academic enlargement of statistics and machine learning than today’s data science initiatives, while being able to accommodate the same short-term goals. Based on a presentation at the Tukey Centennial Workshop, Princeton, NJ, September 18, 2015.},
  keywords = {Correction,Cross-study analysis,Data analysis,Data science,Meta analysis,notion,Predictive modeling,Quantitative programming environments,Statistics},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/50 Years of Data Science_2017_Donoho.pdf}
}

@online{AbsoluteZeroReinforced_2025_ZhaoEtAl,
  title = {Absolute {{Zero}}: {{Reinforced Self-play Reasoning}} with {{Zero Data}}},
  shorttitle = {Absolute {{Zero}}},
  author = {Zhao, Andrew and Wu, Yiran and Yue, Yang and Wu, Tong and Xu, Quentin and Yue, Yang and Lin, Matthieu and Wang, Shenzhi and Wu, Qingyun and Zheng, Zilong and Huang, Gao},
  date = {2025-05-07},
  eprint = {2505.03335},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2505.03335},
  url = {http://arxiv.org/abs/2505.03335},
  urldate = {2025-05-09},
  abstract = {Reinforcement learning with verifiable rewards (RLVR) has shown promise in enhancing the reasoning capabilities of large language models by learning directly from outcome-based rewards. Recent RLVR works that operate under the zero setting avoid supervision in labeling the reasoning process, but still depend on manually curated collections of questions and answers for training. The scarcity of high-quality, human-produced examples raises concerns about the long-term scalability of relying on human supervision, a challenge already evident in the domain of language model pretraining. Furthermore, in a hypothetical future where AI surpasses human intelligence, tasks provided by humans may offer limited learning potential for a superintelligent system. To address these concerns, we propose a new RLVR paradigm called Absolute Zero, in which a single model learns to propose tasks that maximize its own learning progress and improves reasoning by solving them, without relying on any external data. Under this paradigm, we introduce the Absolute Zero Reasoner (AZR), a system that self-evolves its training curriculum and reasoning ability by using a code executor to both validate proposed code reasoning tasks and verify answers, serving as an unified source of verifiable reward to guide open-ended yet grounded learning. Despite being trained entirely without external data, AZR achieves overall SOTA performance on coding and mathematical reasoning tasks, outperforming existing zero-setting models that rely on tens of thousands of in-domain human-curated examples. Furthermore, we demonstrate that AZR can be effectively applied across different model scales and is compatible with various model classes.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Absolute Zero_2025_Zhao et al.pdf;/home/baldoinov/Zotero/storage/NQAI2PJK/2505.html}
}

@inproceedings{AccuracyFScoreROC_2006_SokolovaEtAl,
  title = {Beyond {{Accuracy}}, {{F-Score}} and {{ROC}}: {{A Family}} of {{Discriminant Measures}} for {{Performance Evaluation}}},
  shorttitle = {Beyond {{Accuracy}}, {{F-Score}} and {{ROC}}},
  author = {Sokolova, Marina and Japkowicz, Nathalie and Szpakowicz, Stan},
  date = {2006-01-01},
  volume = {Vol. 4304},
  pages = {1015--1021},
  doi = {10.1007/11941439_114},
  abstract = {Different evaluation measures assess different characteristics of machine learning algorithms. The empirical evaluation of algorithms and classifiers is a matter of on-going debate among researchers. Most measures in use today focus on a classifier’s ability to identify classes correctly. We note other useful properties, such as failure avoidance or class discrimination, and we suggest measures to evaluate such properties. These measures – Youden’s index, likelihood, Discriminant power – are used in medical diagnosis. We show that they are interrelated, and we apply them to a case study from the field of electronic negotiations. We also list other learning problems which may benefit from the application of these measures.},
  eventtitle = {{{AI}} 2006: {{Advances}} in {{Artificial Intelligence}}, {{Lecture Notes}} in {{Computer Science}}},
  isbn = {978-3-540-49787-5},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Beyond Accuracy, F-Score and ROC_2006_Sokolova et al.pdf}
}

@misc{ACEAutomaticContent_2008_LinguisticDataConsortium,
  title = {{{ACE}} ({{Automatic Content Extraction}}) {{English Annotation Guidelines}} for {{Entities}} Version 6.6},
  author = {Linguistic Data Consortium},
  date = {2008-06-13},
  url = {https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/english-entities-guidelines-v6.6.pdf},
  urldate = {2024-03-18},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/projeto-heiwa/ACE (Automatic Content Extraction) English Annotation Guidelines for Entities_2008_Linguistic Data Consortium.pdf}
}

@incollection{Acknowledgments_2021_Kissell,
  title = {Acknowledgments},
  booktitle = {Algorithmic {{Trading Methods}} ({{Second Edition}})},
  editor = {Kissell, Robert L.},
  date = {2021-01-01},
  pages = {xxi-xxiv},
  publisher = {Academic Press},
  doi = {10.1016/B978-0-12-815630-8.04001-1},
  url = {https://www.sciencedirect.com/science/article/pii/B9780128156308040011},
  urldate = {2025-04-21},
  isbn = {978-0-12-815630-8}
}

@online{AdamMethodStochastic_2017_KingmaBa,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  date = {2017-01-29},
  eprint = {1412.6980},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1412.6980},
  url = {http://arxiv.org/abs/1412.6980},
  urldate = {2023-11-16},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,notion},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/projeto-heiwa/Adam_2017_Kingma et al.pdf;/home/baldoinov/Zotero/storage/43LT2KPJ/1412.html}
}

@article{AdaptiveIncentiveDesign_2022_Yang,
  title = {Adaptive {{Incentive Design}} with {{Multi-Agent Meta-Gradient Reinforcement Learning}}},
  author = {Yang, Jiachen},
  date = {2022},
  abstract = {Critical sectors of human society are progressing toward the adoption of powerful artificial intelligence (AI) agents, which are trained individually on behalf of self-interested principals but deployed in a shared environment. Short of direct centralized regulation of AI, which is as difficult an issue as regulation of human actions, one must design institutional mechanisms that indirectly guide agents’ behaviors to safeguard and improve social welfare in the shared environment. Our paper focuses on one important class of such mechanisms: the problem of adaptive incentive design, whereby a central planner intervenes on the payoffs of an agent population via incentives in order to optimize a system objective. To tackle this problem in high-dimensional environments whose dynamics may be unknown or too complex to model, we propose a model-free meta-gradient method to learn an adaptive incentive function in the context of multi-agent reinforcement learning. Via the principle of online cross-validation, the incentive designer explicitly accounts for its impact on agents’ learning and, through them, the impact on future social welfare. Experiments on didactic benchmark problems show that the proposed method can induce selfish agents to learn near-optimal cooperative behavior and significantly outperform learning-oblivious baselines. When applied to a complex simulated economy, the proposed method finds tax policies that achieve better trade-off between economic productivity and equality than baselines, a result that we interpret via a detailed behavioral analysis.},
  langid = {english},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Adaptive Incentive Design with Multi-Agent Meta-Gradient Reinforcement Learning_2022_Yang.pdf}
}

@article{AdvancesAgentbasedModeling_2021_SteinbacherEtAl,
  title = {Advances in the Agent-Based Modeling of Economic and Social Behavior},
  author = {Steinbacher, Mitja and Raddant, Matthias and Karimi, Fariba and Camacho Cuena, Eva and Alfarano, Simone and Iori, Giulia and Lux, Thomas},
  date = {2021-07-07},
  journaltitle = {SN Business \& Economics},
  shortjournal = {SN Bus Econ},
  volume = {1},
  number = {7},
  pages = {99},
  issn = {2662-9399},
  doi = {10.1007/s43546-021-00103-3},
  url = {https://link.springer.com/10.1007/s43546-021-00103-3},
  urldate = {2024-05-14},
  abstract = {Abstract             In this review we discuss advances in the agent-based modeling of economic and social systems. We show the state of the art of the heuristic design of agents and how behavioral economics and laboratory experiments have improved the modeling of agent behavior. We further discuss how economic networks and social systems can be modeled and we discuss novel methodology and data sources. Lastly,~we present an overview of estimation techniques to calibrate and validate agent-based models and show avenues for future research.},
  langid = {english},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Advances in the agent-based modeling of economic and social behavior_2021_Steinbacher et al.pdf}
}

@inproceedings{AdvancingArtSimulation_1997_Axelrod,
  title = {Advancing the {{Art}} of {{Simulation}} in the {{Social Sciences}}},
  booktitle = {Simulating {{Social Phenomena}}},
  author = {Axelrod, Robert},
  editor = {Conte, Rosaria and Hegselmann, Rainer and Terna, Pietro},
  date = {1997},
  pages = {21--40},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-662-03366-1_2},
  abstract = {Advancing the state of the art of simulation in the social sciences requires appreciating the unique value of simulation as a third way of doing science, in contrast to both induction and deduction. This essay offers advice for doing simulation research, focusing on the programming of a simulation model, analyzing the results and sharing the results with others. Replicating other people’s simulations gets special emphasis, with examples of the procedures and difficulties involved in the process of replication. Finally, suggestions are offered for building of a community of social scientists who do simulation.},
  isbn = {978-3-662-03366-1},
  langid = {english},
  keywords = {Artificial Society,Organizational Code,Rational Choice,Simulation Research,Social Science Citation Index},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Advancing the Art of Simulation in the Social Sciences_1997_Axelrod.pdf}
}

@inproceedings{AdvancingNeuralEncoding_2023_RodriguesEtAl,
  title = {Advancing {{Neural Encoding}} of {{Portuguese}} with {{Transformer Albertina PT-}}*},
  booktitle = {Progress in {{Artificial Intelligence}}},
  author = {Rodrigues, João and Gomes, Luís and Silva, João and Branco, António and Santos, Rodrigo and Cardoso, Henrique Lopes and Osório, Tomás},
  editor = {Moniz, Nuno and Vale, Zita and Cascalho, José and Silva, Catarina and Sebastião, Raquel},
  date = {2023},
  pages = {441--453},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-49008-8_35},
  abstract = {To advance the neural encoding of Portuguese (PT), and a fortiori the technological preparation of this language for the digital age, we developed a Transformer-based foundation model that sets a new state of the art in this respect for two of its variants, namely European Portuguese from Portugal (PT-PT) and American Portuguese from Brazil (PT-BR). To develop this encoder, which we named Albertina PT-*, a strong model was used as a starting point, DeBERTa, and its pre-training was done over data sets of Portuguese, namely over a data set we gathered for PT-PT and over the brWaC corpus for PT-BR. The performance of Albertina and competing models was assessed by evaluating them on prominent downstream language processing tasks adapted for Portuguese. Both Albertina versions are distributed free of charge and under a most permissive license possible and can be run on consumer-grade hardware, thus seeking to contribute to the advancement of research and innovation in language technology for Portuguese.},
  isbn = {978-3-031-49008-8},
  langid = {english},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/projeto-heiwa/Advancing Neural Encoding of Portuguese with Transformer Albertina PT-_2023_Rodrigues et al.pdf}
}

@online{AFaCTAAssistingAnnotation_2024_NiEtAl,
  title = {{{AFaCTA}}: {{Assisting}} the {{Annotation}} of {{Factual Claim Detection}} with {{Reliable LLM Annotators}}},
  shorttitle = {{{AFaCTA}}},
  author = {Ni, Jingwei and Shi, Minjing and Stammbach, Dominik and Sachan, Mrinmaya and Ash, Elliott and Leippold, Markus},
  date = {2024-02-16},
  eprint = {2402.11073},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2402.11073},
  url = {http://arxiv.org/abs/2402.11073},
  urldate = {2024-03-01},
  abstract = {With the rise of generative AI, automated fact-checking methods to combat misinformation are becoming more and more important. However, factual claim detection, the first step in a fact-checking pipeline, suffers from two key issues that limit its scalability and generalizability: (1) inconsistency in definitions of the task and what a claim is, and (2) the high cost of manual annotation. To address (1), we review the definitions in related work and propose a unifying definition of factual claims that focuses on verifiability. To address (2), we introduce AFaCTA (Automatic Factual Claim deTection Annotator), a novel framework that assists in the annotation of factual claims with the help of large language models (LLMs). AFaCTA calibrates its annotation confidence with consistency along three predefined reasoning paths. Extensive evaluation and experiments in the domain of political speech reveal that AFaCTA can efficiently assist experts in annotating factual claims and training high-quality classifiers, and can work with or without expert supervision. Our analyses also result in PoliClaim, a comprehensive claim detection dataset spanning diverse political topics.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,notion},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/projeto-heiwa/AFaCTA_2024_Ni et al.pdf;/home/baldoinov/Zotero/storage/ZK28BXAY/2402.html}
}

@misc{Agenda2023,
  type = {General Assembly},
  title = {Transforming Our World: The 2030 Agenda for Sustainable Development},
  author = {United Nations, Department of Economic and Development, Social Affairs - Sustainable},
  date = {2015},
  pages = {16301},
  url = {https://sdgs.un.org/2030agenda},
  isbn = {A/RES/70/1},
  file = {/home/baldoinov/baldoinov/PDFs/Mackenzie/projeto-aplicado-iii/Transforming our world_2015_United Nations et al.pdf}
}

@article{AgentbasedComputationalEconomics_2003_Tesfatsion,
  title = {Agent-Based Computational Economics: Modeling Economies as Complex Adaptive Systems},
  shorttitle = {Agent-Based Computational Economics},
  author = {Tesfatsion, Leigh},
  date = {2003-02-01},
  journaltitle = {Information Sciences},
  shortjournal = {Information Sciences},
  volume = {149},
  number = {4},
  pages = {262--268},
  issn = {0020-0255},
  doi = {10.1016/S0020-0255(02)00280-3},
  url = {https://www.sciencedirect.com/science/article/pii/S0020025502002803},
  urldate = {2024-06-12},
  abstract = {Agent-based computational economics (ACE) is the computational study of economies modeled as evolving systems of autonomous interacting agents. Thus, ACE is a specialization to economics of the basic complex adaptive systems paradigm. This paper outlines the main objectives and defining characteristics of the ACE methodology, and discusses several active research areas.},
  keywords = {Agent-based computational economics,Complex adaptive systems},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Agent-based computational economics_2003_Tesfatsion.pdf}
}

@incollection{AgentBasedComputationalEconomics_2006_Tesfatsion,
  title = {Agent-{{Based Computational Economics}}: {{A Constructive Approach}} to {{Economic Theory}}},
  shorttitle = {Chapter 16 {{Agent-Based Computational Economics}}},
  booktitle = {Handbook of {{Computational Economics}}},
  author = {Tesfatsion, Leigh},
  editor = {Tesfatsion, L. and Judd, K. L.},
  date = {2006-01-01},
  volume = {2},
  pages = {831--880},
  publisher = {Elsevier},
  doi = {10.1016/S1574-0021(05)02016-2},
  url = {https://www.sciencedirect.com/science/article/pii/S1574002105020162},
  urldate = {2023-09-27},
  abstract = {Economies are complicated systems encompassing micro behaviors, interaction patterns, and global regularities. Whether partial or general in scope, studies of economic systems must consider how to handle difficult real-world aspects such as asymmetric information, imperfect competition, strategic interaction, collective learning, and the possibility of multiple equilibria. Recent advances in analytical and computational tools are permitting new approaches to the quantitative study of these aspects. One such approach is Agent-based Computational Economics (ACE), the computational study of economic processes modeled as dynamic systems of interacting agents. This chapter explores the potential advantages and disadvantages of ACE for the study of economic systems. General points are concretely illustrated using an ACE model of a two-sector decentralized market economy. Six issues are highlighted: Constructive understanding of production, pricing, and trade processes; the essential primacy of survival; strategic rivalry and market power; behavioral uncertainty and learning; the role of conventions and organizations; and the complex interactions among structural attributes, institutional arrangements, and behavioral dispositions.},
  keywords = {agent-based computational economics,agent-oriented programming,behavioral uncertainty,complex adaptive systems,decentralized market processes,endogenous interactions,institutions,learning,notion,strategic rivalry},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Agent-Based Computational Economics_2006_Tesfatsion.pdf}
}

@article{AgentBasedComputationalEconomics_2022_Tesfatsion,
  title = {Agent-{{Based Computational Economics}}: {{Overview}} and {{Brief History}}},
  shorttitle = {Agent-{{Based Computational Economics}}},
  author = {Tesfatsion, Leigh},
  date = {2022-05-12},
  url = {https://dr.lib.iastate.edu/handle/20.500.12876/104713},
  urldate = {2023-09-26},
  abstract = {Scientists and engineers seek to understand how real-world systems work and could work better. Any modeling method devised for such purposes must simplify reality. Ideally, however, the modeling method should be flexible as well as logically rigorous; it should permit model simplifications to be appropriately tailored for the specific purpose at hand. Flexibility and logical rigor have been the two key goals motivating the development of Agent-based Computational Economics (ACE), a completely agent-based modeling method characterized by seven specific modeling principles. This perspective provides an overview of ACE, a brief history of its development, and its role within a broader spectrum of experiment-based modeling methods.},
  langid = {english},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Agent-Based Computational Economics_2022_Tesfatsion.pdf}
}

@article{AgentbasedInputOutput_2010_OlivaEtAl,
  title = {Agent-Based Input–Output Interdependency Model},
  author = {Oliva, Gabriele and Panzieri, Stefano and Setola, Roberto},
  date = {2010-07-01},
  journaltitle = {International Journal of Critical Infrastructure Protection},
  shortjournal = {International Journal of Critical Infrastructure Protection},
  volume = {3},
  number = {2},
  pages = {76--82},
  issn = {1874-5482},
  doi = {10.1016/j.ijcip.2010.05.001},
  url = {https://www.sciencedirect.com/science/article/pii/S187454821000020X},
  urldate = {2023-09-19},
  abstract = {The modeling and analysis of critical infrastructures and their interdependencies are essential to discovering hidden vulnerabilities and the related threats to national and international security. Over the past few years, several approaches have been proposed to address this problem. The so-called holistic approaches are relatively abstract, but are easily validated using real economic data. Other approaches based on agent-based models provide deeper views of the interdependencies existing between subsystems of different infrastructures. However, agent-based models are often difficult to validate because quantitative data of the appropriate granularity may not be available. This paper presents an agent-based input–output inoperability model designed to overcome the limitations of the holistic and agent-based paradigms. In order to provide a detailed and expressive framework, the exchange of resources between infrastructures is explicitly modeled while inoperability becomes an internal parameter. Nevertheless, the model is easily transformed into a fine-grained, input–output inoperability model whose coefficients can be obtained based on real data.},
  keywords = {Agent-based model,Complex systems,Input–output inoperability model,notion},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Agent-based input–output interdependency model_2010_Oliva et al.pdf;/home/baldoinov/Zotero/storage/JREH6842/S187454821000020X.html}
}

@article{AgentBasedModel_2013_GarridoMittone,
  title = {An Agent Based Model for Studying Optimal Tax Collection Policy Using Experimental Data: {{The}} Cases of {{Chile}} and {{Italy}}},
  shorttitle = {An Agent Based Model for Studying Optimal Tax Collection Policy Using Experimental Data},
  author = {Garrido, Nicolás and Mittone, Luigi},
  date = {2013-02},
  journaltitle = {The Journal of Socio-Economics},
  shortjournal = {The Journal of Socio-Economics},
  volume = {42},
  pages = {24--30},
  issn = {10535357},
  doi = {10.1016/j.socec.2012.11.002},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S105353571200114X},
  urldate = {2024-06-11},
  abstract = {This paper investigates optimal audit programs in an economy populated by artificial agents. The behavior of the artificial agents is calibrated using data obtained from experiments on fiscal evasion made in northern Chile (Antofagasta) and northern Italy (Trento). We identify a tax collection policy that is optimal in the sense that its outperforms the tax payments made by the calibrated agents, using any other standard collection plans used by governments. We find that the design of an optimal audit scheme depends on three components: income distribution, the identification of patterns of behaviors and the number of times individuals are audited.},
  langid = {english},
  keywords = {Computational agent based economics,Experimental economics,notion,Optimal tax collection},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/An agent based model for studying optimal tax collection policy using_2013_Garrido et al.pdf;/home/baldoinov/baldoinov/PDFs/Feausp/monografia/An agent based model for studying optimal tax collection policy using_2013_Garrido et al2.pdf;/home/baldoinov/Zotero/storage/6EPW32H3/S105353571200114X.html}
}

@incollection{AgentbasedModelingBridge_2006_Axelrod,
  title = {Agent-Based {{Modeling}} as a {{Bridge Between Disciplines}}},
  booktitle = {Handbook of {{Computational Economics}}},
  author = {Axelrod, Robert},
  editor = {Tesfatsion, L. and Judd, K. L.},
  date = {2006-01-01},
  volume = {2},
  pages = {1565--1584},
  publisher = {Elsevier},
  doi = {10.1016/S1574-0021(05)02033-2},
  url = {https://www.sciencedirect.com/science/article/pii/S1574002105020332},
  urldate = {2023-09-27},
  abstract = {Using the author's own experiences, this chapter shows how agent-based modeling (ABM) can address research questions common to many disciplines, facilitate interdisciplinary collaboration, provide a useful multidisciplinary tool when the math is intractable, and reveal unity across disciplines. While ABM can be a hard sell, convergence within the agent-based community can enhance the interdisciplinary value of the methodology.},
  keywords = {agent-based models,evolutionary biology,interdisciplinary research,notion,prisoner's dilemma},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Agent-based Modeling as a Bridge Between Disciplines_2006_Axelrod.pdf}
}

@article{AgentBasedModelingEconomics__AxtellFarmer,
  title = {Agent-{{Based Modeling}} in {{Economics}} and {{Finance}}: {{Past}}, {{Present}}, and {{Future}}},
  shorttitle = {Agent-{{Based Modeling}} in {{Economics}} and {{Finance}}},
  author = {Axtell, Robert L. and Farmer, J. Doyne},
  journaltitle = {Journal of Economic Literature},
  issn = {0022-0515},
  doi = {10.1257/jel.20221319},
  url = {https://www.aeaweb.org/articles?id=10.1257/jel.20221319&from=f},
  urldate = {2023-12-05},
  langid = {english},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Agent-Based Modeling in Economics and Finance_Axtell et al.pdf}
}

@article{AgentbasedModelingMethods_2002_Bonabeau,
  title = {Agent-Based Modeling: {{Methods}} and Techniques for Simulating Human Systems},
  shorttitle = {Agent-Based Modeling},
  author = {Bonabeau, Eric},
  date = {2002-05-14},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {99},
  pages = {7280--7287},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.082080899},
  url = {https://www.pnas.org/doi/10.1073/pnas.082080899},
  urldate = {2023-08-19},
  abstract = {Agent-based modeling is a powerful simulation modeling technique that has seen a number of applications in the last few years, including applications to real-world business problems. After the basic principles of agent-based simulation are briefly introduced, its four areas of application are discussed by using real-world applications: flow simulation, organizational simulation, market simulation, and diffusion simulation. For each category, one or several business applications are described and analyzed.},
  issue = {suppl\_3},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Agent-based modeling_2002_Bonabeau.pdf}
}

@incollection{AgentBasedModelingRight_2011_BorrillTesfatsion,
  title = {Agent-{{Based Modeling}}: {{The Right Mathematics}} for the {{Social Sciences}}?},
  shorttitle = {Agent-{{Based Modeling}}},
  booktitle = {Elgar {{Companion}} to {{Recent Economic Methodology}}},
  author = {Borrill, Paul L. and Tesfatsion, Leigh},
  editor = {Davis, J. B. and Hands, D. W.},
  date = {2011},
  pages = {228},
  publisher = {Edward Elgar Publishers},
  url = {https://philarchive.org/rec/BORAM-3},
  urldate = {2024-06-17},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Agent-Based Modeling_2011_Borrill et al.pdf}
}

@book{AgentBasedModellingEconomics_2016_HamillGilbert,
  title = {Agent-{{Based Modelling}} in {{Economics}}},
  author = {Hamill, Lynne and Gilbert, Nigel},
  date = {2016-01-19},
  edition = {1st edition},
  publisher = {Wiley},
  location = {Chichester, UK ; Hoboken, NJ},
  abstract = {Agent-based modelling in economics~Lynne Hamill and Nigel Gilbert, Centre for Research in Social Simulation (CRESS), University of Surrey, UK~New methods of economic modelling have been sought as a result of the global economic downturn in 2008.This unique book highlights the benefits of an agent-based modelling (ABM) approach. It demonstrates how ABM can easily handle complexity: heterogeneous people, households and firms interacting dynamically. Unlike traditional methods, ABM does not require people or firms to optimise or economic systems to reach equilibrium. ABM offers a way to link micro foundations directly to the macro situation.~~Key features:Introduces the concept of agent-based modelling and shows how it differs from existing approaches.Provides a theoretical and methodological rationale for using ABM in economics, along with practical advice on how to design and create the models.Each chapter starts with a short summary of the relevant economic theory and then shows how to apply ABM.Explores both topics covered in basic economics textbooks and current important policy themes; unemployment, exchange rates, banking and environmental issues.Describes the models in pseudocode, enabling the reader to develop programs in their chosen language.Supported by a website featuring the NetLogo models described in the book.~Agent-based Modelling in Economics provides students and researchers with the skills to design, implement, and analyze agent-based models. Third year undergraduate, master and doctoral students, faculty and professional economists will find this book an invaluable resource.},
  isbn = {978-1-118-45607-1},
  langid = {english},
  pagetotal = {256},
  keywords = {notion}
}

@article{AgentBasedModelsAssisted_2023_Platas-LopezEtAl,
  title = {Agent-{{Based Models Assisted}} by {{Supervised Learning}}: {{A Proposal}} for {{Model Specification}}},
  shorttitle = {Agent-{{Based Models Assisted}} by {{Supervised Learning}}},
  author = {Platas-López, Alejandro and Guerra-Hernández, Alejandro and Quiroz-Castellanos, Marcela and Cruz-Ramírez, Nicandro},
  date = {2023-01},
  journaltitle = {Electronics},
  volume = {12},
  number = {3},
  pages = {495},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2079-9292},
  doi = {10.3390/electronics12030495},
  url = {https://www.mdpi.com/2079-9292/12/3/495},
  urldate = {2023-08-19},
  abstract = {Agent-based modeling (ABM) has become popular since it allows a direct representation of heterogeneous individual entities, their decisions, and their interactions, in a given space. With the increase in the amount of data in different domains, an opportunity to support the design, implementation, and analysis of these models, using Machine Learning techniques, has emerged. A vast and diverse literature evidences the interest and benefits of this symbiosis, but also exhibits the inadequacy of current specification standards, such as the Overview, Design concepts and Details (ODD) protocol, to cover such diversity and, in consequence, its lack of use. Given the relevance of standard specifications for the sake of reproducible ABMs, this paper proposes an extension to the ODD Protocol to provide a standardized description of the uses of Machine Learning (ML) in supporting agent-based modeling. The extension is based on categorization, a result of a broad, but integrated, review of the literature, considering the purpose of learning, the moment when the learning process is executed, the components of the model affected by learning, and the algorithms and data used in learning. The proposed extension of the ODD protocol allows orderly and transparent communication of ML workflows in ABM, facilitating its understanding and potential replication in other investigations. The presentation of a full-featured agent-based model of tax evasion illustrates the application of the proposed approach where the adoption of machine learning results in an error statistically significantly lower, with a p-value of 0.02 in the Wilcoxon signed-rank test. Furthermore, our analysis provides numerical estimates that reveal the strong impact of the penalty and tax rate on tax evasion. Future work considers other kinds of learning applications, e.g., the calibration of parameters and the analysis of the ABM results.},
  issue = {3},
  langid = {english},
  keywords = {agent-based modeling and simulation,machine learning,notion,ODD protocol,tax evasion},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Agent-Based Models Assisted by Supervised Learning_2023_Platas-Lopez et al.pdf}
}

@incollection{AgentLearningRepresentation_2006_Brenner,
  title = {Agent {{Learning Representation}}: {{Advice}} on {{Modelling Economic Learning}}},
  shorttitle = {Chapter 18 {{Agent Learning Representation}}},
  booktitle = {Handbook of {{Computational Economics}}},
  author = {Brenner, Thomas},
  editor = {Tesfatsion, L. and Judd, K. L.},
  date = {2006-01-01},
  volume = {2},
  pages = {895--947},
  publisher = {Elsevier},
  doi = {10.1016/S1574-0021(05)02018-6},
  url = {https://www.sciencedirect.com/science/article/pii/S1574002105020186},
  urldate = {2023-09-13},
  abstract = {This chapter presents an overview of the existing learning models in the economic literature. Furthermore, it discusses the choice of models that should be used under various circumstances and how adequate learning models can be chosen in simulation approaches. It gives advice for using the many existing models and selecting the appropriate model for each application.},
  keywords = {economic learning,modelling,notion},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Agent Learning Representation_2006_Brenner.pdf;/home/baldoinov/Zotero/storage/GUT3EDAQ/S1574002105020186.html}
}

@article{AIAgentsState_2002_Alonso,
  title = {{{AI}} and {{Agents}}: {{State}} of the {{Art}}},
  shorttitle = {{{AI}} and {{Agents}}},
  author = {Alonso, Eduardo},
  date = {2002-09-15},
  journaltitle = {AI Magazine},
  volume = {23},
  number = {3},
  pages = {25--25},
  issn = {2371-9621},
  doi = {10.1609/aimag.v23i3.1654},
  url = {https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/1654},
  urldate = {2025-06-29},
  abstract = {This article is a reflection on agent-based AI. My contention is that AI research should focus on interactive, autonomous systems, that is, agents. Emergent technologies demand so. We see how recent developments in (multi-) agent-oriented research have taken us closer to the original AI goal, namely, to build intelligent systems of general competence. Agents are not the panacea though. I point out several areas such as design description, implementation, reusability, and security that must be developed before agents are universally accepted as the AI of the future.},
  issue = {3},
  langid = {english},
  file = {/home/baldoinov/baldoinov/PDFs/Mestrado/AI and Agents_2002_Alonso.pdf}
}

@online{AIEconomistImproving_2020_ZhengEtAl,
  title = {The {{AI Economist}}: {{Improving Equality}} and {{Productivity}} with {{AI-Driven Tax Policies}}},
  shorttitle = {The {{AI Economist}}},
  author = {Zheng, Stephan and Trott, Alexander and Srinivasa, Sunil and Naik, Nikhil and Gruesbeck, Melvin and Parkes, David C. and Socher, Richard},
  date = {2020-04-28},
  eprint = {2004.13332},
  eprinttype = {arXiv},
  eprintclass = {cs, econ, q-fin, stat},
  doi = {10.48550/arXiv.2004.13332},
  url = {http://arxiv.org/abs/2004.13332},
  urldate = {2023-09-15},
  abstract = {Tackling real-world socio-economic challenges requires designing and testing economic policies. However, this is hard in practice, due to a lack of appropriate (micro-level) economic data and limited opportunity to experiment. In this work, we train social planners that discover tax policies in dynamic economies that can effectively trade-off economic equality and productivity. We propose a two-level deep reinforcement learning approach to learn dynamic tax policies, based on economic simulations in which both agents and a government learn and adapt. Our data-driven approach does not make use of economic modeling assumptions, and learns from observational data alone. We make four main contributions. First, we present an economic simulation environment that features competitive pressures and market dynamics. We validate the simulation by showing that baseline tax systems perform in a way that is consistent with economic theory, including in regard to learned agent behaviors and specializations. Second, we show that AI-driven tax policies improve the trade-off between equality and productivity by 16\% over baseline policies, including the prominent Saez tax framework. Third, we showcase several emergent features: AI-driven tax policies are qualitatively different from baselines, setting a higher top tax rate and higher net subsidies for low incomes. Moreover, AI-driven tax policies perform strongly in the face of emergent tax-gaming strategies learned by AI agents. Lastly, AI-driven tax policies are also effective when used in experiments with human participants. In experiments conducted on MTurk, an AI tax policy provides an equality-productivity trade-off that is similar to that provided by the Saez framework along with higher inverse-income weighted social welfare.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Economics - General Economics,notion,Statistics - Machine Learning},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/The AI Economist_2020_Zheng et al.pdf;/home/baldoinov/Zotero/storage/EPVEIBC5/2004.html}
}

@online{AIEconomistOptimal_2021_ZhengEtAl,
  title = {The {{AI Economist}}: {{Optimal Economic Policy Design}} via {{Two-level Deep Reinforcement Learning}}},
  shorttitle = {The {{AI Economist}}},
  author = {Zheng, Stephan and Trott, Alexander and Srinivasa, Sunil and Parkes, David C. and Socher, Richard},
  date = {2021-08-05},
  eprint = {2108.02755},
  eprinttype = {arXiv},
  eprintclass = {cs, econ, q-fin},
  doi = {10.48550/arXiv.2108.02755},
  url = {http://arxiv.org/abs/2108.02755},
  urldate = {2023-09-15},
  abstract = {AI and reinforcement learning (RL) have improved many areas, but are not yet widely adopted in economic policy design, mechanism design, or economics at large. At the same time, current economic methodology is limited by a lack of counterfactual data, simplistic behavioral models, and limited opportunities to experiment with policies and evaluate behavioral responses. Here we show that machine-learning-based economic simulation is a powerful policy and mechanism design framework to overcome these limitations. The AI Economist is a two-level, deep RL framework that trains both agents and a social planner who co-adapt, providing a tractable solution to the highly unstable and novel two-level RL challenge. From a simple specification of an economy, we learn rational agent behaviors that adapt to learned planner policies and vice versa. We demonstrate the efficacy of the AI Economist on the problem of optimal taxation. In simple one-step economies, the AI Economist recovers the optimal tax policy of economic theory. In complex, dynamic economies, the AI Economist substantially improves both utilitarian social welfare and the trade-off between equality and productivity over baselines. It does so despite emergent tax-gaming strategies, while accounting for agent interactions and behavioral change more accurately than economic theory. These results demonstrate for the first time that two-level, deep RL can be used for understanding and as a complement to theory for economic design, unlocking a new computational learning-based approach to understanding economic policy.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Economics - General Economics,notion},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/The AI Economist_2021_Zheng et al.pdf;/home/baldoinov/Zotero/storage/VM9GBT8Q/2108.html}
}

@article{AIEconomistTaxation_2022_ZhengEtAl,
  title = {The {{AI Economist}}: {{Taxation}} Policy Design via Two-Level Deep Multiagent Reinforcement Learning},
  shorttitle = {The {{AI Economist}}},
  author = {Zheng, Stephan and Trott, Alexander and Srinivasa, Sunil and Parkes, David C. and Socher, Richard},
  date = {2022-05-04},
  journaltitle = {Science Advances},
  volume = {8},
  number = {18},
  pages = {eabk2607},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/sciadv.abk2607},
  url = {https://www.science.org/doi/10.1126/sciadv.abk2607},
  urldate = {2023-09-15},
  abstract = {Artificial intelligence (AI) and reinforcement learning (RL) have improved many areas but are not yet widely adopted in economic policy design, mechanism design, or economics at large. The AI Economist is a two-level, deep RL framework for policy design in which agents and a social planner coadapt. In particular, the AI Economist uses structured curriculum learning to stabilize the challenging two-level, coadaptive learning problem. We validate this framework in the domain of taxation. In one-step economies, the AI Economist recovers the optimal tax policy of economic theory. In spatiotemporal economies, the AI Economist substantially improves both utilitarian social welfare and the trade-off between equality and productivity over baselines. It does so despite emergent tax-gaming strategies while accounting for emergent labor specialization, agent interactions, and behavioral change. These results demonstrate that two-level, deep RL complements economic theory and unlocks an AI-based approach to designing and understanding economic policy.},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/The AI Economist_2022_Zheng et al.pdf;/home/baldoinov/baldoinov/PDFs/Feausp/monografia/The AI Economist_2022_Zheng et al2.pdf}
}

@article{alamgir_2020,
  title = {Emerging Challenges for Sustainable Development and Forest Conservation in {{Sarawak}}, {{Borneo}}},
  author = {Alamgir, M. and Campbell, M.J. and Sloan, S. and Engert, J. and Word, J. and Laurance, W.F.},
  date = {2020},
  journaltitle = {PloS one},
  volume = {15},
  number = {3}
}

@inproceedings{ALBERTLiteBERT_2020_LanEtAl,
  title = {{{ALBERT}}: {{A Lite BERT}} for {{Self-supervised Learning}} of {{Language Representations}}},
  shorttitle = {{{ALBERT}}},
  author = {Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  date = {2020-04},
  url = {https://iclr.cc/virtual_2020/poster_H1eA7AEtvS.html},
  urldate = {2024-08-27},
  abstract = {Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT\textasciitilde\textbackslash citep\{devlin2018bert\}. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \textbackslash squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.},
  eventtitle = {Eighth {{International Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/projeto-heiwa/ALBERT_2020_Lan et al.pdf}
}

@book{AlgebraLinearCom_2011_Rorres,
  title = {Álgebra Linear com Aplicações},
  author = {Rorres, Chris},
  namea = {Anton, Howard},
  nameatype = {collaborator},
  date = {2011-12-16},
  publisher = {Bookman},
  isbn = {978-85-407-0170-0},
  langid = {portuguese},
  keywords = {Matemática}
}

@unpublished{AlgebraTopologyDifferential__GallierQuaintance,
  title = {Algebra, {{Topology}}, {{Diﬀerential Calculus}}, and {{Optimization Theory For Computer Science}} and {{Machine Learning}}},
  author = {Gallier, Jean and Quaintance, Jocelyn},
  langid = {english},
  file = {/home/baldoinov/baldoinov/PDFs/Livros/Algebra, Topology, Diﬀerential Calculus, and Optimization Theory For Computer_Gallier et al.pdf}
}

@book{AlgebraTrigonometry__JayAbramsonEtAl,
  title = {Algebra and {{Trigonometry}}},
  author = {{Jay Abramson} and {Valeree Falduto} and {Rachael Gross} and {David Lippman} and {Melonie Rasmussen} and {Rick Norwood} and {Nicholas Belloit} and {Harold Whipple} and {Jean-Marie Magnier} and {Christina Fernandez}},
  langid = {english},
  file = {/home/baldoinov/baldoinov/PDFs/Livros/Algebra and Trigonometry_Jay Abramson et al.pdf}
}

@book{AlgebraTrigonometry_2012_StewartEtAl,
  title = {Algebra and {{Trigonometry}}},
  author = {Stewart, James and Redlin, L. and Watson, Saleem},
  date = {2012},
  edition = {3rd ed},
  publisher = {Brooks/Cole, Cengage Learning},
  location = {Belmont, CA},
  isbn = {978-0-8400-6813-2},
  pagetotal = {4},
  keywords = {Algebra,Textbooks,Trigonometry}
}

@book{AlgorithmDesignManual_2020_Skiena,
  title = {The {{Algorithm Design Manual}}},
  author = {Skiena, Steven S.},
  date = {2020},
  series = {Texts in {{Computer Science}}},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-54256-6},
  url = {http://link.springer.com/10.1007/978-3-030-54256-6},
  urldate = {2024-04-01},
  isbn = {978-3-030-54255-9 978-3-030-54256-6},
  langid = {english},
  file = {/home/baldoinov/baldoinov/PDFs/Livros/The Algorithm Design Manual_2020_Skiena.pdf}
}

@book{AlgorithmicGameTheory_2007_Nisan,
  title = {Algorithmic {{Game Theory}}},
  editor = {Nisan, Noam},
  date = {2007},
  publisher = {Cambridge University Press},
  location = {Cambridge ; New York},
  isbn = {978-0-521-87282-9},
  pagetotal = {754},
  keywords = {Algorithms,Game theory},
  annotation = {OCLC: ocn122526907},
  file = {/home/baldoinov/baldoinov/PDFs/Livros/Algorithmic Game Theory_2007_Nisan.pdf}
}

@book{AlgorithmicTradingMethods_2021_,
  title = {Algorithmic {{Trading Methods}}},
  date = {2021},
  publisher = {Elsevier},
  doi = {10.1016/C2017-0-03456-0},
  url = {https://linkinghub.elsevier.com/retrieve/pii/C20170034560},
  urldate = {2025-04-21},
  isbn = {978-0-12-815630-8},
  langid = {english},
  file = {/home/baldoinov/baldoinov/PDFs/Livros/Algorithmic Trading Methods_2021_.pdf}
}

@book{AlgorithmicTradingMethods_2021_Kissell,
  title = {Algorithmic Trading Methods: Applications Using Advanced Statistics, Optimization, and Machine Learning Techniques},
  shorttitle = {Algorithmic Trading Methods},
  author = {Kissell, Robert},
  date = {2021},
  edition = {Second edition},
  publisher = {Academic Press},
  location = {London, United Kingdom},
  abstract = {Algorithmic Trading Methods: Applications using Advanced Statistics, Optimization, and Machine Learning Techniques, Second Edition, is a sequel to The Science of Algorithmic Trading and Portfolio Management. This edition includes new chapters on algorithmic trading, advanced trading analytics, regression analysis, optimization, and advanced statistical methods. Increasing its focus on trading strategies and models, this edition includes new insights into the ever-changing financial environment, pre-trade and post-trade analysis, liquidation cost \& risk analysis, and compliance and regulatory reporting requirements. Highlighting new investment techniques, this book includes material to assist in the best execution process, model validation, quality and assurance testing, limit order modeling, and smart order routing analysis. Includes advanced modeling techniques using machine learning, predictive analytics, and neural networks. The text provides readers with a suite of transaction cost analysis functions packaged as a TCA library. These programming tools are accessible via numerous software applications and programming languages. Provides insight into all necessary components of algorithmic trading including: transaction cost analysis, market impact estimation, risk modeling and optimization, and advanced examination of trading algorithms and corresponding data requirements Increased coverage of essential mathematics, probability and statistics, machine learning, predictive analytics, and neural networks, and applications to trading and finance Advanced multiperiod trade schedule optimization and portfolio construction techniques Techniques to decode broker-dealer and third-party vendor models Methods to incorporate TCA into proprietary alpha models and portfolio optimizers TCA library for numerous software applications and programming languages including: MATLAB, Excel Add-In, Python, Java, C/C++, .Net, Hadoop, and as standalone .EXE and .COM applications},
  isbn = {978-0-12-815630-8 978-0-12-815631-5},
  langid = {english},
  pagetotal = {1}
}

@book{AlgoritmosTeoriaPratica_2011_CormenEtAl,
  title = {Algoritmos - Teoria e Prática},
  author = {Cormen, Thomas and Leiserson, Charles and Rivest, Ronald and Stein, Clifford},
  date = {2011-07-08},
  publisher = {Elsevier},
  isbn = {978-85-352-3699-6},
  langid = {portuguese},
  keywords = {Tecnologia (ciências aplicadas)}
}

@article{althaus_2009,
  title = {Impacts of Bottom Trawling on Deep-Coral Ecosystems of Seamounts Are Long-Lasting},
  author = {Althaus, F and Williams, A and family=Schlacher, given=TA, given-i=TA and family=Kloser, given=RJ, given-i=RJ and family=Green, given=MA, given-i=MA and family=Barker, given=BA, given-i=BA and family=Bax, given=NJ, given-i=NJ and Brodie, P and Schlacher-Hoenlinger, Monica A},
  date = {2009},
  journaltitle = {Marine Ecology Progress Series},
  volume = {397},
  pages = {279--294}
}

@inproceedings{AnaliseAutomaticaCoerencia_2015_SilvaFeltrim,
  title = {Análise Automática de Coerência Textual em Resumos Científicos: Avaliando Quebras de Linearidade},
  shorttitle = {Análise Automática de Coerência Textual em Resumos Científicos},
  booktitle = {Simpósio Brasileiro de Tecnologia da Informação e da Linguagem Humana (STIL)},
  author = {family=Silva, given=Leandro Lago, prefix=da, useprefix=false and Feltrim, Valéria Delisandra},
  date = {2015-11-04},
  pages = {45--49},
  publisher = {SBC},
  issn = {0000-0000},
  url = {https://sol.sbc.org.br/index.php/stil/article/view/3997},
  urldate = {2024-08-28},
  abstract = {Este artigo apresenta uma extensão do módulo de análise de coerência que é parte da ferramenta SciPo, visando à análise automática da dimensão chamada Quebra de Linearidade. A implementação proposta é baseada na combinação do modelo grade de entidades com informações provenientes da estrutura retórica do resumo, permitindo que o módulo gere mensagens que indiquem possíveis quebras de linearidade em regiões específicas do resumo. Experimentos mostraram que a combinação do modelo grade de entidades com a estrutura retórica é viável e pode vir a ser utilizada como parte da ferramenta SciPo.},
  eventtitle = {Simpósio Brasileiro de Tecnologia da Informação e da Linguagem Humana (STIL)},
  langid = {portuguese},
  file = {/home/baldoinov/baldoinov/PDFs/Mestrado/Analise Automatica de Coerencia Textual em Resumos Cientificos_2015_Silva et al.pdf}
}

@article{AnaliseSentimentoPor_2021_TokudaEtAl,
  title = {Análise de sentimento por meio de deep learning aplicada à mineração de argumentos},
  author = {Tokuda, Nivaldo H. O. and Coelho, Orlando B. and family=Araujo, given=Renata M., prefix=de, useprefix=false},
  date = {2021-05-31},
  publisher = {Universidade Presbiteriana Mackenzie},
  url = {https://dspace.mackenzie.br/handle/10899/31037},
  urldate = {2023-04-15},
  abstract = {Mineração de Argumentos é um tópico que vem obtendo crescente   importância na comunidade de pesquisa em Processamento de Linguagem Natural.   Há uma abundância de conteúdo em linguagem natural disponível nas redes   sociais, com potencial para análise das argumentações que se estabelecem entre   os participantes da rede. Dado esse contexto, este trabalho tem o objetivo de   explorar técnicas de Mineração de Argumentos para identificação de estruturas de   argumentação em corpora de discussões em redes sociais. Mais especificamente,   este trabalho tem como objetivo focar em um dos aspectos essenciais em um   processo de mineração de argumentos: a análise de sentimento das sentenças   apresentadas nas discussões, usando técnicas modernas de Deep Learning.Para   essa finalidade, foi utilizada a arquitetura Transformer, que obteve resultados   promissores.},
  langid = {brazilian},
  keywords = {notion},
  annotation = {Accepted: 2022-11-17T18:11:01Z},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/projeto-heiwa/Analise de sentimento por meio de deep learning aplicada a mineracao de_2021_Tokuda et al.pdf}
}

@book{AnalysisTimeSeries_2019_ChatfieldXing,
  title = {The {{Analysis}} of {{Time Series}}: {{An Introduction}} with {{R}}},
  shorttitle = {The Analysis of Time Series},
  author = {Chatfield, Christopher and Xing, Haipeng},
  date = {2019},
  series = {Chapman \& {{Hall}}/{{CRC}} Texts in Statistical Science Series},
  edition = {Seventh edition},
  publisher = {CRC Press, Taylor \& Francis Group},
  location = {Boca Raton},
  isbn = {978-1-138-06613-7 978-1-4987-9563-0},
  langid = {english},
  pagetotal = {398},
  keywords = {Time-series analysis}
}

@online{AnnoLLMMakingLarge_2023_HeEtAl,
  title = {{{AnnoLLM}}: {{Making Large Language Models}} to {{Be Better Crowdsourced Annotators}}},
  shorttitle = {{{AnnoLLM}}},
  author = {He, Xingwei and Lin, Zhenghao and Gong, Yeyun and Jin, A.-Long and Zhang, Hang and Lin, Chen and Jiao, Jian and Yiu, Siu Ming and Duan, Nan and Chen, Weizhu},
  date = {2023-03-29},
  eprint = {2303.16854},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2303.16854},
  url = {http://arxiv.org/abs/2303.16854},
  urldate = {2024-03-01},
  abstract = {Many natural language processing (NLP) tasks rely on labeled data to train machine learning models to achieve high performance. However, data annotation can be a time-consuming and expensive process, especially when the task involves a large amount of data or requires specialized domains. Recently, GPT-3.5 series models have demonstrated remarkable few-shot and zero-shot ability across various NLP tasks. In this paper, we first claim that large language models (LLMs), such as GPT-3.5, can serve as an excellent crowdsourced annotator by providing them with sufficient guidance and demonstrated examples. To make LLMs to be better annotators, we propose a two-step approach, 'explain-then-annotate'. To be more precise, we begin by creating prompts for every demonstrated example, which we subsequently utilize to prompt a LLM to provide an explanation for why the specific ground truth answer/label was chosen for that particular example. Following this, we construct the few-shot chain-of-thought prompt with the self-generated explanation and employ it to annotate the unlabeled data. We conduct experiments on three tasks, including user input and keyword relevance assessment, BoolQ and WiC. The annotation results from GPT-3.5 surpasses those from crowdsourced annotation for user input and keyword relevance assessment. Additionally, for the other two tasks, GPT-3.5 achieves results that are comparable to those obtained through crowdsourced annotation.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,notion},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/projeto-heiwa/AnnoLLM_2023_He et al.pdf;/home/baldoinov/Zotero/storage/XI4E7EZS/2303.html}
}

@inproceedings{AnnotatedCorpusSentiment_2015_ArrudaEtAl,
  title = {An Annotated Corpus for Sentiment Analysis in Political News},
  booktitle = {Simpósio Brasileiro de Tecnologia da Informação e da Linguagem Humana (STIL)},
  author = {family=Arruda, given=Gabriel Domingos, prefix=de, useprefix=false and Roman, Norton Trevisan and Monteiro, Ana Maria},
  date = {2015-11-04},
  pages = {101--110},
  publisher = {SBC},
  issn = {0000-0000},
  url = {https://sol.sbc.org.br/index.php/stil/article/view/3970},
  urldate = {2024-08-28},
  abstract = {This article describes a corpus of news texts in Brazilian Portuguese. News were collected from four big newswire outlets, segmented in paragraphs, and marked up by a group of four annotators, who had to classify each paragraph according to two dimensions: target entity (that is the person which is the main subject of the news contained in the paragraph), and the paragraph's polarity with respect to the target entity. The corpus comprises 131 news, segmented in 1,447 paragraphs, with 65,675 words in total. Along with the corpus, we have also built a gold standard, where paragraphs are classified according to the opinion of the majority of annotators. This gold standard and annotated corpus are available to the community under a Creative Commons licence.},
  eventtitle = {Simpósio Brasileiro de Tecnologia da Informação e da Linguagem Humana (STIL)},
  langid = {portuguese},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/projeto-heiwa/An Annotated Corpus for Sentiment Analysis in Political News_2015_Arruda et al.pdf}
}

@online{AnnotatedTransformer__,
  title = {The {{Annotated Transformer}}},
  url = {https://nlp.seas.harvard.edu/annotated-transformer/},
  urldate = {2024-05-23},
  keywords = {notion},
  file = {/home/baldoinov/Zotero/storage/UL85G8WE/annotated-transformer.html}
}

@inproceedings{AnotandoUmCorpus_2015_DosciattiEtAl,
  title = {Anotando um Corpus de Notícias para a Análise de Sentimento: um Relato de Experiência},
  shorttitle = {Anotando um Corpus de Notícias para a Análise de Sentimento},
  booktitle = {Simpósio Brasileiro de Tecnologia da Informação e da Linguagem Humana (STIL)},
  author = {Dosciatti, Mariza Miola and Ferreira, Lohann Paterno Coutinho and Paraiso, Emerson Cabrera},
  date = {2015-11-04},
  pages = {121--130},
  publisher = {SBC},
  issn = {0000-0000},
  url = {https://sol.sbc.org.br/index.php/stil/article/view/3972},
  urldate = {2024-08-28},
  abstract = {Este artigo relata o processo de construção e anotação de um corpus de notícias para a Análise de Sentimento. Os textos, extraídos de jornais do Brasil, foram anotados com as emoções básicas (alegria, tristeza, raiva, surpresa, repugnância e medo) ou a ausência de emoção (neutro). O processo de anotação resultou em valor de concordância baixo (kappa = 0,38). Apresentamos o processo de anotação e os resultados de alguns experimentos realizados durante e após a anotação, com o objetivo de entender os motivos da baixa concordância. O corpus anotado foi submetido a um método de identificação de emoções, sendo os resultados obtidos também apresentados.},
  eventtitle = {Simpósio Brasileiro de Tecnologia da Informação e da Linguagem Humana (STIL)},
  langid = {portuguese},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/projeto-heiwa/Anotando um Corpus de Noticias para a Analise de Sentimento_2015_Dosciatti et al.pdf}
}

@online{AnswerNearestNeighbor_2022_Noah,
  title = {Answer to "{{Nearest Neighbor Matching}}"},
  author = {Noah},
  date = {2022-03-07},
  url = {https://stats.stackexchange.com/a/566981},
  urldate = {2025-07-29},
  organization = {Cross Validated},
  file = {/home/baldoinov/Zotero/storage/QYPENPAI/nearest-neighbor-matching.html}
}

@article{anwar_2015,
  title = {Causes of Ozone Layer Depletion and Its Effects on Human},
  author = {Anwar, F. and Chaudhry, F.N. and Nazeer, S. and Zaman, N. and Azam, S.},
  date = {2015},
  journaltitle = {Atmospheric and Climate Sciences},
  volume = {6},
  number = {1},
  pages = {129--134}
}

@unpublished{AposilaTeoriaMusical_2008_JorgeNobre,
  title = {Aposila de {{Teoria Musical}}},
  author = {{Jorge Nobre}},
  date = {2008},
  file = {/home/baldoinov/baldoinov/PDFs/Livros/Aposila de Teoria Musical_2008_Jorge Nobre.pdf}
}

@book{ApostilaBasicaAudio_2004_Filho,
  title = {Apostila Básica de Áudio},
  author = {Filho, Filippo V},
  date = {2004},
  langid = {portuguese},
  file = {/home/baldoinov/baldoinov/PDFs/Livros/Apostila Basica de Audio_2004_Filho.pdf}
}

@book{AppliedEconometricTime_2015_Enders,
  title = {Applied {{Econometric Time Series}}},
  author = {Enders, Walter},
  date = {2015},
  edition = {Fourth edition},
  publisher = {Wiley},
  location = {Hoboken, NJ},
  isbn = {978-1-118-80856-6},
  langid = {english},
  pagetotal = {485},
  keywords = {Econometrics,Time-series analysis}
}

@book{AppliedEconomicsThinking_2009_Sowell,
  title = {Applied Economics: Thinking beyond Stage One},
  shorttitle = {Applied Economics},
  author = {Sowell, Thomas},
  date = {2009},
  edition = {Rev. and enl. ed},
  publisher = {Basic Books},
  location = {New York},
  isbn = {978-0-465-00345-7},
  langid = {english},
  pagetotal = {336},
  keywords = {Economic development,Economic policy,Economics,Equality,Political aspects,Social aspects,Social problems}
}

@unpublished{ApplyingAgentbasedModelling_2018_NigelGilbert,
  title = {Applying Agent-Based Modelling ({{ABM}}) to Evaluation},
  author = {{Nigel Gilbert}},
  date = {2018-07-05},
  url = {https://www.youtube.com/watch?v=e9dLh6_ZgW8},
  urldate = {2023-09-30},
  abstract = {Professor Nigel Gilbert was presenting at the 8th ESRC Research Methods Festival, 3rd - 5th July 2018 at the University of Bath. The Festival is organised every two years by the National Centre for Research Methods: www.ncrm.ac.uk.},
  venue = {University of Bath},
  keywords = {notion}
}

@book{ApproachingAlmostAny__Thakur,
  title = {Approaching ({{Almost}}) {{Any Machine Learning Problem}}},
  author = {Thakur, Abhishek},
  url = {https://store.pothi.com/book/abhishek-thakur-approaching-almost-any-machine-learning-problem/},
  urldate = {2024-03-13},
  abstract = {Buy Approaching (Almost) Any Machine Learning Problem by Abhishek Thakur in India. This is not a traditional book. The book has a lot of code. If you don't like the code first approach do not buy this book. Making code available on Github is not an option. This book is for people who have some theoretical knowledge of machine learning and deep},
  isbn = {978-93-90274-43-7},
  langid = {english},
  file = {/home/baldoinov/baldoinov/PDFs/Livros/Approaching (Almost) Any Machine Learning Problem_Thakur.pdf}
}

@article{araujo_2009,
  title = {Property Rights and Deforestation in the {{Brazilian Amazon}}},
  author = {Araujo, C. and Bonjean, C.A. and Combes, J.L. and Motel, P.C. and Reis, E.J.},
  date = {2009},
  journaltitle = {Ecological economics},
  volume = {68},
  number = {8},
  pages = {2461--2468}
}

@book{ArchitectureFormSpace_2007_Ching,
  title = {Architecture: Form, Space and Order},
  shorttitle = {Architecture},
  author = {Ching, Francis D. K.},
  date = {2007},
  edition = {3. ed},
  publisher = {Wiley},
  location = {Hoboken, N.J},
  isbn = {978-0-471-75216-5},
  langid = {english},
  pagetotal = {431}
}

@book{ArchitectureIslamicWest_2020_Bloom,
  title = {Architecture of the {{Islamic West}}: {{North Africa}} and the {{Iberian Peninsula}}, 700-1800},
  shorttitle = {Architecture of the {{Islamic}} West},
  author = {Bloom, Jonathan Max},
  date = {2020},
  publisher = {Yale university press},
  location = {New Haven (Conn.)},
  abstract = {Some of the most outstanding examples of world architecture, such as the Mosque of Cordoba, the ceiling of the Cappella Palatina in Palermo, the Giralda tower in Seville, and the Alhambra Palace in Granada, belong to the Western Islamic tradition. This architectural style flourished for over a thousand years along the southern and western shores of the Mediterranean-between Tunisia and Spain-from the 8th century through the 19th, blending new ideas with local building practices from across the region.0Jonathan M. Bloom's 'Architecture of the Islamic West' introduces readers to the full scope of this vibrant tradition, presenting both famous and little-known buildings in six countries in North Africa and southern Europe. It is richly illustrated with photographs, specially commissioned architectural plans, and historical documents. The result is a personally guided tour of Islamic architecture led by one of the finest scholars in the field and a powerful testament to Muslim cultural achievement},
  isbn = {978-0-300-21870-1},
  langid = {english}
}

@article{Argumentation2016US_2020_VisserEtAl,
  title = {Argumentation in the 2016 {{US}} Presidential Elections: Annotated Corpora of Television Debates and Social Media Reaction},
  shorttitle = {Argumentation in the 2016 {{US}} Presidential Elections},
  author = {Visser, Jacky and Konat, Barbara and Duthie, Rory and Koszowy, Marcin and Budzynska, Katarzyna and Reed, Chris},
  date = {2020-03-01},
  journaltitle = {Language Resources and Evaluation},
  shortjournal = {Lang Resources \& Evaluation},
  volume = {54},
  number = {1},
  pages = {123--154},
  issn = {1574-0218},
  doi = {10.1007/s10579-019-09446-8},
  url = {https://doi.org/10.1007/s10579-019-09446-8},
  urldate = {2023-12-14},
  abstract = {In this paper we present US2016, the largest publicly available set of corpora of annotated dialogical argumentation. The annotation covers argumentative relations, dialogue acts and pragmatic features. The corpora comprise transcriptions of television debates leading up to the 2016 US presidential elections, and reactions to the debates on Reddit.These two constitutive parts of the corpora are integrated by means of the intertextual correspondence between them. The rhetorical richness and high argument density of the communicative context results in cross-genre corpora that are robust resources for the study of the dialogical dynamics of argumentation in three ways: first, in empirical strands of research in discourse analysis and argumentation studies; second,in the burgeoning field of argument mining where automatic techniques require such data; and third, in formulating algorithmic techniques for sensemaking through the development of Argument Analytics.},
  langid = {english},
  keywords = {Argumentation,Corpus,Intertextual correspondence,notion,Political discourse,Reddit,Television debate,US elections},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/projeto-heiwa-revisao-de-literatura/Argumentation in the 2016 US presidential elections_2020_Visser et al.pdf}
}

@book{ArgumentationMining_2018_StedeSchneider,
  title = {Argumentation {{Mining}}},
  author = {Stede, Manfred and Schneider, Jodi},
  date = {2018-12-20},
  eprint = {Z3WBDwAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {Morgan \& Claypool Publishers},
  abstract = {Argumentation mining is an application of natural language processing (NLP) that emerged a few years ago and has recently enjoyed considerable popularity, as demonstrated by a series of international workshops and by a rising number of publications at the major conferences and journals of the field. Its goals are to identify argumentation in text or dialogue; to construct representations of the constellation of claims, supporting and attacking moves (in different levels of detail); and to characterize the patterns of reasoning that appear to license the argumentation. Furthermore, recent work also addresses the difficult tasks of evaluating the persuasiveness and quality of arguments. Some of the linguistic genres that are being studied include legal text, student essays, political discourse and debate, newspaper editorials, scientific writing, and others. The book starts with a discussion of the linguistic perspective, characteristics of argumentative language, and their relationship to certain other notions such as subjectivity. Besides the connection to linguistics, argumentation has for a long time been a topic in Artificial Intelligence, where the focus is on devising adequate representations and reasoning formalisms that capture the properties of argumentative exchange. It is generally very difficult to connect the two realms of reasoning and text analysis, but we are convinced that it should be attempted in the long term, and therefore we also touch upon some fundamentals of reasoning approaches. Then the book turns to its focus, the computational side of mining argumentation in text. We first introduce a number of annotated corpora that have been used in the research. From the NLP perspective, argumentation mining shares subtasks with research fields such as subjectivity and sentiment analysis, semantic relation extraction, and discourse parsing. Therefore, many technical approaches are being borrowed from those (and other) fields. We break argumentation mining into a series of subtasks, starting with the preparatory steps of classifying text as argumentative (or not) and segmenting it into elementary units. Then, central steps are the automatic identification of claims, and finding statements that support or oppose the claim. For certain applications, it is also of interest to compute a full structure of an argumentative constellation of statements.  Next, we discuss a few steps that try to 'dig deeper': to infer the underlying reasoning pattern for a textual argument, to reconstruct unstated premises (so-called 'enthymemes'), and to evaluate the quality of the argumentation. We also take a brief look at 'the other side' of mining, i.e., the generation or synthesis of argumentative text. The book finishes with a summary of the argumentation mining tasks, a sketch of potential applications, and a--necessarily subjective--outlook for the field.},
  isbn = {978-1-68173-460-6},
  langid = {english},
  pagetotal = {193},
  keywords = {Computers / Artificial Intelligence / General,Computers / Artificial Intelligence / Natural Language Processing,Language Arts & Disciplines / Linguistics / General,notion},
  file = {/home/baldoinov/baldoinov/PDFs/Livros/Argumentation Mining_2018_Stede et al.pdf}
}

@inproceedings{ArgumentationMiningDetection_2009_PalauMoens,
  title = {Argumentation Mining: The Detection, Classification and Structure of Arguments in Text},
  shorttitle = {Argumentation Mining},
  booktitle = {Proceedings of the 12th {{International Conference}} on {{Artificial Intelligence}} and {{Law}}},
  author = {Palau, Raquel Mochales and Moens, Marie-Francine},
  date = {2009-06-08},
  series = {{{ICAIL}} '09},
  pages = {98--107},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/1568234.1568246},
  url = {https://doi.org/10.1145/1568234.1568246},
  urldate = {2023-04-19},
  abstract = {Argumentation is the process by which arguments are constructed and handled. Argumentation constitutes a major component of human intelligence. The ability to engage in argumentation is essential for humans to understand new problems, to perform scientific reasoning, to express, to clarify and to defend their opinions in their daily lives. Argumentation mining aims to detect the arguments presented in a text document, the relations between them and the internal structure of each individual argument. In this paper we analyse the main research questions when dealing with argumentation mining and the different methods we have studied and developed in order to successfully confront the challenges of argumentation mining in legal texts.},
  isbn = {978-1-60558-597-0},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Argumentation mining_2009_Palau et al.pdf}
}

@article{ArgumentationMiningState_2016_LippiTorroni,
  title = {Argumentation {{Mining}}: {{State}} of the {{Art}} and {{Emerging Trends}}},
  shorttitle = {Argumentation {{Mining}}},
  author = {Lippi, Marco and Torroni, Paolo},
  date = {2016-03-30},
  journaltitle = {ACM Transactions on Internet Technology},
  shortjournal = {ACM Trans. Internet Technol.},
  volume = {16},
  number = {2},
  pages = {10:1--10:25},
  issn = {1533-5399},
  doi = {10.1145/2850417},
  url = {https://doi.org/10.1145/2850417},
  urldate = {2023-04-15},
  abstract = {Argumentation mining aims at automatically extracting structured arguments from unstructured textual documents. It has recently become a hot topic also due to its potential in processing information originating from the Web, and in particular from social media, in innovative ways. Recent advances in machine learning methods promise to enable breakthrough applications to social and economic sciences, policy making, and information technology: something that only a few years ago was unthinkable. In this survey article, we introduce argumentation models and methods, review existing systems and applications, and discuss challenges and perspectives of this exciting new research area.},
  keywords = {Argumentation mining,artificial intelligence,computational linguistics,knowledge representation,machine learning,notion,social media}
}

@article{ArgumentIdentificationIndonesian_2021_HuwaidahEtAl,
  title = {Argument {{Identification}} in {{Indonesian Tweets}} on the {{Issue}} of {{Moving}} the {{Indonesian Capital}}},
  author = {Huwaidah, Amalia and {Adiwijaya} and Faraby, Said Al},
  date = {2021-01-01},
  journaltitle = {Procedia Computer Science},
  shortjournal = {Procedia Computer Science},
  series = {5th {{International Conference}} on {{Computer Science}} and {{Computational Intelligence}} 2020},
  volume = {179},
  pages = {407--415},
  issn = {1877-0509},
  doi = {10.1016/j.procs.2021.01.023},
  url = {https://www.sciencedirect.com/science/article/pii/S1877050921000259},
  urldate = {2023-12-14},
  abstract = {Last October 2019, Indonesian Twitter community is busy discussing the issue of moving the capital city, and people are very eager to share their opinion in various expressions. This form of expression was alleged as a form of society expressing their opinions and arguments. This research uses a dataset from online discussions about moving Indonesian capital on Twitter. The goal of this study aims to identify whether a tweet contains argument or not. In this experiment, we use Multi-Class Support Vector Machine (SVM), and Multinomial Naïve Bayes (MNB) as the classifier and TF-IDF as feature extraction. Variation of Twitter data characters that have a lot of noise will be a challenge in this study so that some preprocessing processes will be carried out to overcome this problem. This research will investigate several combinations of preprocessing to discover the best result. We classify each tweet information such as argument, non-argument, and unknown. The best results with an accuracy of 71.42\% were obtained by performing SVM with only a unigram feature. This study shows that the stopwords feature has effectiveness depends on which feature combination is implemented in the model.},
  keywords = {argument mining,moving capital,multinomial naïve bayes,notion,support vector machine,text classification},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/projeto-heiwa-revisao-de-literatura/Argument Identification in Indonesian Tweets on the Issue of Moving the_2021_Huwaidah et al.pdf}
}

@inproceedings{ArgumentMiningMachine_2015_LippiTorroni,
  title = {Argument {{Mining}}: {{A Machine Learning Perspective}}},
  shorttitle = {Argument {{Mining}}},
  booktitle = {Theory and {{Applications}} of {{Formal Argumentation}}},
  author = {Lippi, Marco and Torroni, Paolo},
  editor = {Black, Elizabeth and Modgil, Sanjay and Oren, Nir},
  date = {2015},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {163--176},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-28460-6_10},
  abstract = {Argument mining has recently become a hot topic, attracting the interests of several and diverse research communities, ranging from artificial intelligence, to computational linguistics, natural language processing, social and philosophical sciences. In this paper, we attempt to describe the problems and challenges of argument mining from a machine learning angle. In particular, we advocate that machine learning techniques so far have been under-exploited, and that a more proper standardization of the problem, also with regards to the underlying argument model, could provide a crucial element to develop better systems.},
  isbn = {978-3-319-28460-6},
  langid = {english},
  keywords = {Annotate Corpus,Natural Language Processing,notion,Parse Tree,Sentiment Analysis,Support Vector Machine},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/projeto-heiwa/Argument Mining_2015_Lippi et al.pdf}
}

@article{ArgumentMiningSurvey_2020_LawrenceReed,
  title = {Argument {{Mining}}: {{A Survey}}},
  shorttitle = {Argument {{Mining}}},
  author = {Lawrence, John and Reed, Chris},
  date = {2020-01-01},
  journaltitle = {Computational Linguistics},
  shortjournal = {Computational Linguistics},
  volume = {45},
  number = {4},
  pages = {765--818},
  issn = {0891-2017},
  doi = {10.1162/coli_a_00364},
  url = {https://doi.org/10.1162/coli_a_00364},
  urldate = {2023-04-15},
  abstract = {Argument mining is the automatic identification and extraction of the structure of inference and reasoning expressed as arguments presented in natural language. Understanding argumentative structure makes it possible to determine not only what positions people are adopting, but also why they hold the opinions they do, providing valuable insights in domains as diverse as financial market prediction and public relations. This survey explores the techniques that establish the foundations for argument mining, provides a review of recent advances in argument mining techniques, and discusses the challenges faced in automatically extracting a deeper understanding of reasoning expressed in language in general.},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/projeto-heiwa/Argument Mining_2020_Lawrence et al.pdf}
}

@inproceedings{ArgumentMiningTweets_2021_IskenderEtAl,
  title = {Argument {{Mining}} in {{Tweets}}: {{Comparing Crowd}} and {{Expert Annotations}} for~{{Automated Claim}} and {{Evidence Detection}}},
  shorttitle = {Argument {{Mining}} in {{Tweets}}},
  booktitle = {Natural {{Language Processing}} and {{Information Systems}}},
  author = {Iskender, Neslihan and Schaefer, Robin and Polzehl, Tim and Möller, Sebastian},
  editor = {Métais, Elisabeth and Meziane, Farid and Horacek, Helmut and Kapetanios, Epaminondas},
  date = {2021},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {275--288},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-80599-9_25},
  abstract = {One of the main challenges in the development of argument mining tools is the availability of annotated data of adequate size and quality. However, generating data sets using experts is expensive from both organizational and financial perspectives, which is also the case for tools developed for identifying argumentative content in informal social media texts like tweets. As a solution, we propose using crowdsourcing as a fast, scalable, and cost-effective alternative to linguistic experts. To investigate the crowd workers’ performance, we compare crowd and expert annotations of argumentative content, dividing it into claim and evidence, for 300 German tweet pairs from the domain of climate change. As being the first work comparing crowd and expert annotations for argument mining in tweets, we show that crowd workers can achieve similar results to experts when annotating claims; however, identifying evidence is a more challenging task both for naive crowds and experts. Further, we train supervised classification and sequence labeling models for claim and evidence detection, showing that crowdsourced data delivers promising results when comparing to experts.},
  isbn = {978-3-030-80599-9},
  langid = {english},
  keywords = {Argument mining,Corpus annotation,Crowdsourcing,notion},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/projeto-heiwa-revisao-de-literatura/Argument Mining in Tweets_2021_Iskender et al.pdf}
}

@article{ArgumentMiningTwitter_2021_SchaeferStede,
  title = {Argument {{Mining}} on {{Twitter}}: {{A}} Survey},
  shorttitle = {Argument {{Mining}} on {{Twitter}}},
  author = {Schaefer, Robin and Stede, Manfred},
  date = {2021-02-01},
  journaltitle = {Information Technology},
  volume = {63},
  number = {1},
  pages = {45--58},
  publisher = {De Gruyter Oldenbourg},
  issn = {2196-7032},
  doi = {10.1515/itit-2020-0053},
  url = {https://www.degruyter.com/document/doi/10.1515/itit-2020-0053/html},
  urldate = {2023-12-06},
  abstract = {In the last decade, the field of argument mining has grown notably. However, only relatively few studies have investigated argumentation in social media and specifically on Twitter. Here, we provide the, to our knowledge, first critical in-depth survey of the state of the art in tweet-based argument mining. We discuss approaches to modelling the structure of arguments in the context of tweet corpus annotation, and we review current progress in the task of detecting argument components and their relations in tweets. We also survey the intersection of argument mining and stance detection, before we conclude with an outlook.},
  langid = {english},
  keywords = {Argument Mining,notion,Stance Detection,Twitter},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/projeto-heiwa/Argument Mining on Twitter_2021_Schaefer et al.pdf}
}

@book{ArtArchitectureIslamic_2011_Alami,
  title = {Art and Architecture in the {{Islamic}} Tradition: Aesthetics, Politics Ad Desire in Early {{Islam}}},
  shorttitle = {Art and Architecture in the {{Islamic}} Tradition},
  author = {Alami, Muhammad Hamduni},
  date = {2011},
  series = {Library of Modern {{Middle East}} Studies},
  publisher = {I. B. Tauris},
  location = {London},
  isbn = {978-1-84885-544-1},
  langid = {english}
}

@book{ArtDoingScience_1997_Hamming,
  title = {The Art of Doing Science and Engineering: Learning to Learn},
  shorttitle = {Art of Doing Science and Engineering},
  author = {Hamming, Richard R.},
  date = {1997},
  edition = {1ª edição},
  publisher = {CRC Press},
  location = {Amsterdam},
  abstract = {Highly effective thinking is an art that engineers and scientists can be taught to develop. By presenting actual experiences and analyzing them as they are described, the author conveys the developmental thought processes employed and shows a style of thinking that leads to successful results is something that can be learned. Along with spectacular successes, the author also conveys how failures contributed to shaping the thought processes. Provides the reader with a style of thinking that will enhance a person's ability to function as a problem-solver of complex technical issues. Consists of a collection of stories about the author's participation in significant discoveries, relating how those discoveries came about and, most importantly, provides analysis about the thought processes and reasoning that took place as the author and his associates progressed through engineering problems.},
  isbn = {978-90-5699-501-0},
  langid = {Inglês},
  file = {/home/baldoinov/baldoinov/PDFs/Livros/The Art of Doing Science and Engineering_1997_Hamming.pdf}
}

@book{ArtDoingScience_2020_HammingVictor,
  title = {The Art of Doing Science and Engineering: Learning to Learn},
  shorttitle = {The Art of Doing Science and Engineering},
  author = {Hamming, Richard W. and Victor, Bret},
  date = {2020-05-26},
  publisher = {Stripe Press},
  abstract = {A groundbreaking treatise by one of the great mathematicians of our time, who argues that highly effective thinking can be learned.What spurs on and inspires a great idea? Can we train ourselves to think in a way that will enable world-changing understandings and insights to emerge?Richard Hamming said we can, and first inspired a generation of engineers, scientists, and researchers in 1986 with "You and Your Research," an electrifying sermon on why some scientists do great work, why most don't, why he did, and why you should, too. The Art of Doing Science and Engineering is the full expression of what "You and Your Research" outlined. It's a book about thinking; more specifically, a style of thinking by which great ideas are conceived.The book is filled with stories of great people performing mighty deeds--but they are not meant to simply be admired. Instead, they are to be aspired to, learned from, and surpassed. Hamming consistently returns to Shannon's information theory, Einstein's relativity, Grace Hopper's work on high-level programming, Kaiser's work on digital fillers, and his own error-correcting codes. He also recounts a number of his spectacular failures as clear examples of what to avoid.Originally published in 1996 and adapted from a course that Hamming taught at the U.S. Naval Postgraduate School, this edition includes an all-new foreword by designer, engineer, and founder of Dynamicland Bret Victor, and more than 70 redrawn graphs and charts.The Art of Doing Science and Engineering is a reminder that a childlike capacity for learning and creativity are accessible to everyone. Hamming was as much a teacher as a scientist, and having spent a lifetime forming and confirming a theory of great people, he prepares the next generation for even greater greatness.},
  langid = {Inglês},
  pagetotal = {433},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Livros/The Art of Doing Science and Engineering_2020_Hamming et al.epub}
}

@book{ArteProjetarEm_1996_Neufert,
  title = {Arte de projetar em arquitetura},
  author = {Neufert, Ernst},
  date = {1996-07-15},
  edition = {1ª edição},
  publisher = {Editorial Gustavo Gili, S.L.},
  location = {Barcelona},
  abstract = {O livro Arte de projetar em Arquitetura, ou o Neufert, assim conhecido no mundo inteiro, é uma das obras técnicas com a história mais brilhante do nosso tempo. Três semanas depois da sua publicação, esgotou-se a primeira edição alemã, à qual sucederam, com algumas variantes e pequenos aumentos, dez edições até o fim da segunda guerra mundial, em 1945. Püblicou-se então a décima segunda edição alemã, revisada a fundo. A intensa atividade construtiva foi-se desenvolvendo de tal forma que tornou-se indispensável a publicação da vigésima primeira edição alemã, totalmente revisada e que conserva pouquíssimas páginas das edições anteriores. Além das vinte e uma edições alemãs e das nove espanholas, publicaram-se quotro italianas e três francesas. A introdução de novos e diferentes sistemas construtivos exigiu um estudo mais delicado e minucioso; dividiram-se, por conseguinte, os vinte capítulos das edições anteriores em quarenta e teve-se que aumentar consideràvelmente a extensão do livro, apesar dos esforços realizados para presentar na forma mais concisa possível os desenvolvimentos mais significativos da nossa época.},
  isbn = {978-84-252-1691-6},
  langid = {portuguese}
}

@incollection{ArtificialIntelligenceBehavioral_2018_Camerer,
  title = {Artificial {{Intelligence}} and {{Behavioral Economics}}},
  booktitle = {The {{Economics}} of {{Artificial Intelligence}}: {{An Agenda}}},
  author = {Camerer, Colin F.},
  date = {2018-01},
  pages = {587--608},
  publisher = {University of Chicago Press},
  url = {https://www.nber.org/books-and-chapters/economics-artificial-intelligence-agenda/artificial-intelligence-and-behavioral-economics},
  urldate = {2023-04-12},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Artificial Intelligence and Behavioral Economics_2018_Camerer.pdf}
}

@article{ArtificialIntelligenceInspired_2021_Samothrakis,
  title = {Artificial {{Intelligence}} Inspired Methods for the Allocation of Common Goods and Services},
  author = {Samothrakis, Spyridon},
  date = {2021-09-29},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {16},
  number = {9},
  pages = {e0257399},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0257399},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0257399},
  urldate = {2023-08-18},
  abstract = {The debate over the optimal way of allocating societal surplus (i.e. products and services) has been raging, in one form or another, practically forever; following the collapse of the Soviet Union in 1991, the market has taken the lead vs the public sector to do this. Working within the tradition of Marx, Leontief, Beer and Cockshott, we propose what we deem an automated planning system that aims to operate on unit level (e.g., factories and citizens), rather than on aggregate demand and sectors. We explain why it is both a viable and desirable alternative to current market conditions and position our solution within current societal structures. Our experiments show that it would be trivial to plan for up to 50K industrial goods and 5K final goods in commodity hardware. Our approach bridges the gap between traditional planning methods and modern AI planning, opening up venues for further research.},
  langid = {english},
  keywords = {Artificial intelligence,Economics,Milk,notion,Optimization,Privatization,Salaries,Sensory perception,Socialism},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Artificial Intelligence inspired methods for the allocation of common goods and_2021_Samothrakis.pdf}
}

@book{ArtificialIntelligenceModern_2022_RussellEtAl,
  title = {Artificial {{Intelligence}}: A Modern Approach},
  shorttitle = {Artificial Intelligence},
  author = {Russell, Stuart J. and Norvig, Peter and Chang, Ming-wei and Devlin, Jacob and Dragan, Anca and Forsyth, David and Goodfellow, Ian and Malik, Jitendra and Mansinghka, Vikas and Pearl, Judea and Wooldridge, Michael J.},
  date = {2022},
  series = {Pearson Series in Artificial Intelligence},
  edition = {Fourth edition, global edition},
  publisher = {Pearson},
  location = {Harlow},
  abstract = {"Updated edition of popular textbook on Artificial Intelligence. This edition specific looks at ways of keeping artificial intelligence under control"},
  isbn = {978-1-292-40113-3},
  langid = {english},
  pagetotal = {1166}
}

@article{ArtificialIntelligenceRevolution_2019_Jordan,
  title = {Artificial {{Intelligence}}—{{The Revolution Hasn}}’t {{Happened Yet}}},
  author = {Jordan, Michael I.},
  date = {2019-07-03},
  journaltitle = {Harvard Data Science Review},
  volume = {1},
  number = {1},
  issn = {2644-2353, 2688-8513},
  doi = {10.1162/99608f92.f06c6e61},
  url = {https://hdsr.mitpress.mit.edu/pub/wot7mkc1/release/10},
  urldate = {2023-04-13},
  langid = {english},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Artificial Intelligence—The Revolution Hasn’t Happened Yet_2019_Jordan.pdf}
}

@book{ArtificialSocietiesComputer_1995_GilbertEtAl,
  title = {Artificial Societies: The Computer Simulation of Social Life},
  shorttitle = {Artificial Societies},
  editor = {Gilbert, G. Nigel and Gilbert, Nigel and Conte, Rosaria},
  date = {1995},
  edition = {1. publ},
  publisher = {UCL Press},
  location = {London},
  isbn = {978-1-85728-305-1},
  langid = {english},
  pagetotal = {302}
}

@inproceedings{AttentionAllYou_2017_VaswaniEtAl,
  title = {Attention Is {{All}} You {{Need}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
  date = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
  urldate = {2024-03-02},
  abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/projeto-heiwa/Attention is All you Need_2017_Vaswani et al.pdf}
}

@article{AttentionMechanismTransformers_2023_GhojoghGhodsi,
  title = {Attention {{Mechanism}}, {{Transformers}}, {{BERT}}, and {{GPT}}: {{Tutorial}} and {{Survey}}},
  shorttitle = {Attention {{Mechanism}}, {{Transformers}}, {{BERT}}, and {{GPT}}},
  author = {Ghojogh, Benyamin and Ghodsi, Ali},
  date = {2023-12-07},
  publisher = {OSF},
  doi = {10.31219/osf.io/m6gcn},
  url = {https://osf.io/m6gcn},
  urldate = {2023-12-07},
  abstract = {This is a tutorial and survey paper on the attention mechanism, transformers, BERT, and GPT. We first explain attention mechanism, sequence-to-sequence model without and with attention, self-attention, and attention in different areas such as natural language processing and computer vision. Then, we explain transformers which do not use any recurrence. We explain all the parts of encoder and decoder in the transformer, including positional encoding, multihead self-attention and cross-attention, and masked multihead attention. Thereafter, we introduce the Bidirectional Encoder Representations from Transformers (BERT) and Generative Pre-trained Transformer (GPT) as the stacks of encoders and decoders of transformer, respectively. We explain their characteristics and how they work.},
  langid = {american},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Attention Mechanism, Transformers, BERT, and GPT_2023_Ghojogh et al.pdf;/home/baldoinov/Zotero/storage/9QK4BPCY/m6gcn.html}
}

@article{AutomatedDeepReinforcement_2023_AfsharEtAl,
  title = {An {{Automated Deep Reinforcement Learning Pipeline}} for {{Dynamic Pricing}}},
  author = {Afshar, Reza Refaei and Rhuggenaath, Jason and Zhang, Yingqian and Kaymak, Uzay},
  date = {2023-06},
  journaltitle = {IEEE Transactions on Artificial Intelligence},
  volume = {4},
  number = {3},
  pages = {428--437},
  issn = {2691-4581},
  doi = {10.1109/TAI.2022.3186292},
  abstract = {A dynamic pricing problem is difficult due to the highly dynamic environment and unknown demand distributions. In this article, we propose a deep reinforcement learning (DRL) framework, which is a pipeline that automatically defines the DRL components for solving a dynamic pricing problem. The automated DRL pipeline is necessary because the DRL framework can be designed in numerous ways, and manually finding optimal configurations is tedious. The levels of automation make nonexperts capable of using DRL for dynamic pricing. Our DRL pipeline contains three steps of DRL design, including Markov decision process modeling, algorithm selection, and hyperparameter optimization. It starts with transforming available information to state representation and defining reward function using a reward shaping approach. Then, the hyperparameters are tuned using a novel hyperparameter optimization method that integrates Bayesian optimization and the selection operator of the genetic algorithm. We employ our DRL pipeline on reserve price optimization problems in online advertising as a case study. We show that using the DRL configuration obtained by our DRL pipeline, a pricing policy is obtained whose revenue is significantly higher than the benchmark methods. The evaluation is performed by developing a simulation for the real-time bidding environment that makes exploration possible for the reinforcement learning agent.},
  eventtitle = {{{IEEE Transactions}} on {{Artificial Intelligence}}},
  keywords = {Automated reinforcement learning (AutoRL) pipeline,Bayesian optimization (BO),dynamic pricing (DP),Heuristic algorithms,Machine learning algorithms,Mathematical models,notion,Optimization,Pipelines,Pricing,Reinforcement learning},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/An Automated Deep Reinforcement Learning Pipeline for Dynamic Pricing_2023_Afshar et al.pdf;/home/baldoinov/Zotero/storage/V6CSXZAW/9807363.html}
}

@incollection{AutomatedMarketsTrading_2006_MacKie-MasonWellman,
  title = {Automated {{Markets}} and {{Trading Agents}}},
  booktitle = {Handbook of {{Computational Economics}}},
  author = {MacKie-Mason, Jeffrey K. and Wellman, Michael P.},
  editor = {Tesfatsion, L. and Judd, K. L.},
  date = {2006-01-01},
  volume = {2},
  pages = {1381--1431},
  publisher = {Elsevier},
  doi = {10.1016/S1574-0021(05)02028-9},
  url = {https://www.sciencedirect.com/science/article/pii/S1574002105020289},
  urldate = {2024-05-30},
  abstract = {Computer automation has the potential, just starting to be realized, of transforming the design and operation of markets, and the behaviors of agents trading in them. We discuss the possibilities for automating markets, presenting a broad conceptual framework covering resource allocation as well as enabling marketplace services such as search and transaction execution. One of the most intriguing opportunities is provided by markets implementing computationally sophisticated negotiation mechanisms, for example combinatorial auctions. An important theme that emerges from the literature is the centrality of design decisions about matching the domain of goods over which a mechanism operates to the domain over which agents have preferences. When the match is imperfect (as is almost inevitable), the market game induced by the mechanism is analytically intractable, and the literature provides an incomplete characterization of rational bidding policies. A review of the literature suggests that much of our existing knowledge comes from computational simulations, including controlled studies of abstract market designs (e.g., simultaneous ascending auctions), and research tournaments comparing agent strategies in a variety of market scenarios. An empirical game-theoretic methodology combines the advantages of simulation, agent-based modeling, and statistical and game-theoretic analysis.},
  keywords = {automated markets,computational markets,mechanism design,trading agents},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Automated Markets and Trading Agents_2006_MacKie-Mason et al.pdf;/home/baldoinov/Zotero/storage/MMFM2DB8/S1574002105020289.html}
}

@article{AutomatedMechanismDesign_2016_NarasimhanEtAl,
  title = {Automated {{Mechanism Design}} without {{Money}} via {{Machine Learning}}},
  author = {Narasimhan, Harikrishna and Agarwal, Shivani and Parkes, David C},
  date = {2016},
  abstract = {We use statistical machine learning to develop methods for automatically designing mechanisms in domains without money. Our goal is to find a mechanism that best approximates a given target function subject to a design constraint such as strategy-proofness or stability. The proposed approach involves identifying a rich parametrized class of mechanisms that resemble discriminantbased multiclass classifiers, and relaxing the resulting search problem into an SVM-style surrogate optimization problem. We use this methodology to design strategy-proof mechanisms for social choice problems with single-peaked preferences, and stable mechanisms for two-sided matching problems. To the best of our knowledge, ours is the first automated approach for designing stable matching rules. Experiments on synthetic and real-world data confirm the usefulness of our methods.},
  langid = {english},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Automated Mechanism Design without Money via Machine Learning_2016_Narasimhan et al.pdf}
}

@online{AutomaticDifferentiationAlgorithms_2014_GunesBaydinPearlmutter,
  title = {Automatic {{Differentiation}} of {{Algorithms}} for {{Machine Learning}}},
  author = {Gunes Baydin, Atilim and Pearlmutter, Barak A.},
  date = {2014-04-01},
  doi = {10.48550/arXiv.1404.7456},
  url = {https://ui.adsabs.harvard.edu/abs/2014arXiv1404.7456G},
  urldate = {2024-04-15},
  abstract = {Automatic differentiation---the mechanical transformation of numeric computer programs to calculate derivatives efficiently and accurately---dates to the origin of the computer age. Reverse mode automatic differentiation both antedates and generalizes the method of backwards propagation of errors used in machine learning. Despite this, practitioners in a variety of fields, including machine learning, have been little influenced by automatic differentiation, and make scant use of available tools. Here we review the technique of automatic differentiation, describe its two main modes, and explain how it can benefit machine learning practitioners. To reach the widest possible audience our treatment assumes only elementary differential calculus, and does not assume any knowledge of linear algebra.},
  organization = {arXiv e-prints},
  pubstate = {prepublished},
  keywords = {65D25,68T05,68W30,Computer Science - Machine Learning,Computer Science - Symbolic Computation,G.1.4,I.2.6,Statistics - Machine Learning},
  annotation = {ADS Bibcode: 2014arXiv1404.7456G},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/machine-learning-w-pytorch-n-sklearn/Automatic Differentiation of Algorithms for Machine Learning_2014_Gunes Baydin et al.pdf}
}

@online{AutomatingDataAnnotation_2023_AUGEREAU,
  title = {Automating {{Data Annotation}} with {{Large Language Models}}},
  author = {AUGEREAU, Pierre-Sylvain},
  date = {2023-05-24T17:26:50},
  url = {https://medium.com/@ps.augereau/automating-data-annotation-with-large-language-models-6b2346f8c44d},
  urldate = {2024-03-01},
  abstract = {In my last article, I was talking about how Large Language Models (LLMs) can level up Named Entity Recognition (NER), now let’s dive into…},
  langid = {english},
  organization = {Medium},
  keywords = {notion},
  file = {/home/baldoinov/Zotero/storage/BD8ACWFU/automating-data-annotation-with-large-language-models-6b2346f8c44d.html}
}

@book{AvaliacaoEconomicaProjetos_2017_PeixotoEtAl,
  title = {Avaliação econômica de projetos sociais},
  author = {Peixoto, Betânia and Campos de Xavier Pinto, Cristine and Lima, Lycia and Nathan Foguel, Miguel and Paes de Barros, Ricardo},
  editor = {Aquino Menezes Filho, Naercio and Campos de Xavier Pinto, Cristine},
  date = {2017-11-10},
  publisher = {Fundação Itaú Social},
  isbn = {978-85-66932-31-7},
  langid = {portuguese},
  keywords = {Educação},
  file = {/home/baldoinov/baldoinov/PDFs/Livros/Avaliacao economica de projetos sociais_2017_Peixoto et al.pdf}
}

@book{AvaliacaoImpactoNa_2018_GertlerEtAl,
  title = {Avaliação de {{Impacto}} Na {{Prática}}},
  author = {Gertler, Paul J. and {Sebastián Martinez} and {Patrick Premand} and {Laura B. Rawlings} and {Christel M. J. Vermeersch}},
  date = {2018},
  publisher = {Banco Interamericano de Desenvolvimento e Banco Mundial},
  location = {Washington, DC},
  isbn = {978-1-4648-0889-0},
  annotation = {OCLC: 966870680},
  file = {/home/baldoinov/baldoinov/PDFs/Livros/Avaliacao de Impacto na Pratica_2018_Gertler et al.pdf}
}

@book{AvaliacaoPoliticasCiencia_2009_Galvao,
  title = {Avaliação De Políticas De Ciência, Tecnologia E Inovação: Diálogo Entre Experiências Internacionais E Nacionais},
  shorttitle = {Avaliação De Políticas De Ciência, Tecnologia E Inovação},
  editor = {Galvão, Antonio Carlos Filgueira},
  namea = {Velho, Léa and Souza-Paula, Maria Carlota De},
  nameatype = {collaborator},
  date = {2009-05-18},
  series = {Cadernos},
  publisher = {Cgee},
  isbn = {978-85-60755-10-3},
  langid = {portuguese},
  keywords = {Generalidades},
  file = {/home/baldoinov/baldoinov/PDFs/Livros/Avaliacao De Politicas De Ciencia, Tecnologia E Inovacao_2009_Galvao.pdf}
}

@book{AvessoPele_2020_TenorioNunes,
  title = {O avesso da pele},
  author = {Tenório, Jeferson and Nunes, Alceu Chiesorin},
  date = {2020-08-10},
  edition = {1ª edição},
  publisher = {Companhia das Letras},
  location = {São Paulo, SP},
  abstract = {Um romance sobre identidade e as complexas relações raciais, sobre violência e negritude, O avesso da pele é uma obra contundente no panorama da nova ficção literária brasileira.Vencedor do Prêmio Jabuti na categoria “Romance Literário”.~O avesso da pele é a história de Pedro, que, após a morte do pai, assassinado numa desastrosa abordagem policial, sai em busca de resgatar o passado da família e refazer os caminhos paternos. Com uma narrativa sensível e por vezes brutal, Jeferson Tenório traz à superfície um país marcado pelo racismo e por um sistema educacional falido, e um denso relato sobre as relações entre pais e filhos. O que está em jogo é a vida de um homem abalado pelas inevitáveis fraturas existenciais da sua condição de negro em um país racista, um processo de dor, de acerto de contas, mas também de redenção, superação e liberdade. Com habilidade incomum para conceber e estruturar personagens e de lidar com as complexidades e pequenas tragédias das relações familiares, Jeferson Tenório se consolida como uma das vozes mais potentes e estilisticamente corajosas da literatura brasileira contemporânea."Não é de graça que Tenório, além de autor premiado, é tão bem acolhido pelo público e pela crítica. Ele não faz turismo, safári social, na desgraça geral do país, não faz da crítica à desigualdade um truque, um atalho apelativo e barato, panfletário, para ter mais aceitação, reconhecimento. Estamos diante de um escritor que, correndo todos os riscos, sabe arquitetar uma boa trama e encantar o leitor. Por muitas vezes durante a leitura eu disse para mim mesmo: como ele consegue construir personagens tão reais e fáceis de serem amados? Eu agradeço, a literatura brasileira agradece." ― Paulo Scott"Através de um profundo mergulho em seus personagens, O avesso da pele consegue abordar as questões centrais da sociedade brasileira. E o mais potente nisso tudo é que, aqui, o real e as reflexões partem sempre de dentro pra fora." ― Geovani Martins * Leitura obrigatória do vestibular do ITA.},
  isbn = {978-85-359-3339-0},
  langid = {portuguese},
  keywords = {notion}
}

@software{Base_dos_Dados_Base_dos_Dados,
  title = {Base Dos {{Dados}}},
  author = {{Base dos Dados}},
  url = {https://github.com/basedosdados/mais}
}

@misc{basedosdados_acesso_oportunidades,
  title = {Projeto Acesso a Oportunidades},
  author = {{Base dos Dados}},
  date = {2022},
  url = {https://basedosdados.org/dataset/br-ipea-acesso-oportunidades?bdm_table=estatisticas_2019}
}

@misc{basedosdados_caged,
  title = {Cadastro Geral de Empregados e Desempregados ({{CAGED}})},
  author = {{Base dos Dados}},
  date = {2022},
  url = {https://basedosdados.org/dataset/br-me-caged?bdm_table=microdados_antigos}
}

@misc{basedosdados_indicadores_educacionais,
  title = {Indicadores Educacionais},
  author = {{Base dos Dados}},
  date = {2022},
  url = {https://basedosdados.org/dataset/br-inep-indicadores-educacionais?bdm_table=escola}
}

@misc{basedosdados_populacao,
  title = {População Brasileira},
  author = {{Base dos Dados}},
  date = {2022},
  url = {https://basedosdados.org/dataset/br-ibge-populacao?bdm_table=municipio}
}

@misc{basedosdados_siconfi,
  title = {Sistema de Informações Contábeis e Fiscais Do Setor Público Brasileiro (Siconfi)},
  author = {{Base dos Dados}},
  date = {2022},
  url = {https://basedosdados.org/dataset/br-me-siconfi?bdm_table=municipio_despesas_orcamentarias}
}

@misc{basedosdados_sim,
  title = {Sistema de Informações Sobre Mortalidade ({{SIM}})},
  author = {{Base dos Dados}},
  date = {2022},
  url = {https://basedosdados.org/dataset/br-ms-sim?bdm_table=microdados}
}

@inproceedings{BayesianDeepLearning_2020_WilsonIzmailov,
  title = {Bayesian Deep Learning and a Probabilistic Perspective of Generalization},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Wilson, Andrew Gordon and Izmailov, Pavel},
  date = {2020-12-06},
  series = {{{NIPS}}'20},
  pages = {4697--4708},
  publisher = {Curran Associates Inc.},
  location = {Red Hook, NY, USA},
  abstract = {The key distinguishing property of a Bayesian approach is marginalization, rather than using a single setting of weights. Bayesian marginalization can particularly improve the accuracy and calibration of modern deep neural networks, which are typically underspecified by the data, and can represent many compelling but different solutions. We show that deep ensembles provide an effective mechanism for approximate Bayesian marginalization, and propose a related approach that further improves the predictive distribution by marginalizing within basins of attraction, without significant overhead. We also investigate the prior over functions implied by a vague distribution over neural network weights, explaining the generalization properties of such models from a probabilistic perspective. From this perspective, we explain results that have been presented as mysterious and distinct to neural network generalization, such as the ability to fit images with random labels, and show that these results can be reproduced with Gaussian processes. We also show that Bayesian model averaging alleviates double descent, resulting in monotonic performance improvements with increased flexibility.},
  isbn = {978-1-71382-954-6},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Bayesian deep learning and a probabilistic perspective of generalization_2020_Wilson et al.pdf}
}

@inproceedings{BERTabaporuAssessingGenreSpecific_2023_CostaEtAl,
  title = {{{BERTabaporu}}: {{Assessing}} a {{Genre-Specific Language Model}} for {{Portuguese NLP}}},
  shorttitle = {{{BERTabaporu}}},
  booktitle = {Proceedings of the 14th {{International Conference}} on {{Recent Advances}} in {{Natural Language Processing}}},
  author = {Costa, Pablo Botton and Pavan, Matheus Camasmie and Santos, Wesley Ramos and Silva, Samuel Caetano and Paraboni, Ivandré},
  editor = {Mitkov, Ruslan and Angelova, Galia},
  date = {2023-09},
  pages = {217--223},
  publisher = {INCOMA Ltd., Shoumen, Bulgaria},
  location = {Varna, Bulgaria},
  url = {https://aclanthology.org/2023.ranlp-1.24},
  urldate = {2023-11-16},
  abstract = {Transformer-based language models such as Bidirectional Encoder Representations from Transformers (BERT) are now mainstream in the NLP field, but extensions to languages other than English, to new domains and/or to more specific text genres are still in demand. In this paper we introduced BERTabaporu, a BERT language model that has been pre-trained on Twitter data in the Brazilian Portuguese language. The model is shown to outperform the best-known general-purpose model for this language in three Twitter-related NLP tasks, making a potentially useful resource for Portuguese NLP in general.},
  eventtitle = {{{RANLP}} 2023},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/projeto-heiwa/BERTabaporu_2023_Costa et al.pdf}
}

@incollection{BERTimbauPretrainedBERT_2020_SouzaEtAl,
  title = {{{BERTimbau}}: {{Pretrained BERT Models}} for {{Brazilian Portuguese}}},
  shorttitle = {{{BERTimbau}}},
  author = {Souza, Fábio and Nogueira, Rodrigo and Lotufo, Roberto},
  date = {2020-10-01},
  pages = {403--417},
  doi = {10.1007/978-3-030-61377-8_28},
  abstract = {Recent advances in language representation using neural networks have made it viable to transfer the learned internal states of large pretrained language models (LMs) to downstream natural language processing (NLP) tasks. This transfer learning approach improves the overall performance on many tasks and is highly beneficial when labeled data is scarce, making pretrained LMs valuable resources specially for languages with few annotated training examples. In this work, we train BERT (Bidirectional Encoder Representations from Transformers) models for Brazilian Portuguese, which we nickname BERTimbau. We evaluate our models on three downstream NLP tasks: sentence textual similarity, recognizing textual entailment, and named entity recognition. Our models improve the state-of-the-art in all of these tasks, outperforming Multilingual BERT and confirming the effectiveness of large pretrained LMs for Portuguese. We release our models to the community hoping to provide strong baselines for future NLP research: https://github.com/neuralmind-ai/portuguese-bert.},
  isbn = {978-3-030-61376-1},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/projeto-heiwa/BERTimbau_2020_Souza et al.pdf}
}

@inproceedings{BERTPretrainingDeep_2019_DevlinEtAl,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  editor = {Burstein, Jill and Doran, Christy and Solorio, Thamar},
  date = {2019-06},
  pages = {4171--4186},
  publisher = {Association for Computational Linguistics},
  location = {Minneapolis, Minnesota},
  doi = {10.18653/v1/N19-1423},
  url = {https://aclanthology.org/N19-1423},
  urldate = {2024-03-02},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  eventtitle = {{{NAACL-HLT}} 2019},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/projeto-heiwa/BERT_2019_Devlin et al.pdf}
}

@thesis{BERTweetBRPreTrained_2023_Carneiro,
  type = {Dissertação de Mestrado},
  title = {{{BERTweet}}.{{BR}}: {{A Pre-Trained Language Model}} for {{Tweets}} in {{Portuguese}}.},
  author = {Carneiro, Fernando Pereira},
  date = {2023},
  institution = {Universidade Federal Fluminense},
  location = {Niterói}
}

@online{BetterFasterLarge_2024_GloeckleEtAl,
  title = {Better \& {{Faster Large Language Models}} via {{Multi-token Prediction}}},
  author = {Gloeckle, Fabian and Idrissi, Badr Youbi and Rozière, Baptiste and Lopez-Paz, David and Synnaeve, Gabriel},
  date = {2024-04-30},
  eprint = {2404.19737},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2404.19737},
  url = {http://arxiv.org/abs/2404.19737},
  urldate = {2024-05-03},
  abstract = {Large language models such as GPT and Llama are trained with a next-token prediction loss. In this work, we suggest that training language models to predict multiple future tokens at once results in higher sample efficiency. More specifically, at each position in the training corpus, we ask the model to predict the following n tokens using n independent output heads, operating on top of a shared model trunk. Considering multi-token prediction as an auxiliary training task, we measure improved downstream capabilities with no overhead in training time for both code and natural language models. The method is increasingly useful for larger model sizes, and keeps its appeal when training for multiple epochs. Gains are especially pronounced on generative benchmarks like coding, where our models consistently outperform strong baselines by several percentage points. Our 13B parameter models solves 12 \% more problems on HumanEval and 17 \% more on MBPP than comparable next-token models. Experiments on small algorithmic tasks demonstrate that multi-token prediction is favorable for the development of induction heads and algorithmic reasoning capabilities. As an additional benefit, models trained with 4-token prediction are up to 3 times faster at inference, even with large batch sizes.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Better & Faster Large Language Models via Multi-token Prediction_2024_Gloeckle et al.pdf;/home/baldoinov/Zotero/storage/MDQKWA8Q/2404.html}
}

@article{BiasesAISystems_2021_SrinivasanChander,
  title = {Biases in {{AI Systems}}: {{A}} Survey for Practitioners},
  shorttitle = {Biases in {{AI Systems}}},
  author = {Srinivasan, Ramya and Chander, Ajay},
  date = {2021-05-12},
  journaltitle = {Queue},
  shortjournal = {Queue},
  volume = {19},
  number = {2},
  pages = {Pages 10:45--Pages 10:64},
  issn = {1542-7730},
  doi = {10.1145/3466132.3466134},
  url = {https://dl.acm.org/doi/10.1145/3466132.3466134},
  urldate = {2023-10-22},
  abstract = {This article provides an organization of various kinds of biases that can occur in the AI pipeline starting from dataset creation and problem formulation to data analysis and evaluation. It highlights the challenges associated with the design of bias-mitigation strategies, and it outlines some best practices suggested by researchers. Finally, a set of guidelines is presented that could aid ML developers in identifying potential sources of bias, as well as avoiding the introduction of unwanted biases. The work is meant to serve as an educational resource for ML developers in handling and addressing issues related to bias in AI systems.},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Biases in AI Systems_2021_Srinivasan et al.pdf}
}

@book{BigBookSmall_2021_Sweigart,
  title = {The {{Big Book}} of {{Small Python Projects}}: 81 {{Easy Practice Programs}}},
  shorttitle = {The {{Big Book}} of {{Small Python Projects}}},
  author = {Sweigart, Al},
  date = {2021-06-25},
  publisher = {No Starch Press},
  location = {San Francisco},
  abstract = {Best-selling author Al Sweigart shows you how to easily build over 80 fun programs with minimal code and maximum creativity.If you’ve mastered basic Python syntax and you’re ready to start writing programs, you’ll find The Big Book of Small Python Projects both enlightening and fun. This collection of 81 Python projects will have you making digital art, games, animations, counting pro- grams, and more right away. Once you see how the code works, you’ll practice re-creating the programs and experiment by adding your own custom touches.These simple, text-based programs are 256 lines of code or less. And whether it’s a vintage screensaver, a snail-racing game, a clickbait headline generator, or animated strands of DNA, each project is designed to be self-contained so you can easily share it online.You’ll create:• Hangman, Blackjack, and other games to play against your friends or the computer• Simulations of a forest fire, a million dice rolls, and a Japanese abacus• Animations like a virtual fish tank, a rotating cube, and a bouncing DVD logo screensaver• A first-person 3D maze game• Encryption programs that use ciphers like ROT13 and Vigenère to conceal textIf you’re tired of standard step-by-step tutorials, you’ll love the learn-by-doing approach of The Big Book of Small Python Projects. It’s proof that good things come in small programs!},
  isbn = {978-1-71850-124-9},
  langid = {english},
  pagetotal = {432}
}

@article{BigDataNew_2014_Varian,
  title = {Big {{Data}}: {{New Tricks}} for {{Econometrics}}},
  shorttitle = {Big {{Data}}},
  author = {Varian, Hal R.},
  date = {2014-05},
  journaltitle = {Journal of Economic Perspectives},
  volume = {28},
  number = {2},
  pages = {3--28},
  issn = {0895-3309},
  doi = {10.1257/jep.28.2.3},
  url = {https://www.aeaweb.org/articles?id=10.1257/jep.28.2.3},
  urldate = {2023-04-13},
  abstract = {Computers are now involved in many economic transactions and can capture data associated with these transactions, which can then be manipulated and analyzed. Conventional statistical and econometric techniques such as regression often work well, but there are issues unique to big datasets that may require different tools. First, the sheer size of the data involved may require more powerful data manipulation tools. Second, we may have more potential predictors than appropriate for estimation, so we need to do some kind of variable selection. Third, large datasets may allow for more flexible relationships than simple linear models. Machine learning techniques such as decision trees, support vector machines, neural nets, deep learning, and so on may allow for more effective ways to model complex relationships. In this essay, I will describe a few of these tools for manipulating and analyzing big data. I believe that these methods have a lot to offer and should be more widely known and used by economists.},
  langid = {english},
  keywords = {Modeling with Large Data Sets,notion},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Big Data_2014_Varian.pdf}
}

@online{BlackScholesModel_2016_MartinHaugh,
  title = {The {{Black Scholes Model}}},
  author = {{Martin Haugh}},
  date = {2016},
  url = {https://www.columbia.edu/~mh2078/FoundationsFE/BlackScholes.pdf},
  urldate = {2024-07-21},
  pubstate = {prepublished},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/The Black Scholes Model_2016_Martin Haugh.pdf}
}

@incollection{bleeker_nitrogen_2011,
  title = {Nitrogen Flows from {{European}} Regional Watersheds to Coastal Marine Waters},
  booktitle = {The European Nitrogen Assessment: {{Sources}}, Effects and Policy Perspectives},
  author = {Billen, Gilles and Silvestre, Marie and Grizzetti, Bruna and Leip, Adrian and Garnier, Josette and Voss, Maren and Howarth, Robert and Bouraoui, Fayçal and Lepistö, Ahti and Kortelainen, Pirkko and Johnes, Penny and Curtis, Chris and Humborg, Christoph and Smedberg, Erik and Kaste, Øyvind and Ganeshram, Raja and Beusen, Arthur and Lancelot, Christiane},
  editor = {Bleeker, Albert and Grizzetti, Bruna and Howard, Clare M. and Billen, Gilles and family=Grinsven, given=Hans, prefix=van, useprefix=true and Erisman, Jan Willem and Sutton, Mark A. and Grennfelt, Peringe},
  date = {2011},
  pages = {271--297},
  publisher = {Cambridge University Press},
  location = {Cambridge},
  doi = {10.1017/CBO9780511976988.016},
  url = {https://www.cambridge.org/core/books/european-nitrogen-assessment/nitrogen-flows-from-european-regional-watersheds-to-coastal-marine-waters/D90A89569A26F2D11CBA2DA0449EB6CA},
  urldate = {2023-12-11},
  abstract = {Executive summaryNature of the problemMost regional watersheds in Europe constitute managed human territories importing large amounts of new reactive nitrogen.As a consequence, groundwater, surface freshwater and coastal seawater are undergoing severe nitrogen contamination and/or eutrophication problems.ApproachesA comprehensive evaluation of net anthropogenic inputs of reactive nitrogen (NANI) through atmospheric deposition, crop N fixation, fertiliser use and import of food and feed has been carried out for all European watersheds. A database on N, P and Si fluxes delivered at the basin outlets has been assembled.A number of modelling approaches based on either statistical regression analysis or mechanistic description of the processes involved in nitrogen transfer and transformations have been developed for relating N inputs to watersheds to outputs into coastal marine ecosystems.Key findings/state of knowledgeThroughout Europe, NANI represents 3700 kgN/km²/yr (range, 0–8400 depending on the watershed), i.e. five times the background rate of natural N2 fixation.A mean of approximately 78\% of NANI does not reach the basin outlet, but instead is stored (in soils, sediments or ground water) or eliminated to the atmosphere as reactive N forms or as N2.N delivery to the European marine coastal zone totals 810 kgN/km²/yr (range, 200–4000 depending on the watershed), about four times the natural background. In areas of limited availability of silica, these inputs cause harmful algal blooms.},
  isbn = {978-1-107-00612-6}
}

@article{BreakingTraditionalSurvey_2023_ChenEtAl,
  title = {Breaking the Traditional: A Survey of Algorithmic Mechanism Design Applied to Economic and Complex Environments},
  shorttitle = {Breaking the Traditional},
  author = {Chen, Qian and Wang, Xuan and Jiang, Zoe Lin and Wu, Yulin and Li, Huale and Cui, Lei and Sun, Xiaozhen},
  date = {2023-08-01},
  journaltitle = {Neural Computing and Applications},
  shortjournal = {Neural Comput \& Applic},
  volume = {35},
  number = {22},
  pages = {16193--16222},
  issn = {1433-3058},
  doi = {10.1007/s00521-023-08647-1},
  url = {https://doi.org/10.1007/s00521-023-08647-1},
  urldate = {2023-08-18},
  abstract = {The mechanism design theory can be applied not only in the economy but also in many fields, such as politics and military affairs, which has important practical and strategic significance for countries in the period of system innovation and transformation. As Nobel Laureate Paul said, the complexity of the real economy makes it difficult for “Unorganized Markets” to ensure supply-demand balance and the efficient allocation of resources. When traditional economic theory cannot explain and calculate the complex scenes of reality, we require a high-performance computing solution based on traditional theory to evaluate the mechanisms, meanwhile, get better social welfare. The mechanism design theory is undoubtedly the best option. Different from other existing works, which are based on the theoretical exploration of optimal solutions or single perspective analysis of scenarios, this paper focuses on the more real and complex markets. It explores to discover the common difficulties and feasible solutions for the applications. Firstly, we review the history of traditional mechanism design and algorithm mechanism design. Subsequently, we present the main challenges in designing the actual data-driven market mechanisms, including the inherent challenges in the mechanism design theory, the challenges brought by new markets and the common challenges faced by both. In addition, we also comb and discuss theoretical support and computer-aided methods in detail. This paper guides cross-disciplinary researchers who wish to explore the resource allocation problem in real markets for the first time and offers a different perspective for researchers struggling to solve complex social problems. Finally, we discuss and propose new ideas and look to the future.},
  langid = {english},
  keywords = {Algorithmic mechanism design,Incentive mechanism,notion,Online advertising,Privacy preserving},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Breaking the traditional_2023_Chen et al.pdf}
}

@inproceedings{BrWaCCorpusNew_2018_WagnerFilhoEtAl,
  title = {The {{brWaC Corpus}}: {{A New Open Resource}} for {{Brazilian Portuguese}}},
  shorttitle = {The {{brWaC Corpus}}},
  booktitle = {Proceedings of the {{Eleventh International Conference}} on {{Language Resources}} and {{Evaluation}} ({{LREC}} 2018)},
  author = {Wagner Filho, Jorge A. and Wilkens, Rodrigo and Idiart, Marco and Villavicencio, Aline},
  editor = {Calzolari, Nicoletta and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Hasida, Koiti and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios and Tokunaga, Takenobu},
  date = {2018-05},
  publisher = {European Language Resources Association (ELRA)},
  location = {Miyazaki, Japan},
  url = {https://aclanthology.org/L18-1686},
  urldate = {2024-09-01},
  eventtitle = {{{LREC}} 2018},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/projeto-heiwa/The brWaC Corpus_2018_Wagner Filho et al.pdf}
}

@online{BuildingFoundationDataDriven_2021_TrottEtAl,
  title = {Building a {{Foundation}} for {{Data-Driven}}, {{Interpretable}}, and {{Robust Policy Design}} Using the {{AI Economist}}},
  author = {Trott, Alexander and Srinivasa, Sunil and family=Wal, given=Douwe, prefix=van der, useprefix=true and Haneuse, Sebastien and Zheng, Stephan},
  date = {2021-08-05},
  eprint = {2108.02904},
  eprinttype = {arXiv},
  eprintclass = {cs, econ, q-fin},
  doi = {10.48550/arXiv.2108.02904},
  url = {http://arxiv.org/abs/2108.02904},
  urldate = {2024-05-05},
  abstract = {Optimizing economic and public policy is critical to address socioeconomic issues and trade-offs, e.g., improving equality, productivity, or wellness, and poses a complex mechanism design problem. A policy designer needs to consider multiple objectives, policy levers, and behavioral responses from strategic actors who optimize for their individual objectives. Moreover, real-world policies should be explainable and robust to simulation-to-reality gaps, e.g., due to calibration issues. Existing approaches are often limited to a narrow set of policy levers or objectives that are hard to measure, do not yield explicit optimal policies, or do not consider strategic behavior, for example. Hence, it remains challenging to optimize policy in real-world scenarios. Here we show that the AI Economist framework enables effective, flexible, and interpretable policy design using two-level reinforcement learning (RL) and data-driven simulations. We validate our framework on optimizing the stringency of US state policies and Federal subsidies during a pandemic, e.g., COVID-19, using a simulation fitted to real data. We find that log-linear policies trained using RL significantly improve social welfare, based on both public health and economic outcomes, compared to past outcomes. Their behavior can be explained, e.g., well-performing policies respond strongly to changes in recovery and vaccination rates. They are also robust to calibration errors, e.g., infection rates that are over or underestimated. As of yet, real-world policymaking has not seen adoption of machine learning methods at large, including RL and AI-driven simulations. Our results show the potential of AI to guide policy design and improve social welfare amidst the complexity of the real world.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems,Economics - Econometrics,Economics - General Economics},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Building a Foundation for Data-Driven, Interpretable, and Robust Policy Design_2021_Trott et al.pdf;/home/baldoinov/Zotero/storage/FP68F8P2/2108.html}
}

@online{BuildingLargescaleTransactional_2020_Uber,
  title = {Building a {{Large-scale Transactional Data Lake}} at {{Uber Using Apache Hudi}}},
  author = {{Uber}},
  date = {2020-06-09T09:00:51+00:00},
  url = {https://www.uber.com/en-SK/blog/apache-hudi-graduation/},
  urldate = {2025-03-09},
  abstract = {The Apache Hudi team at Uber reflects on the open source project's history as it graduates to a Top Level Project under the Apache Software Foundation.},
  langid = {english},
  organization = {Uber Blog},
  file = {/home/baldoinov/Zotero/storage/8GDLLHJT/apache-hudi-graduation.html}
}

@book{BuildingMachineLearning_2020_HapkeNelson,
  title = {Building Machine Learning Pipelines: Automating Model Life Cycles with Tensorflow},
  shorttitle = {Building Machine Learning Pipelines},
  author = {Hapke, Hannes and Nelson, Catherine},
  date = {2020},
  edition = {1ª edição},
  publisher = {O'Reilly Media},
  location = {Sebastopol, California},
  abstract = {Companies are spending billions on machine learning projects, but it's money wasted if the models can't be deployed effectively. In this practical guide, Hannes Hapke and Catherine Nelson walk you through the steps of automating a machine learning pipeline using the TensorFlow ecosystem. You'll learn the techniques and tools that will cut deployment time from days to minutes, so that you can focus on developing new models rather than maintaining legacy systems.  Data scientists, machine learning engineers, and DevOps engineers will discover how to go beyond model development to successfully productize their data science projects, while managers will better understand the role they play in helping to accelerate these projects. Understand the steps to build a machine learning pipeline Build your pipeline using components from TensorFlow Extended Orchestrate your machine learning pipeline with Apache Beam, Apache Airflow, and Kubeflow Pipelines Work with data using TensorFlow Data Validation and TensorFlow Transform Analyze a model in detail using TensorFlow Model Analysis Examine fairness and bias in your model performance Deploy models with TensorFlow Serving or TensorFlow Lite for mobile devices Learn privacy-preserving machine learning techniques},
  isbn = {978-1-4920-5319-4},
  langid = {Inglês}
}

@inproceedings{BuildingSentimentCorpus_2018_BrumVolpeNunes,
  title = {Building a {{Sentiment Corpus}} of {{Tweets}} in {{Brazilian Portuguese}}},
  booktitle = {Proceedings of the {{Eleventh International Conference}} on {{Language Resources}} and {{Evaluation}} ({{LREC}} 2018)},
  author = {Brum, Henrico and Volpe Nunes, Maria das Graças},
  editor = {Calzolari, Nicoletta and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Hasida, Koiti and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios and Tokunaga, Takenobu},
  date = {2018-05},
  publisher = {European Language Resources Association (ELRA)},
  location = {Miyazaki, Japan},
  url = {https://aclanthology.org/L18-1658},
  urldate = {2024-08-28},
  eventtitle = {{{LREC}} 2018},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/projeto-heiwa/Building a Sentiment Corpus of Tweets in Brazilian Portuguese_2018_Brum et al.pdf}
}

@book{CapitalTwentyFirstCentury_2014_PikettyGoldhammer,
  title = {Capital in the Twenty-First Century},
  author = {Piketty, Thomas and Goldhammer, Arthur},
  date = {2014},
  edition = {Illustrated edição},
  publisher = {Belknap Press},
  location = {Cambridge, Massachusetts London},
  abstract = {What are the grand dynamics that drive the accumulation and distribution of capital? Questions about the long-term evolution of inequality, the concentration of wealth, and the prospects for economic growth lie at the heart of political economy. But satisfactory answers have been hard to find for lack of adequate data and clear guiding theories. In Capital in the Twenty-First Century, Thomas Piketty analyzes a unique collection of data from twenty countries, ranging as far back as the eighteenth century, to uncover key economic and social patterns. His findings will transform debate and set the agenda for the next generation of thought about wealth and inequality. Piketty shows that modern economic growth and the diffusion of knowledge have allowed us to avoid inequalities on the apocalyptic scale predicted by Karl Marx. But we have not modified the deep structures of capital and inequality as much as we thought in the optimistic decades following World War II. The main driver of inequality—the tendency of returns on capital to exceed the rate of economic growth—today threatens to generate extreme inequalities that stir discontent and undermine democratic values. But economic trends are not acts of God. Political action has curbed dangerous inequalities in the past, Piketty says, and may do so again. A work of extraordinary ambition, originality, and rigor, Capital in the Twenty-First Century reorients our understanding of economic history and confronts us with sobering lessons for today.},
  isbn = {978-0-674-43000-6},
  langid = {Inglês}
}

@article{CarolinaMethodologyBuilding__SturzenekerMorales,
  title = {Carolina’s {{Methodology}}: Building a Large Corpus with Provenance and Typology Information},
  author = {Sturzeneker, Mariana Lourenço and Morales, Maria Clara Ramos},
  abstract = {This paper presents the salient aspects of WaC-wiPT methodology, developed for the construction of the Carolina Open Corpus for Linguistics and Artificial Intelligence, a large corpus for contemporary Brazilian Portuguese. Both the corpus and the methodology are under development at the Center for Artificial Intelligence of the University of São Paulo. This paper describes the paths we took this far into the making of the Carolina Corpus, presents its current state and discloses the future agenda of the project.},
  langid = {english},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Carolina’s Methodology_Sturzeneker et al.pdf}
}

@article{CategoricalVariablesRegression_2012_Alkharusi,
  title = {Categorical {{Variables}} in {{Regression Analysis}}: {{A Comparison}} of {{Dummy}} and {{Effect Coding}}},
  shorttitle = {Categorical {{Variables}} in {{Regression Analysis}}},
  author = {Alkharusi, Hussain},
  date = {2012-06-17},
  journaltitle = {International Journal of Education},
  volume = {4},
  number = {2},
  pages = {202--210},
  issn = {1948-5476},
  doi = {10.5296/ije.v4i2.1962},
  url = {https://www.macrothink.org/journal/index.php/ije/article/view/1962},
  urldate = {2023-10-25},
  abstract = {The use of categorical variables in regression involves the application of coding methods. The purpose of this paper is to describe how categorical independent variables can be incorporated into regression by virtue of two coding methods: dummy and effect coding. The paper discusses the uses, interpretations, and underlying assumptions of each method. In general, overall results of the regression are unaffected by the methods used for coding the categorical independent variables. In any of the methods, the analysis tests whether group membership is related to the dependent variables. Both methods yield identical R2 and F. However, the interpretations of the intercept and regression coefficients depend on what coding method has been applied and whether the groups have equal sample sizes.},
  issue = {2},
  langid = {english},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Categorical Variables in Regression Analysis_2012_Alkharusi.pdf}
}

@article{CausalDecisionTrees_2017_LiEtAl,
  title = {Causal {{Decision Trees}}},
  author = {Li, Jiuyong and Ma, Saisai and Le, Thuc Duy and Liu, Lin and Liu, Jixue},
  date = {2017-02-01},
  journaltitle = {IEEE Transactions on Knowledge and Data Engineering},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  volume = {29},
  number = {2},
  eprint = {1508.03812},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {257--271},
  issn = {1041-4347},
  doi = {10.1109/TKDE.2016.2619350},
  url = {http://arxiv.org/abs/1508.03812},
  urldate = {2025-04-14},
  abstract = {Uncovering causal relationships in data is a major objective of data analytics. Causal relationships are normally discovered with designed experiments, e.g. randomised controlled trials, which, however are expensive or infeasible to be conducted in many cases. Causal relationships can also be found using some well designed observational studies, but they require domain experts' knowledge and the process is normally time consuming. Hence there is a need for scalable and automated methods for causal relationship exploration in data. Classification methods are fast and they could be practical substitutes for finding causal signals in data. However, classification methods are not designed for causal discovery and a classification method may find false causal signals and miss the true ones. In this paper, we develop a causal decision tree where nodes have causal interpretations. Our method follows a well established causal inference framework and makes use of a classic statistical test. The method is practical for finding causal signals in large data sets.},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Causal Decision Trees_2017_Li et al.pdf;/home/baldoinov/Zotero/storage/QALX9LWM/1508.html}
}

@book{CausalInferenceMixtape_2021_Cunningham,
  title = {Causal Inference: The Mixtape},
  shorttitle = {Causal Inference},
  author = {Cunningham, Scott},
  date = {2021-01-26},
  publisher = {Yale University Press},
  location = {New Haven London},
  abstract = {An accessible, contemporary introduction to the methods for determining cause and effect in the social sciences"Causation versus correlation has been the basis of arguments--economic and otherwise--since the beginning of time. Causal Inference: The Mixtape uses legit real-world examples that I found genuinely thought-provoking. It's rare that a book prompts readers to expand their outlook; this one did for me."--Marvin Young (Young MC) Causal inference encompasses the tools that allow social scientists to determine what causes what. In a messy world, causal inference is what helps establish the causes and effects of the actions being studied--for example, the impact (or lack thereof) of increases in the minimum wage on employment, the effects of early childhood education on incarceration later in life, or the influence on economic growth of introducing malaria nets in developing regions. Scott Cunningham introduces students and practitioners to the methods necessary to arrive at meaningful answers to the questions of causation, using a range of modeling techniques and coding instructions for both the R and the Stata programming languages.},
  isbn = {978-0-300-25168-5},
  langid = {Inglês}
}

@online{ChainofThoughtPromptingElicits_2023_WeiEtAl,
  title = {Chain-of-{{Thought Prompting Elicits Reasoning}} in {{Large Language Models}}},
  author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  date = {2023-01-10},
  eprint = {2201.11903},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2201.11903},
  url = {http://arxiv.org/abs/2201.11903},
  urldate = {2024-03-02},
  abstract = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,notion},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Chain-of-Thought Prompting Elicits Reasoning in Large Language Models_2023_Wei et al.pdf;/home/baldoinov/Zotero/storage/APHVPYD8/2201.html}
}

@online{ChainofThoughtReasoningPrompting_2024_WangZhou,
  title = {Chain-of-{{Thought Reasoning Without Prompting}}},
  author = {Wang, Xuezhi and Zhou, Denny},
  date = {2024-02-15},
  eprint = {2402.10200},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2402.10200},
  url = {http://arxiv.org/abs/2402.10200},
  urldate = {2024-02-21},
  abstract = {In enhancing the reasoning capabilities of large language models (LLMs), prior research primarily focuses on specific prompting techniques such as few-shot or zero-shot chain-of-thought (CoT) prompting. These methods, while effective, often involve manually intensive prompt engineering. Our study takes a novel approach by asking: Can LLMs reason effectively without prompting? Our findings reveal that, intriguingly, CoT reasoning paths can be elicited from pre-trained LLMs by simply altering the \textbackslash textit\{decoding\} process. Rather than conventional greedy decoding, we investigate the top-\$k\$ alternative tokens, uncovering that CoT paths are frequently inherent in these sequences. This approach not only bypasses the confounders of prompting but also allows us to assess the LLMs' \textbackslash textit\{intrinsic\} reasoning abilities. Moreover, we observe that the presence of a CoT in the decoding path correlates with a higher confidence in the model's decoded answer. This confidence metric effectively differentiates between CoT and non-CoT paths. Extensive empirical studies on various reasoning benchmarks show that the proposed CoT-decoding substantially outperforms the standard greedy decoding.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,notion},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Chain-of-Thought Reasoning Without Prompting_2024_Wang et al.pdf;/home/baldoinov/Zotero/storage/ZYXUYPAK/2402.html}
}

@article{challender_2014,
  title = {Poaching Is More than an Enforcement Problem},
  author = {Challender, D.W. and MacMillan, D.C.},
  date = {2014},
  journaltitle = {Conservation Letters},
  volume = {7},
  number = {5},
  pages = {484--494}
}

@online{ChallengesApplicationsLarge_2023_KaddourEtAl,
  title = {Challenges and {{Applications}} of {{Large Language Models}}},
  author = {Kaddour, Jean and Harris, Joshua and Mozes, Maximilian and Bradley, Herbie and Raileanu, Roberta and McHardy, Robert},
  date = {2023-07-19},
  eprint = {2307.10169},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2307.10169},
  url = {http://arxiv.org/abs/2307.10169},
  urldate = {2024-02-21},
  abstract = {Large Language Models (LLMs) went from non-existent to ubiquitous in the machine learning discourse within a few years. Due to the fast pace of the field, it is difficult to identify the remaining challenges and already fruitful application areas. In this paper, we aim to establish a systematic set of open problems and application successes so that ML researchers can comprehend the field's current state more quickly and become productive.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,notion},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Challenges and Applications of Large Language Models_2023_Kaddour et al.pdf;/home/baldoinov/Zotero/storage/R49LVA6L/2307.html}
}

@online{ChameleonMixedModalEarlyFusion_2024_ChameleonTeam,
  title = {Chameleon: {{Mixed-Modal Early-Fusion Foundation Models}}},
  shorttitle = {Chameleon},
  author = {Chameleon Team},
  date = {2024-05-16},
  eprint = {2405.09818},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2405.09818},
  urldate = {2024-05-23},
  abstract = {We present Chameleon, a family of early-fusion token-based mixed-modal models capable of understanding and generating images and text in any arbitrary sequence. We outline a stable training approach from inception, an alignment recipe, and an architectural parameterization tailored for the early-fusion, token-based, mixed-modal setting. The models are evaluated on a comprehensive range of tasks, including visual question answering, image captioning, text generation, image generation, and long-form mixed modal generation. Chameleon demonstrates broad and general capabilities, including state-of-the-art performance in image captioning tasks, outperforms Llama-2 in text-only tasks while being competitive with models such as Mixtral 8x7B and Gemini-Pro, and performs non-trivial image generation, all in a single model. It also matches or exceeds the performance of much larger models, including Gemini Pro and GPT-4V, according to human judgments on a new long-form mixed-modal generation evaluation, where either the prompt or outputs contain mixed sequences of both images and text. Chameleon marks a significant step forward in a unified modeling of full multimodal documents.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Chameleon_2024_Chameleon Team.pdf}
}

@article{ChangingViewsPersuasion_2020_DuttaEtAl,
  title = {Changing Views: {{Persuasion}} Modeling and Argument Extraction from Online Discussions},
  shorttitle = {Changing Views},
  author = {Dutta, Subhabrata and Das, Dipankar and Chakraborty, Tanmoy},
  date = {2020-03-01},
  journaltitle = {Information Processing \& Management},
  shortjournal = {Information Processing \& Management},
  volume = {57},
  number = {2},
  pages = {102085},
  issn = {0306-4573},
  doi = {10.1016/j.ipm.2019.102085},
  url = {https://www.sciencedirect.com/science/article/pii/S0306457319301165},
  urldate = {2023-12-14},
  abstract = {Persuasion and argumentation are possibly among the most complex examples of the interplay between multiple human subjects. With the advent of the Internet, online forums provide wide platforms for people to share their opinions and reasonings around various diverse topics. In this work, we attempt to model persuasive interaction between users on Reddit, a popular online discussion forum. We propose a deep LSTM model to classify whether a conversation leads to a successful persuasion or not, and use this model to predict whether a certain chain of arguments can lead to persuasion. While learning persuasion dynamics, our model tends to identify argument facets implicitly, using an attention mechanism. We also propose a semi-supervised approach to extract argumentative components from discussion threads. Both these models provide useful insight into how people engage in argumentation on online discussion forums.},
  keywords = {Argument mining,Attention mechanism,Deep LSTM,Dynamic time warping distance,notion,Persuasion modeling,Social media},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/projeto-heiwa-revisao-de-literatura/Changing views_2020_Dutta et al.pdf}
}

@incollection{Chapter10Estimating_2021_Kissell,
  title = {Chapter 10 - {{Estimating I-Star Market Impact Model Parameters}}},
  booktitle = {Algorithmic {{Trading Methods}} ({{Second Edition}})},
  author = {Kissell, Robert L.},
  editor = {Kissell, Robert L.},
  date = {2021-01-01},
  pages = {233--267},
  publisher = {Academic Press},
  doi = {10.1016/B978-0-12-815630-8.00010-7},
  url = {https://www.sciencedirect.com/science/article/pii/B9780128156308000107},
  urldate = {2025-04-21},
  isbn = {978-0-12-815630-8}
}

@incollection{Chapter11Risk_2021_Kissell,
  title = {Chapter 11 - {{Risk}}, {{Volatility}}, and {{Factor Models}}},
  booktitle = {Algorithmic {{Trading Methods}} ({{Second Edition}})},
  author = {Kissell, Robert L.},
  editor = {Kissell, Robert L.},
  date = {2021-01-01},
  pages = {269--299},
  publisher = {Academic Press},
  doi = {10.1016/B978-0-12-815630-8.00011-9},
  url = {https://www.sciencedirect.com/science/article/pii/B9780128156308000119},
  urldate = {2025-04-21},
  isbn = {978-0-12-815630-8}
}

@incollection{Chapter12Volume_2021_Kissell,
  title = {Chapter 12 - {{Volume Forecasting Techniques}}},
  booktitle = {Algorithmic {{Trading Methods}} ({{Second Edition}})},
  author = {Kissell, Robert L.},
  editor = {Kissell, Robert L.},
  date = {2021-01-01},
  pages = {301--322},
  publisher = {Academic Press},
  doi = {10.1016/B978-0-12-815630-8.00012-0},
  url = {https://www.sciencedirect.com/science/article/pii/B9780128156308000120},
  urldate = {2025-04-21},
  isbn = {978-0-12-815630-8}
}

@incollection{Chapter13Algorithmic_2021_Kissell,
  title = {Chapter 13 - {{Algorithmic Decision-Making Framework}}},
  booktitle = {Algorithmic {{Trading Methods}} ({{Second Edition}})},
  author = {Kissell, Robert L.},
  editor = {Kissell, Robert L.},
  date = {2021-01-01},
  pages = {323--348},
  publisher = {Academic Press},
  doi = {10.1016/B978-0-12-815630-8.00013-2},
  url = {https://www.sciencedirect.com/science/article/pii/B9780128156308000132},
  urldate = {2025-04-21},
  isbn = {978-0-12-815630-8}
}

@incollection{Chapter14Portfolio_2021_Kissell,
  title = {Chapter 14 - {{Portfolio Algorithms}} and {{Trade Schedule Optimization}}},
  booktitle = {Algorithmic {{Trading Methods}} ({{Second Edition}})},
  author = {Kissell, Robert L.},
  editor = {Kissell, Robert L.},
  date = {2021-01-01},
  pages = {349--374},
  publisher = {Academic Press},
  doi = {10.1016/B978-0-12-815630-8.00014-4},
  url = {https://www.sciencedirect.com/science/article/pii/B9780128156308000144},
  urldate = {2025-04-21},
  isbn = {978-0-12-815630-8}
}

@incollection{Chapter15Advanced_2021_Kissell,
  title = {Chapter 15 - {{Advanced Algorithmic Modeling Techniques}}},
  booktitle = {Algorithmic {{Trading Methods}} ({{Second Edition}})},
  author = {Kissell, Robert L.},
  editor = {Kissell, Robert L.},
  date = {2021-01-01},
  pages = {375--403},
  publisher = {Academic Press},
  doi = {10.1016/B978-0-12-815630-8.00015-6},
  url = {https://www.sciencedirect.com/science/article/pii/B9780128156308000156},
  urldate = {2025-04-21},
  isbn = {978-0-12-815630-8}
}

@incollection{Chapter16Decoding_2021_Kissell,
  title = {Chapter 16 - {{Decoding}} and {{Reverse Engineering Broker Models}} with {{Machine Learning Techniques}}},
  booktitle = {Algorithmic {{Trading Methods}} ({{Second Edition}})},
  author = {Kissell, Robert L.},
  editor = {Kissell, Robert L.},
  date = {2021-01-01},
  pages = {405--428},
  publisher = {Academic Press},
  doi = {10.1016/B978-0-12-815630-8.00016-8},
  url = {https://www.sciencedirect.com/science/article/pii/B9780128156308000168},
  urldate = {2025-04-21},
  isbn = {978-0-12-815630-8}
}

@incollection{Chapter17Portfolio_2021_Kissell,
  title = {Chapter 17 - {{Portfolio Construction}} with {{Transaction Cost Analysis}}},
  booktitle = {Algorithmic {{Trading Methods}} ({{Second Edition}})},
  author = {Kissell, Robert L.},
  editor = {Kissell, Robert L.},
  date = {2021-01-01},
  pages = {429--467},
  publisher = {Academic Press},
  doi = {10.1016/B978-0-12-815630-8.00017-X},
  url = {https://www.sciencedirect.com/science/article/pii/B978012815630800017X},
  urldate = {2025-04-21},
  isbn = {978-0-12-815630-8}
}

@incollection{Chapter18Quantitative_2021_Kissell,
  title = {Chapter 18 - {{Quantitative Analysis}} with {{TCA}}},
  booktitle = {Algorithmic {{Trading Methods}} ({{Second Edition}})},
  author = {Kissell, Robert L.},
  editor = {Kissell, Robert L.},
  date = {2021-01-01},
  pages = {469--518},
  publisher = {Academic Press},
  doi = {10.1016/B978-0-12-815630-8.00018-1},
  url = {https://www.sciencedirect.com/science/article/pii/B9780128156308000181},
  urldate = {2025-04-21},
  isbn = {978-0-12-815630-8}
}

@incollection{Chapter19Machine_2021_Kissell,
  title = {Chapter 19 - {{Machine Learning}} and {{Trade Schedule Optimization}}},
  booktitle = {Algorithmic {{Trading Methods}} ({{Second Edition}})},
  author = {Kissell, Robert L.},
  editor = {Kissell, Robert L.},
  date = {2021-01-01},
  pages = {519--542},
  publisher = {Academic Press},
  doi = {10.1016/B978-0-12-815630-8.00019-3},
  url = {https://www.sciencedirect.com/science/article/pii/B9780128156308000193},
  urldate = {2025-04-21},
  isbn = {978-0-12-815630-8}
}

@incollection{Chapter1Introduction_2021_Kissell,
  title = {Chapter 1 - {{Introduction}}},
  booktitle = {Algorithmic {{Trading Methods}} ({{Second Edition}})},
  author = {Kissell, Robert L.},
  editor = {Kissell, Robert L.},
  date = {2021-01-01},
  pages = {1--21},
  publisher = {Academic Press},
  doi = {10.1016/B978-0-12-815630-8.00001-6},
  url = {https://www.sciencedirect.com/science/article/pii/B9780128156308000016},
  urldate = {2025-04-21},
  isbn = {978-0-12-815630-8}
}

@incollection{Chapter20TCA_2021_Kissell,
  title = {Chapter 20 - {{TCA Analysis Using MATLAB}}, {{Excel}}, and {{Python}}},
  booktitle = {Algorithmic {{Trading Methods}} ({{Second Edition}})},
  author = {Kissell, Robert L.},
  editor = {Kissell, Robert L.},
  date = {2021-01-01},
  pages = {543--558},
  publisher = {Academic Press},
  doi = {10.1016/B978-0-12-815630-8.00020-X},
  url = {https://www.sciencedirect.com/science/article/pii/B978012815630800020X},
  urldate = {2025-04-21},
  isbn = {978-0-12-815630-8}
}

@incollection{Chapter21Transaction_2021_Kissell,
  title = {Chapter 21 - {{Transaction Cost Analysis}} ({{TCA}}) {{Library}}},
  booktitle = {Algorithmic {{Trading Methods}} ({{Second Edition}})},
  author = {Kissell, Robert L.},
  editor = {Kissell, Robert L.},
  date = {2021-01-01},
  pages = {559--567},
  publisher = {Academic Press},
  doi = {10.1016/B978-0-12-815630-8.00021-1},
  url = {https://www.sciencedirect.com/science/article/pii/B9780128156308000211},
  urldate = {2025-04-21},
  isbn = {978-0-12-815630-8}
}

@incollection{Chapter2Algorithmic_2021_Kissell,
  title = {Chapter 2 - {{Algorithmic Trading}}},
  booktitle = {Algorithmic {{Trading Methods}} ({{Second Edition}})},
  author = {Kissell, Robert L.},
  editor = {Kissell, Robert L.},
  date = {2021-01-01},
  pages = {23--56},
  publisher = {Academic Press},
  doi = {10.1016/B978-0-12-815630-8.00002-8},
  url = {https://www.sciencedirect.com/science/article/pii/B9780128156308000028},
  urldate = {2025-04-21},
  isbn = {978-0-12-815630-8}
}

@incollection{Chapter3Transaction_2021_Kissell,
  title = {Chapter 3 - {{Transaction Costs}}},
  booktitle = {Algorithmic {{Trading Methods}} ({{Second Edition}})},
  author = {Kissell, Robert L.},
  editor = {Kissell, Robert L.},
  date = {2021-01-01},
  pages = {57--97},
  publisher = {Academic Press},
  doi = {10.1016/B978-0-12-815630-8.00003-X},
  url = {https://www.sciencedirect.com/science/article/pii/B978012815630800003X},
  urldate = {2025-04-21},
  isbn = {978-0-12-815630-8}
}

@incollection{Chapter4Market_2021_Kissell,
  title = {Chapter 4 - {{Market Impact Models}}},
  booktitle = {Algorithmic {{Trading Methods}} ({{Second Edition}})},
  author = {Kissell, Robert L.},
  editor = {Kissell, Robert L.},
  date = {2021-01-01},
  pages = {99--128},
  publisher = {Academic Press},
  doi = {10.1016/B978-0-12-815630-8.00004-1},
  url = {https://www.sciencedirect.com/science/article/pii/B9780128156308000041},
  urldate = {2025-04-21},
  isbn = {978-0-12-815630-8}
}

@incollection{Chapter5Probability_2021_Kissell,
  title = {Chapter 5 - {{Probability}} and {{Statistics}}},
  booktitle = {Algorithmic {{Trading Methods}} ({{Second Edition}})},
  author = {Kissell, Robert L.},
  editor = {Kissell, Robert L.},
  date = {2021-01-01},
  pages = {129--150},
  publisher = {Academic Press},
  doi = {10.1016/B978-0-12-815630-8.00005-3},
  url = {https://www.sciencedirect.com/science/article/pii/B9780128156308000053},
  urldate = {2025-04-21},
  isbn = {978-0-12-815630-8}
}

@incollection{Chapter6Linear_2021_Kissell,
  title = {Chapter 6 - {{Linear Regression Models}}},
  booktitle = {Algorithmic {{Trading Methods}} ({{Second Edition}})},
  author = {Kissell, Robert L.},
  editor = {Kissell, Robert L.},
  date = {2021-01-01},
  pages = {151--173},
  publisher = {Academic Press},
  doi = {10.1016/B978-0-12-815630-8.00006-5},
  url = {https://www.sciencedirect.com/science/article/pii/B9780128156308000065},
  urldate = {2025-04-21},
  isbn = {978-0-12-815630-8}
}

@incollection{Chapter7Probability_2021_Kissell,
  title = {Chapter 7 - {{Probability Models}}},
  booktitle = {Algorithmic {{Trading Methods}} ({{Second Edition}})},
  author = {Kissell, Robert L.},
  editor = {Kissell, Robert L.},
  date = {2021-01-01},
  pages = {175--195},
  publisher = {Academic Press},
  doi = {10.1016/B978-0-12-815630-8.00007-7},
  url = {https://www.sciencedirect.com/science/article/pii/B9780128156308000077},
  urldate = {2025-04-21},
  isbn = {978-0-12-815630-8}
}

@incollection{Chapter8Nonlinear_2021_Kissell,
  title = {Chapter 8 - {{Nonlinear Regression Models}}},
  booktitle = {Algorithmic {{Trading Methods}} ({{Second Edition}})},
  author = {Kissell, Robert L.},
  editor = {Kissell, Robert L.},
  date = {2021-01-01},
  pages = {197--219},
  publisher = {Academic Press},
  doi = {10.1016/B978-0-12-815630-8.00008-9},
  url = {https://www.sciencedirect.com/science/article/pii/B9780128156308000089},
  urldate = {2025-04-21},
  isbn = {978-0-12-815630-8}
}

@incollection{Chapter9Machine_2021_Kissell,
  title = {Chapter 9 - {{Machine Learning Techniques}}},
  booktitle = {Algorithmic {{Trading Methods}} ({{Second Edition}})},
  author = {Kissell, Robert L.},
  editor = {Kissell, Robert L.},
  date = {2021-01-01},
  pages = {221--231},
  publisher = {Academic Press},
  doi = {10.1016/B978-0-12-815630-8.00009-0},
  url = {https://www.sciencedirect.com/science/article/pii/B9780128156308000090},
  urldate = {2025-04-21},
  isbn = {978-0-12-815630-8}
}

@online{ChoroplethMaps_2016_,
  title = {Choropleth {{Maps}}},
  date = {2016-01-06},
  url = {https://www.darkhorseanalytics.com/portfolio/w24s5qofnzm4wqmsdfq98kwx035tew},
  urldate = {2024-10-17},
  abstract = {An animated step-by-step guide to improving your choropleth maps.},
  langid = {american},
  organization = {Darkhorse Analytics | Edmonton, AB}
}

@article{CienciaRecalculaSua_2024_CudischevitchNeves,
  entrysubtype = {newspaper},
  title = {A Ciência Recalcula Sua Rota},
  author = {Cudischevitch, Clarice and Neves, Kleber},
  date = {2024-05},
  journaltitle = {revista piauí},
  url = {https://piaui.folha.uol.com.br/materia/ciencia-recalcula-sua-rota/},
  urldate = {2024-06-02},
  abstract = {Pesquisadores lutam por um sistema de publicação mais aberto e menos excludente},
  file = {/home/baldoinov/baldoinov/PDFs/Diversos/A ciencia recalcula sua rota_2024_Cudischevitch et al.pdf;/home/baldoinov/Zotero/storage/5FL3823U/ciencia-recalcula-sua-rota.html}
}

@book{ClassicComputerScience_2019_Kopec,
  title = {Classic {{Computer Science Problems}} in {{Python}}},
  author = {Kopec, David},
  date = {2019},
  publisher = {Manning Publications Company},
  location = {Shelter Island, New York},
  abstract = {Classic Computer Science Problems in Python deepens your knowledge of problem-solving techniques from the realm of computer science by challenging you with time-tested scenarios, exercises, and algorithms. As you work through examples in search, clustering, graphs, and more, you'll remember important things you've forgotten and discover classic solutions to your "new" problems. Classic approaches are still the best way to solve them! Understanding these techniques in Python expands your potential for success in web development, data munging, machine learning, and more},
  isbn = {978-1-61729-598-0},
  langid = {english},
  pagetotal = {206},
  keywords = {Computer programming,Python (Computer program language)},
  annotation = {OCLC: on1047648544}
}

@online{ClearTable_2014_,
  title = {Clear {{Off}} the {{Table}}},
  date = {2014-03-27},
  url = {https://www.darkhorseanalytics.com/blog/clear-off-the-table},
  urldate = {2024-10-17},
  abstract = {The first animation in our Data Looks Better Naked series reworked a bar chart. Now we show you how to improve a data table.},
  langid = {american},
  organization = {Darkhorse Analytics | Edmonton, AB},
  file = {/home/baldoinov/Zotero/storage/4ZT97RFF/clear-off-the-table.html}
}

@online{CloserLookAUROC_2024_McDermottEtAl,
  title = {A {{Closer Look}} at {{AUROC}} and {{AUPRC}} under {{Class Imbalance}}},
  author = {McDermott, Matthew B. A. and Hansen, Lasse Hyldig and Zhang, Haoran and Angelotti, Giovanni and Gallifant, Jack},
  date = {2024-01-11},
  eprint = {2401.06091},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2401.06091},
  url = {http://arxiv.org/abs/2401.06091},
  urldate = {2024-02-15},
  abstract = {In machine learning (ML), a widespread adage is that the area under the precision-recall curve (AUPRC) is a superior metric for model comparison to the area under the receiver operating characteristic (AUROC) for binary classification tasks with class imbalance. This paper challenges this notion through novel mathematical analysis, illustrating that AUROC and AUPRC can be concisely related in probabilistic terms. We demonstrate that AUPRC, contrary to popular belief, is not superior in cases of class imbalance and might even be a harmful metric, given its inclination to unduly favor model improvements in subpopulations with more frequent positive labels. This bias can inadvertently heighten algorithmic disparities. Prompted by these insights, a thorough review of existing ML literature was conducted, utilizing large language models to analyze over 1.5 million papers from arXiv. Our investigation focused on the prevalence and substantiation of the purported AUPRC superiority. The results expose a significant deficit in empirical backing and a trend of misattributions that have fuelled the widespread acceptance of AUPRC's supposed advantages. Our findings represent a dual contribution: a significant technical advancement in understanding metric behaviors and a stark warning about unchecked assumptions in the ML community. All experiments are accessible at https://github.com/mmcdermott/AUC\_is\_all\_you\_need.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,notion,Statistics - Methodology},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/A Closer Look at AUROC and AUPRC under Class Imbalance_2024_McDermott et al.pdf;/home/baldoinov/Zotero/storage/5M25R9AC/2401.html}
}

@online{ClusteringConceitosBasicos_2022_Azank,
  title = {Clustering — {{Conceitos}} Básicos, Principais Algoritmos e Aplicação},
  author = {Azank, Felipe},
  date = {2022-10-17T15:10:34},
  url = {https://medium.com/turing-talks/clustering-conceitos-b%C3%A1sicos-principais-algoritmos-e-aplica%C3%A7%C3%A3o-ace572a062a9},
  urldate = {2024-09-06},
  abstract = {Olá, sejam bem vindos a mais um Turing Talks, hoje falaremos sobre uma das atividades mais realizadas na Área de Ciência de Dados: os…},
  langid = {english},
  organization = {Turing Talks}
}

@article{ClusteringPassingMessages_2007_FreyDueck,
  title = {Clustering by {{Passing Messages Between Data Points}}},
  author = {Frey, Brendan J. and Dueck, Delbert},
  date = {2007-02-16},
  journaltitle = {Science},
  volume = {315},
  number = {5814},
  pages = {972--976},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.1136800},
  url = {https://www.science.org/doi/10.1126/science.1136800},
  urldate = {2023-06-08},
  abstract = {Clustering data by identifying a subset of representative examples is important for processing sensory signals and detecting patterns in data. Such “exemplars” can be found by randomly choosing an initial subset of data points and then iteratively refining it, but this works well only if that initial choice is close to a good solution. We devised a method called “affinity propagation,” which takes as input measures of similarity between pairs of data points. Real-valued messages are exchanged between data points until a high-quality set of exemplars and corresponding clusters gradually emerges. We used affinity propagation to cluster images of faces, detect genes in microarray data, identify representative sentences in this manuscript, and identify cities that are efficiently accessed by airline travel. Affinity propagation found clusters with much lower error than other methods, and it did so in less than one-hundredth the amount of time.},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Clustering by Passing Messages Between Data Points_2007_Frey et al.pdf}
}

@article{ClusterrobustInferenceGuide_2023_MacKinnonEtAl,
  title = {Cluster-Robust Inference: {{A}} Guide to Empirical Practice},
  shorttitle = {Cluster-Robust Inference},
  author = {MacKinnon, James G. and Nielsen, Morten Ørregaard and Webb, Matthew D.},
  date = {2023-02-01},
  journaltitle = {Journal of Econometrics},
  shortjournal = {Journal of Econometrics},
  volume = {232},
  number = {2},
  pages = {272--299},
  issn = {0304-4076},
  doi = {10.1016/j.jeconom.2022.04.001},
  url = {https://www.sciencedirect.com/science/article/pii/S0304407622000781},
  urldate = {2024-02-15},
  abstract = {Methods for cluster-robust inference are routinely used in economics and many other disciplines. However, it is only recently that theoretical foundations for the use of these methods in many empirically relevant situations have been developed. In this paper, we use these theoretical results to provide a guide to empirical practice. We do not attempt to present a comprehensive survey of the (very large) literature. Instead, we bridge theory and practice by providing a thorough guide on what to do and why, based on recently available econometric theory and simulation evidence. To practice what we preach, we include an empirical analysis of the effects of the minimum wage on labor supply of teenagers using individual data.},
  keywords = {Cluster jackknife,Cluster-robust variance estimator (CRVE),Clustered data,notion,Robust inference,Wild cluster bootstrap},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Cluster-robust inference_2023_MacKinnon et al.pdf}
}

@article{cole_1999,
  title = {When Is Command-and-Control Efficient-Institutions, Technology, and the Comparative Efficiency of Alternative Regulatory Regimes for Environmental Protection},
  author = {Cole, D.H. and Grossman, P.Z.},
  date = {1999},
  journaltitle = {Wis. L. Rev.},
  pages = {887}
}

@article{CollaborativeFilteringRecommender_2022_PapadakisEtAl,
  title = {Collaborative Filtering Recommender Systems Taxonomy},
  author = {Papadakis, Harris and Papagrigoriou, Antonis and Panagiotakis, Costas and Kosmas, Eleftherios and Fragopoulou, Paraskevi},
  date = {2022-01-01},
  journaltitle = {Knowledge and Information Systems},
  shortjournal = {Knowl Inf Syst},
  volume = {64},
  number = {1},
  pages = {35--74},
  issn = {0219-3116},
  doi = {10.1007/s10115-021-01628-7},
  url = {https://doi.org/10.1007/s10115-021-01628-7},
  urldate = {2024-08-16},
  abstract = {In the era of internet access, recommender systems try to alleviate the difficulty that consumers face while trying to find items (e.g., services, products, or information) that better match their needs. To do so, a recommender system selects and proposes (possibly unknown) items that may be of interest to some candidate consumer, by predicting her/his preference for this item. Given the diversity of needs between consumers and the enormous variety of items to be recommended, a large set of approaches have been proposed by the research community. This paper provides a review of the approaches proposed in the entire research area of collaborative filtering recommend systems. To facilitate understanding, we provide a categorization of each approach based on the tools and techniques employed, which results to the main contribution of this paper, a collaborative filtering recommender systems taxonomy. This way, the reader acquires a quick and complete understanding of this research area. Finally, we provide a comparison of collaborative filtering recommender systems according to their ability to efficiently handle well-known drawbacks.},
  langid = {english},
  keywords = {Artificial Intelligence,Collaborative filtering,Recommendation systems,Survey,Taxonomy},
  file = {/home/baldoinov/baldoinov/PDFs/Mackenzie/projeto-aplicado-iii/Collaborative filtering recommender systems taxonomy_2022_Papadakis et al.pdf}
}

@article{CollaborativeInquiryProjectbased_2011_ChuEtAl,
  title = {Collaborative Inquiry Project-Based Learning: {{Effects}} on Reading Ability and Interests},
  shorttitle = {Collaborative Inquiry Project-Based Learning},
  author = {Chu, Samuel Kai Wah and Tse, Shek Kam and Loh, Elizabeth Ka Yee and Chow, Ken},
  date = {2011-07-01},
  journaltitle = {Library \& Information Science Research},
  shortjournal = {Library \& Information Science Research},
  volume = {33},
  number = {3},
  pages = {236--243},
  issn = {0740-8188},
  doi = {10.1016/j.lisr.2010.09.008},
  url = {https://www.sciencedirect.com/science/article/pii/S0740818811000296},
  urldate = {2024-09-11},
  abstract = {The academic performance of students has been shown to be associated with reading ability. Inquiry learning can potentially enhance the reading abilities and interests of students. This study verified this proposition by examining the effects of an inquiry approach to group projects on the reading abilities of primary school students. Using a case study design, an inquiry project-based learning (PBL) approach, with the collaboration between three types of teachers and the school librarian was implemented to support the development of reading abilities and interests of students in a primary school in Hong Kong. The participants included Primary 4 students, teachers, and parents. Progress in International Reading Literacy Study (PIRLS) tests were used to evaluate the students' reading abilities; survey questionnaire and interviews were used to examine the participants' perceptions of the inquiry PBL; and the PIRLS survey was used to measure the students' attitudes and self-perceptions. Quantitative and qualitative data analyses showed positive effects on the reading abilities and attitudes of the participating students. Students' attitudes and self-perceived abilities appeared to influence the improvements in reading abilities. Finally, improvements in the students' reading comprehension, reading speed, and vocabulary were perceived. These findings provide evidence and insights to support further implementation of inquiry PBL in primary schools.},
  file = {/home/baldoinov/baldoinov/PDFs/Mackenzie/projeto-aplicado-iii/Collaborative inquiry project-based learning_2011_Chu et al.pdf;/home/baldoinov/Zotero/storage/N493CVDY/S0740818811000296.html}
}

@article{ColonialOriginsComparative_2001_AcemogluEtAl,
  title = {The {{Colonial Origins}} of {{Comparative Development}}: {{An Empirical Investigation}}},
  shorttitle = {The {{Colonial Origins}} of {{Comparative Development}}},
  author = {Acemoglu, Daron and Johnson, Simon and Robinson, James A.},
  date = {2001-12},
  journaltitle = {American Economic Review},
  volume = {91},
  number = {5},
  pages = {1369--1401},
  issn = {0002-8282},
  doi = {10.1257/aer.91.5.1369},
  url = {https://www.aeaweb.org/articles?id=10.1257/aer.91.5.1369},
  urldate = {2024-03-22},
  abstract = {We exploit differences in European mortality rates to estimate the effect of institutions on economic performance. Europeans adopted very different colonization policies in different colonies, with different associated institutions. In places where Europeans faced high mortality rates, they could not settle and were more likely to set up extractive institutions. These institutions persisted to the present. Exploiting differences in European mortality rates as an instrument for current institutions, we estimate large effects of institutions on income per capita. Once the effect of institutions is controlled for, countries in Africa or those closer to the equator do not have lower incomes.},
  langid = {english},
  keywords = {Growth and Fluctuations: General International or Comparative Comparative Studies of Countries,Macroeconomic Analyses of Economic Development Comparative Analysis of Economic Systems Health Production Economic History: Macroeconomics and Monetary Economics},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/economia-brasileira-ii/The Colonial Origins of Comparative Development_2001_Acemoglu et al.pdf}
}

@article{CombinedForecastingMethod_2022_ZhuangEtAl,
  title = {A Combined Forecasting Method for Intermittent Demand Using the Automotive Aftermarket Data},
  author = {Zhuang, Xiaotian and Yu, Ying and Chen, Aihui},
  date = {2022-06-01},
  journaltitle = {Data Science and Management},
  shortjournal = {Data Science and Management},
  volume = {5},
  number = {2},
  pages = {43--56},
  issn = {2666-7649},
  doi = {10.1016/j.dsm.2022.04.001},
  url = {https://www.sciencedirect.com/science/article/pii/S2666764922000121},
  urldate = {2024-10-21},
  abstract = {Intermittent demand forecasting is an important challenge in the process of smart supply chain transformation, and accurate demand forecasting can reduce costs and increase efficiency for enterprises. This study proposes an intermittent demand combination forecasting method based on internal and external data, builds intermittent demand feature engineering from the perspective of machine learning, predicts the occurrence of demand by classification model, and predicts non-zero demand quantity by regression model. Based on the strategy selection on the inventory side and the stocking needs on the replenishment side, this study focuses on the optimization of the classification problem, incorporates the internal and external data of the enterprise, and proposes two combination forecasting optimization methods on the basis of the best classification threshold searching and transfer learning, respectively. Based on the real data of auto after-sales business, these methods are evaluated and validated in multiple dimensions. Compared with other intermittent forecasting methods, the models proposed in this study have been improved significantly in terms of classification accuracy and forecasting precision, which validates the potential of combined forecasting framework for intermittent demand and provides an empirical study of the framework in industry practice. The results show that this research can further provide accurate upstream inputs for smart inventory and guarantee intelligent supply chain decision-making in terms of accuracy and efficiency.},
  keywords = {Combination forecasting,Intelligent supply chain management,Intermittent demand,Machine learning,Transfer learning},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Recovery/Modelo de Incidencia de Acao Contra/A combined forecasting method for intermittent demand using the automotive_2022_Zhuang et al.pdf;/home/baldoinov/Zotero/storage/5N3LDJ39/S2666764922000121.html}
}

@online{CombiningProbabilisticForecasts_2024_WangEtAl,
  title = {Combining {{Probabilistic Forecasts}} of {{Intermittent Demand}}},
  author = {Wang, Shengjie and Kang, Yanfei and Petropoulos, Fotios},
  date = {2024-04-15},
  eprint = {2304.03092},
  eprinttype = {arXiv},
  url = {http://arxiv.org/abs/2304.03092},
  urldate = {2024-10-21},
  abstract = {In recent decades, new methods and approaches have been developed for forecasting intermittent demand series. However, the majority of research has focused on point forecasting, with little exploration into probabilistic intermittent demand forecasting. This is despite the fact that probabilistic forecasting is crucial for effective decision-making under uncertainty and inventory management. Additionally, most literature on this topic has focused solely on forecasting performance and has overlooked the inventory implications, which are directly relevant to intermittent demand. To address these gaps, this study aims to construct probabilistic forecasting combinations for intermittent demand while considering both forecasting accuracy and inventory control utility in obtaining combinations and evaluating forecasts. Our empirical findings demonstrate that combinations perform better than individual approaches for forecasting intermittent demand, but there is a trade-off between forecasting and inventory performance.},
  pubstate = {prepublished},
  keywords = {Statistics - Applications},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Recovery/Modelo de Incidencia de Acao Contra/Combining Probabilistic Forecasts of Intermittent Demand_2024_Wang et al.pdf;/home/baldoinov/Zotero/storage/BE5MW8LA/2304.html}
}

@book{ComecandoComLinux_2013_Almeida,
  title = {Começando com o Linux. Comando, Serviços e Administração},
  author = {family=Almeida, given=Adriano Henrique, prefix=de, useprefix=false},
  date = {2013-01-01},
  publisher = {Casa do Código},
  isbn = {978-85-66250-29-9},
  langid = {portuguese}
}

@video{ComoCifrarSuas_2024_MarcosLima,
  entrysubtype = {video},
  title = {Como {{Cifrar}} Suas {{Músicas Corretamente}}},
  editor = {{Marcos Lima}},
  editortype = {director},
  date = {2024-03-14},
  url = {https://www.youtube.com/watch?v=YdTmbJa6uQM},
  urldate = {2025-08-17},
  abstract = {Aprenda como escrever sas cifras corretamente! Curso Decole no Violão - Professor Marcos Lima}
}

@video{ComoFazerJejum_2022_Lordeiro,
  entrysubtype = {video},
  title = {Como Fazer Jejum de Forma Simples Transforma a Vida! {{Minha}} Experiência Com o Hábito Do Jejum!},
  editor = {Lordeiro, Giovanna},
  editortype = {director},
  date = {2022-12-29},
  url = {https://www.youtube.com/watch?v=bE1FcGfDP0M},
  urldate = {2024-07-15},
  abstract = {Como fazer jejum de um jeito simples e constante, como comecei a jejuar, as dificuldades, os efeitos, alguns dos resultados sobrenaturais e conselhos para quem começar é o tema desse vídeo! Sem dúvidas, jejuar foi o hábito que mudou completamente não só meu ano, mas minha vida! Eu comecei por puramente obediência, sendo que é uma disciplina na vida cristã, assim como orar, mas que ENORME SURPRESA eu tive com os efeitos do jejum na minha vida e que eu REALMENTE não esperava. Que esse vídeo abra seus olhos para as maravilhas que até mesmo os mais simples jejuns podem fazer na sua vida e te inspire a começar também, mesmo pequeno! Até o próximo vídeo! Beijo!! Link do livro mencionado: https://amzn.to/3nGVLOB Link da pregação mencionada: ~~~•~Luciano~Subirá~-~O~EFEITO~DO~JEJUM~~ Faça parte da nossa família linda e se inscreva no canal: ~~~/~@giovannalordeiro~~ Me acompanha também no Instagram: ~~/~giovannalordeiro~~ \#jejum  00:00 O que aconteceu 01:12 Por que jejuar 02:22 Benefícios  03:27 A virada de Chave 05:01 Começando os jejuns 06:18 Conselho de amiga 07:08 Acontecimentos Sobrenaturais 08:50 Recomeçando 09:37 Visões}
}

@video{ComoFazerRetorno_2025_MusiconaPratica,
  entrysubtype = {video},
  title = {Como {{Fazer Retorno}} de {{Palco}} Com {{Fones}} Para {{Grupos}} de {{Até}} 5 {{Pessoas}}},
  editor = {{Músico na Prática}},
  editortype = {director},
  date = {2025-07-06},
  url = {https://www.youtube.com/watch?app=desktop&v=bqGFWsclJhM&ab_channel=M%C3%BAsiconapr%C3%A1tica},
  urldate = {2025-08-17},
  abstract = {CURSO DE AÚDIO COM 50\% DE DESCONTO 👉 https://go.hotmart.com/H99737010B LISTA VIP MERCADO LIVRE 👉 https://mercadolivre.com/sec/19SmSyz}
}

@video{ComoMontarUma_2020_Goncalves,
  entrysubtype = {video},
  title = {Como Montar Uma Pregação Do Zero},
  editor = {Gonçalves, Douglas},
  editortype = {director},
  namea = {{JesusCopy}},
  nameatype = {collaborator},
  date = {2020-07-14},
  url = {https://www.youtube.com/watch?v=Fb8dRdxw_9Y},
  urldate = {2024-09-13},
  abstract = {LINK CONFERÊNCIA LIDERE COMO JESUS (Inscrição 100\% Grátis): http://jesuscopy.rds.land/lidere-como... Faaala galera, JesusCopy! Douglas Gonçalves por aqui. Hoje eu quero compartilhar com você sobre “Como eu penso um sermão do zero!”  Vou te mostrar 4 passos que eu uso para montar um sermão: 1. Tema: Sobre o que vai ser a pregação.  2. Pesquisa: Você pesquisa sobre o tema escolhido (leia, escute e assista). Depois anote tudo! 3. Compartilhar: Fale para todo mundo aquilo que você está estudando.  4. Organizar: Coloque a sequência do sermão:  - Introdução - Proposição - Desenvolvimento - Evangelho - Conclusão Siga estas dicas que tenho certeza que sua pregação vai ser benção!  Deus te abençoe e não se esqueça, você é uma cópia de Jesus. Douglas Gonçalves \#EVANGELHO \#SERMÃO \#ESTUDO Se inscreva no canal JesusCopy. Participe da nossa lista de emails: http://conteudo.jesuscopy.com/dicas-j... Para saber mais sobre o JesusCopy: Curta: ~~/~jesuscopy~~ Instagram: @jesus\_copy Acesse: www.jesuscopy.com}
}

@video{ComoOrarForma_2024_Subira,
  entrysubtype = {video},
  title = {Como {{Orar}} Da {{Forma Certa}}},
  editor = {Subirá, Luciano},
  editortype = {director},
  namea = {{Trecho da Palavra}},
  nameatype = {collaborator},
  date = {2024-03-21},
  url = {https://www.youtube.com/watch?v=i9CBDz9JJ60},
  urldate = {2024-09-16},
  abstract = {✅ APOIE este Canal → https://www.youtube.com/trechodapalav... Orar da forma certa vai além de palavras bem formuladas; é sobre conexão genuína com Deus. A Bíblia nos ensina que devemos orar com fé, confiança e de acordo com a vontade de Deus. Isso significa reconhecer Sua soberania e buscar Sua orientação em todas as nossas petições. Orar da forma certa envolve humildade, reconhecendo nossa dependência de Deus, e também perseverança, não desistindo mesmo diante das dificuldades. Ao nos achegarmos a Ele com sinceridade de coração, Ele nos ouve e responde conforme o Seu perfeito plano para nossas vidas. → Se você está sendo abençoado: 🙌  APOIE este Canal: https://www.youtube.com/trechodapalav... - - - - - - - - - - - 🎬 Palavra completa: ~~~•~Luciano~Subirá~-~POR~QUE~ORAR?~~ 🎬 Assista também: ~~~•~ESPERAR~EM~DEUS~|~Luciano~Subirá~~ 🔔 Este vídeo te abençoou? Compartilhe! - - - - - - - - - - - 📧 𝐂𝐨𝐧𝐭𝐚𝐭𝐨: ✓ 𝑨𝒏𝒖𝒏𝒄𝒊𝒆 → trechodapalavra@gmail.com ◾ Tópicos do vídeo: 00:00 - A oração que Deus responde 05:10 - Então porque precisamos orar? 09:25 - Entenda o "seja feita a Sua vontade" 14:21 - A oração é necessária pela legalidade *Trecho da pregação do ‪@lucianosubira‬}
}

@article{ComparingForecastingPerformance_2024_QuEtAl,
  title = {Comparing Forecasting Performance with Panel Data},
  author = {Qu, Ritong and Timmermann, Allan and Zhu, Yinchu},
  date = {2024-07-01},
  journaltitle = {International Journal of Forecasting},
  shortjournal = {International Journal of Forecasting},
  volume = {40},
  number = {3},
  pages = {918--941},
  issn = {0169-2070},
  doi = {10.1016/j.ijforecast.2023.08.001},
  url = {https://www.sciencedirect.com/science/article/pii/S0169207023000766},
  urldate = {2024-10-11},
  abstract = {We develop new methods for testing equal predictive accuracy for panels of forecasts, exploiting information in both the time-series and cross-sectional dimensions of the data. We examine general tests of equal forecasting performance averaged across all time periods and individual units, along with tests that focus on subsets of time or clusters of units. Properties of our tests are demonstrated through Monte Carlo simulations and in an empirical application that compares International Monetary Fund forecasts of country-level real gross domestic product growth and inflation to private-sector survey forecasts and forecasts from a simple time-series model.},
  keywords = {Cross-sectional clusters,Inflation forecasts,Real GDP growth forecasts,Term structure of forecast errors,Tests of equal predictive accuracy,Time clusters},
  file = {/home/baldoinov/Zotero/storage/WC2Z6ITB/S0169207023000766.html}
}

@article{ComparisonImputationMethods_2023_MemonEtAl,
  title = {A Comparison of Imputation Methods for Categorical Data},
  author = {Memon, Shaheen MZ. and Wamala, Robert and Kabano, Ignace H.},
  date = {2023-01-01},
  journaltitle = {Informatics in Medicine Unlocked},
  shortjournal = {Informatics in Medicine Unlocked},
  volume = {42},
  pages = {101382},
  issn = {2352-9148},
  doi = {10.1016/j.imu.2023.101382},
  url = {https://www.sciencedirect.com/science/article/pii/S2352914823002289},
  urldate = {2025-02-24},
  abstract = {Objectives Missing data is commonplace in clinical databases, which are being increasingly used for research. Without giving any regard to missing data, results from analysis may become biased and unrepresentative. Clinical databases contain mainly categorical variables. This study aims to assess the methods used for imputation in categorical variables. Materials and methods We utilized data extracted from paper-based maternal health records from Kawempe National Referral Hospital, Uganda. We compared the following imputation methods for categorical data in an empirical analysis: Mode, K-Nearest Neighbors (KNN), Random Forest (RF), Sequential Hot-Deck (SHD), and Multiple Imputation by Chained Equations (MICE). The five imputation methods were first compared by accuracy of predicting the missing values. Next, the imputation methods were compared by predictive accuracy of the outcome variable in four classifiers. The consistency of performance of imputation methods across different levels of missing data (5\%–50~\%) was assessed by Kendall's W test. Results KNN imputation had the highest precision score at levels (5\%–50~\%) of MCAR missing data. At lower proportions of missing data (5~\%, 10~\%, 15~\%, 20~\%), RF imputation had the second-highest precision score. SHD imputation had the worst precision at all levels of missing data. In the prediction of the outcome, the methods performed differently at all proportions of missing data in the four classifiers. Even though KNN imputation was the best method in predicting the missing values, it did not consistently enhance the predictive accuracy of the classifiers at all levels of missing data. Our findings show that a high precision score of an imputation method does not translate into higher predictive accuracy in classifiers. Conclusions KNN imputation is the best method in predicting missing values in categorical variables. There is no universal best imputation method that yields the highest predictive accuracy at all proportions of missing data.},
  keywords = {Categorical variables,Imputation,Multiple imputation,Precision score,Single imputation},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/A comparison of imputation methods for categorical data_2023_Memon et al.pdf}
}

@book{CompetitionLawClimate_2021_HolmesEtAl,
  title = {Competition Law, Climate Change and Environmental Sustainability},
  editor = {Holmes, Simon and Middelschulte, Dirk and Snoep, Martijn},
  date = {2021},
  publisher = {Institute of Competition Law - Concurrences},
  location = {New York},
  isbn = {978-1-939007-72-8},
  pagetotal = {466},
  keywords = {Direito Ambiental,Direito Concorrencial,Sustentabilidade}
}

@incollection{CompetitionPolicySustainable_2021_HolmesEtAl,
  title = {Competition {{Policy}} for a {{Sustainable Food Sector}}: {{An In-House Counsel Perspective}}},
  booktitle = {Competition Law, Climate Change and Environmental Sustainability},
  editor = {Holmes, Simon and Middelschulte, Dirk and Snoep, Martijn and Chu, Martyn},
  date = {2021},
  publisher = {Institute of Competition Law - Concurrences},
  location = {New York},
  isbn = {978-1-939007-72-8},
  keywords = {Direito Ambiental,Direito Concorrencial,Sustentabilidade}
}

@book{CompetitiveProgrammingPython_2020_DurrVie,
  title = {Competitive {{Programming}} in {{Python}}: 128 {{Algorithms}} to {{Develop}} Your {{Coding Skills}}},
  shorttitle = {Competitive {{Programming}} in {{Python}}},
  author = {Dürr, Christoph and Vie, Jill-Jênn},
  translator = {Gibbons, Greg and Gibbons, Danièle},
  date = {2020-11-30},
  edition = {1},
  publisher = {Cambridge University Press},
  doi = {10.1017/9781108591928},
  url = {https://www.cambridge.org/core/product/identifier/9781108591928/type/book},
  urldate = {2024-03-21},
  abstract = {Want to kill it at your job interview in the tech industry? Want to win that coding competition? Learn all the algorithmic techniques and programming skills you need from two experienced coaches, problem setters, and jurors for coding competitions. The authors highlight the versatility of each algorithm by considering a variety of problems and show how to implement algorithms in simple and efficient code. Readers can expect to master 128 algorithms in Python and discover the right way to tackle a problem and quickly implement a solution of low complexity. Classic problems like Dijkstra's shortest path algorithm and Knuth-Morris-Pratt's string matching algorithm are featured alongside lesser known data structures like Fenwick trees and Knuth's dancing links. The book provides a framework to tackle algorithmic problem solving, including: Definition, Complexity, Applications, Algorithm, Key Information, Implementation, Variants, In Practice, and Problems. Python code included in the book and on the companion website.},
  isbn = {978-1-108-59192-8 978-1-108-71682-6},
  langid = {english}
}

@video{CompleteGuideBuild_2025_CodingEntrepreneurs,
  entrysubtype = {video},
  title = {Complete {{Guide}} to {{Build}} and {{Deploy}} an {{AI Agent}} with {{Docker Containers}} and {{Python}}},
  editor = {{CodingEntrepreneurs}},
  editortype = {director},
  date = {2025-06-14},
  url = {https://www.youtube.com/watch?app=desktop&v=KC8HT0eWSGk&ab_channel=CodingEntrepreneurs},
  urldate = {2025-08-17},
  abstract = {Complete Guide to Build and Deploy an AI Agent with Docker Containers and Python If you've ever thought about learning Docker for development and production, this is the course for you.}
}

@article{ComplexityEconomy_1999_Arthur,
  title = {Complexity and the {{Economy}}},
  author = {Arthur, W. Brian},
  date = {1999-04-02},
  journaltitle = {Science},
  volume = {284},
  number = {5411},
  pages = {107--109},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.284.5411.107},
  url = {https://www.science.org/doi/10.1126/science.284.5411.107},
  urldate = {2025-07-04},
  abstract = {After two centuries of studying equilibria—static patterns that call for no further behavioral adjustments—economists are beginning to study the general emergence of structures and the unfolding of patterns in the economy. When viewed in out-of-equilibrium formation, economic patterns sometimes simplify into the simple static equilibria of standard economics. More often they are ever changing, showing perpetually novel behavior and emergent phenomena. Complexity portrays the economy not as deterministic, predictable, and mechanistic, but as process dependent, organic, and always evolving.},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Complexity and the Economy_1999_Arthur.pdf}
}

@inproceedings{ComplexityMechanismDesign_2002_ConitzerSandholm,
  title = {Complexity of {{Mechanism Design}}},
  author = {Conitzer, Vincent and Sandholm, T.},
  date = {2002-05-28},
  url = {https://www.semanticscholar.org/paper/Complexity-of-Mechanism-Design-Conitzer-Sandholm/9f8978d41e76eb574200e25afca9efd3651f65a7},
  urldate = {2024-06-11},
  abstract = {The aggregation of conflicting preferences is a central problem in multiagent systems. The key difficulty is that the agents may report their preferences insincerely. Mechanism design is the art of designing the rules of the game so that the agents are motivated to report their preferences truthfully and a (socially) desirable outcome is chosen. We propose an approach where a mechanism is automatically created for the preference aggregation setting at hand. This has several advantages, but the downside is that the mechanism design optimization problem needs to be solved anew each time. Focusing-on settings where side payments are not possible, we show that the mechanism design problem is NP-complete for deterministic mechanisms. This holds both for dominantstrategy implementation and for Bayes-Nash implementation. We then show that if we allow randomized mechanisms, the mechanism design problem becomes tractable. In other words, the coordinator can tackle the computational complexity introduced by its uncertainty the agents face additional uncertainty. This comes at no loss, and in some cases at a gain, in the (social) objective.},
  eventtitle = {Conference on {{Uncertainty}} in {{Artificial Intelligence}}},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Complexity of Mechanism Design_2002_Conitzer et al.pdf}
}

@online{ComplexSystemsSurvey_2011_Newman,
  title = {Complex {{Systems}}: {{A Survey}}},
  shorttitle = {Complex {{Systems}}},
  author = {Newman, M. E. J.},
  date = {2011-12-06},
  eprint = {1112.1440},
  eprinttype = {arXiv},
  eprintclass = {cond-mat, physics:physics},
  doi = {10.1119/1.3590372},
  url = {http://arxiv.org/abs/1112.1440},
  urldate = {2023-11-30},
  abstract = {A complex system is a system composed of many interacting parts, often called agents, which displays collective behavior that does not follow trivially from the behaviors of the individual parts. Examples include condensed matter systems, ecosystems, stock markets and economies, biological evolution, and indeed the whole of human society. Substantial progress has been made in the quantitative understanding of complex systems, particularly since the 1980s, using a combination of basic theory, much of it derived from physics, and computer simulation. The subject is a broad one, drawing on techniques and ideas from a wide range of areas. Here I give a survey of the main themes and methods of complex systems science and an annotated bibliography of resources, ranging from classic papers to recent books and reviews.},
  pubstate = {prepublished},
  keywords = {Condensed Matter - Statistical Mechanics,Nonlinear Sciences - Adaptation and Self-Organizing Systems,notion,Physics - Physics and Society},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Complex Systems_2011_Newman.pdf;/home/baldoinov/Zotero/storage/AZMINCIC/1112.html}
}

@article{ComprehensiveSurveyClustering_2015_XuTian,
  title = {A {{Comprehensive Survey}} of {{Clustering Algorithms}}},
  author = {Xu, Dongkuan and Tian, Yingjie},
  date = {2015-06-01},
  journaltitle = {Annals of Data Science},
  shortjournal = {Ann. Data. Sci.},
  volume = {2},
  number = {2},
  pages = {165--193},
  issn = {2198-5812},
  doi = {10.1007/s40745-015-0040-1},
  url = {https://doi.org/10.1007/s40745-015-0040-1},
  urldate = {2023-06-06},
  abstract = {Data analysis is used as a common method in modern science research, which is across communication science, computer science and biology science. Clustering, as the basic composition of data analysis, plays a significant role. On one hand, many tools for cluster analysis have been created, along with the information increase and subject intersection. On the other hand, each clustering algorithm has its own strengths and weaknesses, due to the complexity of information. In this review paper, we begin at the definition of clustering, take the basic elements involved in the clustering process, such as the distance or similarity measurement and evaluation indicators, into consideration, and analyze the clustering algorithms from two perspectives, the traditional ones and the modern ones. All the discussed clustering algorithms will be compared in detail and comprehensively shown in Appendix Table~22.},
  langid = {english},
  keywords = {Clustering,Clustering algorithm,Clustering analysis,notion,Survey,Unsupervised learning},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/A Comprehensive Survey of Clustering Algorithms_2015_Xu et al.pdf}
}

@online{ComprehensiveSurveyCompression_2024_ParkEtAl,
  title = {A {{Comprehensive Survey}} of {{Compression Algorithms}} for {{Language Models}}},
  author = {Park, Seungcheol and Choi, Jaehyeon and Lee, Sojin and Kang, U.},
  date = {2024-01-27},
  eprint = {2401.15347},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2401.15347},
  url = {http://arxiv.org/abs/2401.15347},
  urldate = {2024-01-31},
  abstract = {How can we compress language models without sacrificing accuracy? The number of compression algorithms for language models is rapidly growing to benefit from remarkable advances of recent language models without side effects due to the gigantic size of language models, such as increased carbon emissions and expensive maintenance fees. While numerous compression algorithms have shown remarkable progress in compressing language models, it ironically becomes challenging to capture emerging trends and identify the fundamental concepts underlying them due to the excessive number of algorithms. In this paper, we survey and summarize diverse compression algorithms including pruning, quantization, knowledge distillation, low-rank approximation, parameter sharing, and efficient architecture design. We not only summarize the overall trend of diverse compression algorithms but also select representative algorithms and provide in-depth analyses of them. We discuss the value of each category of compression algorithms, and the desired properties of low-cost compression algorithms which have a significant impact due to the emergence of large language models. Finally, we introduce promising future research topics based on our survey results.},
  pubstate = {prepublished},
  keywords = {68T50,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,I.2.7,notion},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/A Comprehensive Survey of Compression Algorithms for Language Models_2024_Park et al.pdf;/home/baldoinov/Zotero/storage/A72ZRZLF/2401.html}
}

@incollection{ComputableGeneralEquilibrium_1996_DixonParmenter,
  title = {Computable General Equilibrium Modelling for Policy Analysis and Forecasting},
  booktitle = {Handbook of {{Computational Economics}}},
  author = {Dixon, Peter B. and Parmenter, B. R.},
  date = {1996-01-01},
  volume = {1},
  pages = {3--85},
  publisher = {Elsevier},
  doi = {10.1016/S1574-0021(96)01003-9},
  url = {https://www.sciencedirect.com/science/article/pii/S1574002196010039},
  urldate = {2023-09-27},
  abstract = {This chapter describes computable general equilibrium (CGE) modeling and the history of its development. The chapter illustrates the computation of solutions for CGE models and reviews its achievements, failures, and potential. The model illustrated in the chapter can be used in two ways: as a single-period model suitable for comparative–static analyses; and as a model for multi-period forecasting. Before CGE models, there were input–output models that emphasized input–output linkages among industries. CGE models go beyond input–output models by linking industries via economy-wide constraints including constraints on the size of government budget deficits; constraints on deficits in the balance of trade; constraints on the availability of labor, capital, and land; and constraints arising from environmental considerations, such as air and water quality. Much of CGE modeling has been concerned with the welfare implications of proposed policy changes—for example, changes in protection, changes in taxes, and changes in environmental regulations. Many interesting welfare results have been obtained, especially in the analysis of tax changes.},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Computable general equilibrium modelling for policy analysis and forecasting_1996_Dixon et al.pdf;/home/baldoinov/Zotero/storage/HKW7VW6P/S1574002196010039.html}
}

@book{ComputationalBeautyNature_1998_Flake,
  title = {The Computational Beauty of Nature – Computer Explorations of Fractals, Chaos, Complex Systems \& Adaption: Computer Explorations of Fractals, Chaos, Complex Systems and Adaptation},
  shorttitle = {The Computational Beauty of Nature – Computer Explorations of Fractals, Chaos, Complex Systems \& Adaption},
  author = {Flake, William Gw},
  date = {1998},
  publisher = {MIT Press},
  location = {Cambridge, Mass},
  abstract = {Honorable Mention, 1998, category of Computer Science, Professional/Scholarly Publishing Annual Awards Competition presented by the Association of American Publishers, Inc. "Simulation," writes Gary Flake in his preface, "becomes a form of experimentation in a universe of theories. The primary purpose of this book is to celebrate this fact."In this book, Gary William Flake develops in depth the simple idea that recurrent rules can produce rich and complicated behaviors. Distinguishing "agents" (e.g., molecules, cells, animals, and species) from their interactions (e.g., chemical reactions, immune system responses, sexual reproduction, and evolution), Flake argues that it is the computational properties of interactions that account for much of what we think of as "beautiful" and "interesting." From this basic thesis, Flake explores what he considers to be today's four most interesting computational topics: fractals, chaos, complex systems, and adaptation.Each of the book's parts can be read independently, enabling even the casual reader to understand and work with the basic equations and programs. Yet the parts are bound together by the theme of the computer as a laboratory and a metaphor for understanding the universe. The inspired reader will experiment further with the ideas presented to create fractal landscapes, chaotic systems, artificial life forms, genetic algorithms, and artificial neural networks.},
  isbn = {978-0-262-06200-8},
  langid = {Inglês}
}

@book{ComputerSystemsProgrammer_2011_BryantOHallaron,
  title = {Computer {{Systems}}: A {{Programmer}}'s {{Perspective}}},
  shorttitle = {Computer Systems},
  author = {Bryant, Randal E. and O'Hallaron, David Richard},
  date = {2011},
  edition = {2. ed},
  publisher = {Prentice Hall},
  location = {Boston, Mass.},
  abstract = {"Computer Systems: A Programmer's Perspective, Second Edition, introduces the important and enduring concepts that underlie computer systems by showing how these ideas affect the correctness, performance, and utility of application programs. Other systems books, written from a builder's perspective, describe how to implement the hardware or some portion of the system software, such as the operating system, compiler, or network interface. This book is written from a programmer's perspective, describing how application programmers can use their knowledge of the entire system to write better programs. Changes in hardware technology and compilers over the past decade have informed this major revision of the 2003 edition."--BOOK JACKET},
  isbn = {978-0-13-610804-7},
  langid = {english},
  pagetotal = {1041}
}

@article{ConceptualizingBigSocial_2017_OlshannikovaEtAl,
  title = {Conceptualizing {{Big Social Data}}},
  author = {Olshannikova, Ekaterina and Olsson, Thomas and Huhtamäki, Jukka and Kärkkäinen, Hannu},
  date = {2017-01-25},
  journaltitle = {Journal of Big Data},
  shortjournal = {Journal of Big Data},
  volume = {4},
  number = {1},
  pages = {3},
  issn = {2196-1115},
  doi = {10.1186/s40537-017-0063-x},
  url = {https://doi.org/10.1186/s40537-017-0063-x},
  urldate = {2023-05-03},
  abstract = {The popularity of social media and computer-mediated communication has resulted in high-volume and highly semantic data about digital social interactions. This constantly accumulating data has been termed as Big Social Data or Social Big Data, and various visions about how to utilize that have been presented. However, as relatively new concepts, there are no solid and commonly agreed definitions of them. We argue that the emerging research field around these concepts would benefit from understanding about the very substance of the concept and the different viewpoints to it. With our review of earlier research, we highlight various perspectives to this multi-disciplinary field and point out conceptual gaps, the diversity of perspectives and lack of consensus in what Big Social Data means. Based on detailed analysis of related work and earlier conceptualizations, we propose a synthesized definition of the term, as well as outline the types of data that Big Social Data covers. With this, we aim to foster future research activities around this intriguing, yet untapped type of Big Data.},
  keywords = {Big Social Data,Big Social Data analysis,Classification,Computational social science,Conceptualization,Digital human,notion,Social Big Data,Social computing,Social Data,Social media},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/projeto-heiwa/Conceptualizing Big Social Data_2017_Olshannikova et al.pdf;/home/baldoinov/Zotero/storage/FF4ZPVDU/s40537-017-0063-x.html}
}

@article{ConformalInferenceCounterfactuals_2021_LeiCandes,
  title = {Conformal {{Inference}} of {{Counterfactuals}} and {{Individual Treatment Effects}}},
  author = {Lei, Lihua and Candès, Emmanuel J.},
  date = {2021-11-01},
  journaltitle = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  shortjournal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  volume = {83},
  number = {5},
  pages = {911--938},
  issn = {1369-7412},
  doi = {10.1111/rssb.12445},
  url = {https://doi.org/10.1111/rssb.12445},
  urldate = {2025-04-21},
  abstract = {Evaluating treatment effect heterogeneity widely informs treatment decision making. At the moment, much emphasis is placed on the estimation of the conditional average treatment effect via flexible machine learning algorithms. While these methods enjoy some theoretical appeal in terms of consistency and convergence rates, they generally perform poorly in terms of uncertainty quantification. This is troubling since assessing risk is crucial for reliable decision-making in sensitive and uncertain environments. In this work, we propose a conformal inference-based approach that can produce reliable interval estimates for counterfactuals and individual treatment effects under the potential outcome framework. For completely randomized or stratified randomized experiments with perfect compliance, the intervals have guaranteed average coverage in finite samples regardless of the unknown data generating mechanism. For randomized experiments with ignorable compliance and general observational studies obeying the strong ignorability assumption, the intervals satisfy a doubly robust property which states the following: the average coverage is approximately controlled if either the propensity score or the conditional quantiles of potential outcomes can be estimated accurately. Numerical studies on both synthetic and real data sets empirically demonstrate that existing methods suffer from a significant coverage deficit even in simple models. In contrast, our methods achieve the desired coverage with reasonably short intervals.},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Conformal Inference of Counterfactuals and Individual Treatment Effects_2021_Lei et al.pdf;/home/baldoinov/Zotero/storage/G5WZGQJ6/7056131.html}
}

@article{ConnectivityConservationPlanning_2024_EquihuaEtAl,
  title = {Connectivity Conservation Planning through Deep Reinforcement Learning},
  author = {Equihua, Julián and Beckmann, Michael and Seppelt, Ralf},
  date = {2024},
  journaltitle = {Methods in Ecology and Evolution},
  volume = {15},
  number = {4},
  pages = {779--790},
  issn = {2041-210X},
  doi = {10.1111/2041-210X.14300},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.14300},
  urldate = {2024-05-17},
  abstract = {The United Nations has declared 2021–2030 the decade on ecosystem restoration with the aim of preventing, stopping and reversing the degradation of the ecosystems of the world, often caused by the fragmentation of natural landscapes. Human activities separate and surround habitats, making them too small to sustain viable animal populations or too far apart to enable foraging and gene flow. Despite the need for strategies to solve fragmentation, it remains unclear how to efficiently reconnect nature. In this paper, we illustrate the potential of deep reinforcement learning (DRL) to tackle the spatial optimisation aspect of connectivity conservation planning. The propensity of spatial optimisation problems to explode in complexity depending on the number of input variables and their states is and will continue to be one of its most serious obstacles. DRL is an emerging class of methods focused on training deep neural networks to solve decision-making tasks and has been used to learn good heuristics for complex optimisation problems. While the potential of DRL to optimise conservation decisions seems huge, only few examples of its application exist. We applied DRL to two real-world raster datasets in a connectivity planning setting, targeting graph-based connectivity indices for optimisation. We show that DRL converges to the known optimums in a small example where the objective is the overall improvement of the Integral Index of Connectivity and the only constraint is the budget. We also show that DRL approximates high-quality solutions on a large example with additional cost and spatial configuration constraints where the more complex Probability of Connectivity Index is targeted. To the best of our knowledge, there is no software that can target this index for optimisation on raster data of this size. DRL can be used to approximate good solutions in complex spatial optimisation problems even when the conservation feature is non-linear like graph-based indices. Furthermore, our methodology decouples the optimisation process and the index calculation, so it can potentially target any other conservation feature implemented in current or future software.},
  langid = {english},
  keywords = {connectivity conservation planning,deep reinforcement learning,ecological restoration,machine learning,spatial optimisation,systematic conservation planning},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Connectivity conservation planning through deep reinforcement learning_2024_Equihua et al.pdf;/home/baldoinov/Zotero/storage/3BASYG75/2041-210X.html}
}

@legislation{ConstituicaoRepublicaFederativa_1988_Brasil,
  title = {Constituição Da {{República Federativa}} Do {{Brasil}} de 1988},
  namea = {{Brasil}},
  nameatype = {collaborator},
  date = {1988-10-05},
  journaltitle = {Diário Oficial da União},
  number = {Artigo 170},
  url = {https://www.planalto.gov.br/ccivil_03/constituicao/constituicao.htm},
  urldate = {2024-04-04},
  annotation = {Brasília, DF},
  file = {/home/baldoinov/Zotero/storage/Z6TF4397/constituicao.html}
}

@book{ContabilidadeSocial_2016_Feijo,
  title = {Contabilidade Social},
  author = {Feijó, Carmem Aparecida},
  namea = {Ramos, Roberto Luís Olinto},
  nameatype = {collaborator},
  date = {2016-06-13},
  publisher = {Elsevier},
  isbn = {978-85-352-6119-6},
  langid = {portuguese},
  keywords = {Administração e serviços auxiliares}
}

@incollection{ContentbasedRecommenderSystems_2011_LopsEtAl,
  title = {Content-Based {{Recommender Systems}}: {{State}} of the {{Art}} and {{Trends}}},
  shorttitle = {Content-Based {{Recommender Systems}}},
  booktitle = {Recommender {{Systems Handbook}}},
  author = {Lops, Pasquale and family=Gemmis, given=Marco, prefix=de, useprefix=true and Semeraro, Giovanni},
  editor = {Ricci, Francesco and Rokach, Lior and Shapira, Bracha and Kantor, Paul B.},
  date = {2011},
  pages = {73--105},
  publisher = {Springer US},
  location = {Boston, MA},
  doi = {10.1007/978-0-387-85820-3_3},
  url = {https://doi.org/10.1007/978-0-387-85820-3_3},
  urldate = {2024-08-16},
  abstract = {Recommender systems have the effect of guiding users in a personalized way to interesting objects in a large space of possible options. Content-based recommendation systems try to recommend items similar to those a given user has liked in the past. Indeed, the basic process performed by a content-based recommender consists in matching up the attributes of a user profile in which preferences and interests are stored, with the attributes of a content object (item), in order to recommend to the user new interesting items. This chapter provides an overview of content-based recommender systems, with the aim of imposing a degree of order on the diversity of the different aspects involved in their design and implementation. The first part of the chapter presents the basic concepts and terminology of contentbased recommender systems, a high level architecture, and their main advantages and drawbacks. The second part of the chapter provides a review of the state of the art of systems adopted in several application domains, by thoroughly describing both classical and advanced techniques for representing items and user profiles. The most widely adopted techniques for learning user profiles are also presented. The last part of the chapter discusses trends and future research which might lead towards the next generation of systems, by describing the role of User Generated Content as a way for taking into account evolving vocabularies, and the challenge of feeding users with serendipitous recommendations, that is to say surprisingly interesting items that they might not have otherwise discovered.},
  isbn = {978-0-387-85820-3},
  langid = {english},
  keywords = {Collaborative Filter,Domain Ontology,Recommender System,Relevance Feedback,User Interest},
  file = {/home/baldoinov/baldoinov/PDFs/Mackenzie/projeto-aplicado-iii/Content-based Recommender Systems_2011_Lops et al.pdf}
}

@book{Contracultura_2016_Platt,
  title = {Contracultura},
  author = {Platt, David},
  date = {2016-01-01},
  edition = {1ª edição},
  publisher = {Edições Vida Nova},
  abstract = {SEJA BEM-VINDO ÀS LINHAS DE FRENTE.Para qualquer lado que nos voltemos, linhas de batalha estão sendo traçadas: casamento tradicional versus casamento gay, grupos pró-vida versus grupos pró-escolha, liberdade individual versus protecionismo estatal. Parece que a cultura mudou do dia para a noite, a ponto de o certo e o errado não mais serem definidos pela verdade universal, mas pela opinião popular. E, à proporção que conversas espinhosas sobre homossexualidade, aborto e liberdade religiosa continuam a despontar no local de trabalho, na igreja, na escola e nos lares, cristãos de toda parte fazem a mesma pergunta: COMO DEVEMOS RESPONDER A TUDO ISSO?Em Contracultura, David Platt, um dos autores mais vendidos segundo o New York Times, mostra aos seguidores de Cristo como assumir posição ativa em questões tão relevantes para os nossos dias. Também desafia os cristãos a se tornarem vozes incansáveis e apaixonadas em favor da causa de Cristo.},
  isbn = {978-85-275-0673-1},
  langid = {portuguese}
}

@online{ContrastiveAudioVisualMasked_2023_GongEtAl,
  title = {Contrastive {{Audio-Visual Masked Autoencoder}}},
  author = {Gong, Yuan and Rouditchenko, Andrew and Liu, Alexander H. and Harwath, David and Karlinsky, Leonid and Kuehne, Hilde and Glass, James},
  date = {2023-04-11},
  eprint = {2210.07839},
  eprinttype = {arXiv},
  eprintclass = {cs, eess},
  doi = {10.48550/arXiv.2210.07839},
  url = {http://arxiv.org/abs/2210.07839},
  urldate = {2023-06-17},
  abstract = {In this paper, we first extend the recent Masked Auto-Encoder (MAE) model from a single modality to audio-visual multi-modalities. Subsequently, we propose the Contrastive Audio-Visual Masked Auto-Encoder (CAV-MAE) by combining contrastive learning and masked data modeling, two major self-supervised learning frameworks, to learn a joint and coordinated audio-visual representation. Our experiments show that the contrastive audio-visual correspondence learning objective not only enables the model to perform audio-visual retrieval tasks, but also helps the model learn a better joint representation. As a result, our fully self-supervised pretrained CAV-MAE achieves a new SOTA accuracy of 65.9\% on VGGSound, and is comparable with the previous best supervised pretrained model on AudioSet in the audio-visual event classification task. Code and pretrained models are at https://github.com/yuangongnd/cav-mae.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Multimedia,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,notion},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Contrastive Audio-Visual Masked Autoencoder_2023_Gong et al.pdf;/home/baldoinov/Zotero/storage/GXJUTA7V/2210.html}
}

@incollection{Copyright_2021_Kissell,
  title = {Copyright},
  booktitle = {Algorithmic {{Trading Methods}} ({{Second Edition}})},
  editor = {Kissell, Robert L.},
  date = {2021-01-01},
  pages = {iv},
  publisher = {Academic Press},
  doi = {10.1016/B978-0-12-815630-8.12001-0},
  url = {https://www.sciencedirect.com/science/article/pii/B9780128156308120010},
  urldate = {2025-04-21},
  isbn = {978-0-12-815630-8}
}

@book{CourseGameTheory_2004_OsborneRubinstein,
  title = {A Course in Game Theory},
  author = {Osborne, Martin J. and Rubinstein, Ariel},
  date = {2004},
  edition = {10},
  publisher = {MIT Press},
  isbn = {978-0-262-15041-5 978-0-262-65040-3},
  langid = {english},
  pagetotal = {352}
}

@book{CrackingCodingInterview_2015_McDowell,
  title = {Cracking the Coding Interview: 189 Programming Questions and Solutions},
  shorttitle = {Cracking the Coding Interview},
  author = {McDowell, Gayle Laakmann},
  date = {2015-07-01},
  edition = {6th ed. edição},
  publisher = {Careercup},
  location = {Palo Alto, CA},
  abstract = {I am not a recruiter. I am a software engineer. And as such, I know what it's like to be asked to whip up brilliant algorithms on the spot and then write flawless code on a whiteboard. I've been through this as a candidate and as an interviewer.  Cracking the Coding Interview, 6th Edition is here to help you through this process, teaching you what you need to know and enabling you to perform at your very best. I've coached and interviewed hundreds of software engineers. The result is this book. Learn how to uncover the hints and hidden details in a question, discover how to break down a problem into manageable chunks, develop techniques to unstick yourself when stuck, learn (or re-learn) core computer science concepts, and practice on 189 interview questions and solutions. These interview questions are real; they are not pulled out of computer science textbooks. They reflect what's truly being asked at the top companies, so that you can be as prepared as possible. WHAT'S INSIDE?189 programming interview questions, ranging from the basics to the trickiest algorithm problems.A walk-through of how to derive each solution, so that you can learn how to get there yourself.Hints on how to solve each of the 189 questions, just like what you would get in a real interview.Five proven strategies to tackle algorithm questions, so that you can solve questions you haven't seen.Extensive coverage of essential topics, such as big O time, data structures, and core algorithms.A behind the scenes� look at how top companies like Google and Facebook hire developers.Techniques to prepare for and ace the soft side of the interview: behavioral questions.For interviewers and companies: details on what makes a good interview question and hiring process.},
  isbn = {978-0-9847828-5-7},
  langid = {Inglês},
  keywords = {notion}
}

@online{CreatingCustomPlotting_2020_Calderini,
  title = {Creating Custom Plotting Functions with Matplotlib},
  author = {Calderini, Matias},
  date = {2020-04-27},
  url = {https://towardsdatascience.com/creating-custom-plotting-functions-with-matplotlib-1f4b8eba6aa1},
  organization = {Towards Data Science}
}

@book{CristaoEmUma_2019_Stott,
  title = {O cristão em uma sociedade não cristã: Como posicionar-se biblicamente diante dos desafios contemporâneos},
  shorttitle = {O cristão em uma sociedade não cristã},
  author = {Stott, John},
  date = {2019-07-15},
  edition = {1ª edição},
  publisher = {Thomas Nelson Brasil},
  abstract = {Uma parcela considerável da Igreja Cristã se formou com a firme convicção de que a única relação que deveria ter com temas intrincados ― como política, diversidade, meio ambiente, aborto e tensão social, entre outros ―, limitava-se à oração. O raso envolvimento nas questões contemporâneas em nome do aprofundamento bíblico criou um abismo que John Stott tenta ajudar a superar com este livro. O cristão em uma sociedade não cristã é mais do que uma análise sobre a sociedade moderna e suas mazelas – é uma convocação ao pensamento crítico e ao engajamento a partir das verdades eternas contidas nas Escrituras Sagradas. Com a visão aguçada e abrangente que fez dele um dos maiores expoentes do cristianismo no século 20, Stott investiga as dinâmicas da sociedade, desde as discussões sobre gênero e eutanásia até os movimentos étnicos e religiosos que estimulam o terrorismo, passando pelos grandes riscos globais, como epidemias e agressões ao ambiente. O autor mostra como uma abordagem ampla e engajada do mundo que nos cerca é tão urgente quanto a intercessão e a proclamação do Reino. Lançada nos anos 1980, revisada e atualizada por Roy McCloughry e endossada pelo próprio Stott, esta obra – que conta ainda com um guia de estudo – continua sendo referência para todo cristão que compreende a relevância de seu papel em um cenário cultural cada vez mais complexo, onde a sede por compaixão, ética e justiça continua tão grande quanto no passado.},
  isbn = {978-85-7167-012-9},
  langid = {portuguese}
}

@book{CriticaRazaoNegra_2014_Mbembe,
  title = {Crítica da Razão Negra},
  author = {Mbembe, Achille},
  translator = {Lança, Marta},
  date = {2014},
  edition = {1a edição},
  publisher = {Antígona},
  location = {Lisboa, Portugal},
  isbn = {978-972-608-254-5},
  langid = {portuguese},
  annotation = {OCLC: 945553503}
}

@inproceedings{CrosslingualLanguageModel_2019_ConneauLample,
  title = {Cross-Lingual Language Model Pretraining},
  booktitle = {Proceedings of the 33rd {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Conneau, Alexis and Lample, Guillaume},
  date = {2019-12-08},
  pages = {7059--7069},
  publisher = {Curran Associates Inc.},
  location = {Red Hook, NY, USA},
  abstract = {Recent studies have demonstrated the efficiency of generative pretraining for English natural language understanding. In this work, we extend this approach to multiple languages and show the effectiveness of cross-lingual pretraining. We propose two methods to learn cross-lingual language models (XLMs): one unsu-pervised that only relies on monolingual data, and one supervised that leverages parallel data with a new cross-lingual language model objective. We obtain state-of-the-art results on cross-lingual classification, unsupervised and supervised machine translation. On XNLI, our approach pushes the state of the art by an absolute gain of 4.9\% accuracy. On unsupervised machine translation, we obtain 34.3 BLEU on WMT'16 German-English, improving the previous state of the art by more than 9 BLEU. On supervised machine translation, we obtain a new state of the art of 38.5 BLEU on WMT'16 Romanian-English, outperforming the previous best approach by more than 4 BLEU. Our code and pretrained models are publicly available.},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/projeto-heiwa/Cross-lingual language model pretraining_2019_Conneau et al.pdf}
}

@article{CrossvalidationPitfallsWhen_2014_KrstajicEtAl,
  title = {Cross-Validation Pitfalls When Selecting and Assessing Regression and Classification Models},
  author = {Krstajic, Damjan and Buturovic, Ljubomir J and Leahy, David E and Thomas, Simon},
  date = {2014-03-29},
  journaltitle = {Journal of Cheminformatics},
  shortjournal = {J Cheminform},
  volume = {6},
  eprint = {24678909},
  eprinttype = {pmid},
  pages = {10},
  issn = {1758-2946},
  doi = {10.1186/1758-2946-6-10},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3994246/},
  urldate = {2023-10-05},
  abstract = {Background We address the problem of selecting and assessing classification and regression models using cross-validation. Current state-of-the-art methods can yield models with high variance, rendering them unsuitable for a number of practical applications including QSAR. In this paper we describe and evaluate best practices which improve reliability and increase confidence in selected models. A key operational component of the proposed methods is cloud computing which enables routine use of previously infeasible approaches. Methods We describe in detail an algorithm for repeated grid-search V-fold cross-validation for parameter tuning in classification and regression, and we define a repeated nested cross-validation algorithm for model assessment. As regards variable selection and parameter tuning we define two algorithms (repeated grid-search cross-validation and double cross-validation), and provide arguments for using the repeated grid-search in the general case. Results We show results of our algorithms on seven QSAR datasets. The variation of the prediction performance, which is the result of choosing different splits of the dataset in V-fold cross-validation, needs to be taken into account when selecting and assessing classification and regression models. Conclusions We demonstrate the importance of repeating cross-validation when selecting an optimal model, as well as the importance of repeating nested cross-validation when assessing a prediction error. Electronic supplementary material The online version of this article (doi:10.1186/1758-2946-6-10) contains supplementary material, which is available to authorized users.},
  pmcid = {PMC3994246},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Cross-validation pitfalls when selecting and assessing regression and_2014_Krstajic et al.pdf}
}

@online{CuboItau__ItauUnibancoS.A.,
  title = {Cubo Itaú},
  author = {{Itaú Unibanco S.A.}},
  url = {https://cubo.network},
  urldate = {2024-10-30},
  abstract = {O Cubo Itaú é uma organização sem fins lucrativos que, desde 2015, conecta as melhores soluções para construir grandes cases de inovação para o mercado.},
  langid = {brazilian},
  file = {/home/baldoinov/Zotero/storage/KIKXJSKK/cubo.network.html}
}

@online{CuradoriaCursosOnline__,
  title = {Curadoria Cursos Online | Hard skills e Soft Skills},
  url = {https://www.estagiotrainee.com/curadoria-cursos-online-email},
  urldate = {2024-06-15},
  abstract = {Confira nossa curadoria de cursos hard skills e soft skills. Cursos importantes para se destacar nos processos seletivos concorridos.},
  langid = {portuguese},
  organization = {EstágioTrainee.com},
  file = {/home/baldoinov/Zotero/storage/IR2TXSI5/curadoria-cursos-online-email.html}
}

@article{curran_2004,
  title = {Lowland Forest Loss in Protected Areas of {{Indonesian Borneo}}},
  author = {Curran, L.M. and Trigg, S.N. and McDonald, A.K. and Astiani, D. and Hardiono, Y.M. and Siregar, P. and Caniago, I. and Kasischke, E.},
  date = {2004},
  journaltitle = {Science (New York, N.Y.)},
  shortjournal = {Science},
  volume = {303},
  number = {5660},
  pages = {1000--1003}
}

@inproceedings{CurriculumLearning_2009_BengioEtAl,
  title = {Curriculum Learning},
  booktitle = {Proceedings of the 26th {{Annual International Conference}} on {{Machine Learning}}},
  author = {Bengio, Yoshua and Louradour, Jérôme and Collobert, Ronan and Weston, Jason},
  date = {2009-06-14},
  series = {{{ICML}} '09},
  pages = {41--48},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/1553374.1553380},
  url = {https://dl.acm.org/doi/10.1145/1553374.1553380},
  urldate = {2023-09-28},
  abstract = {Humans and animals learn much better when the examples are not randomly presented but organized in a meaningful order which illustrates gradually more concepts, and gradually more complex ones. Here, we formalize such training strategies in the context of machine learning, and call them "curriculum learning". In the context of recent research studying the difficulty of training in the presence of non-convex training criteria (for deep deterministic and stochastic neural networks), we explore curriculum learning in various set-ups. The experiments show that significant improvements in generalization can be achieved. We hypothesize that curriculum learning has both an effect on the speed of convergence of the training process to a minimum and, in the case of non-convex criteria, on the quality of the local minima obtained: curriculum learning can be seen as a particular form of continuation method (a general strategy for global optimization of non-convex functions).},
  isbn = {978-1-60558-516-1},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Curriculum learning_2009_Bengio et al.pdf}
}

@book{CursoContabilidadePara_2017_BorinelliPimentel,
  title = {Curso de Contabilidade para Gestores, Analistas e Outros Profissionais},
  author = {Borinelli, Márcio Luiz and Pimentel, Renê Coppe},
  date = {2017-08-02},
  edition = {2},
  publisher = {Editora Atlas Ltda},
  isbn = {978-85-970139-5-5},
  langid = {brazilian}
}

@book{CursoTeorialMusical__EduardoFeldberg,
  title = {Curso de Teorial Musical Completo},
  author = {{Eduardo Feldberg}},
  url = {https://www.eduardofeldberg.com.br/curso-de-musica},
  urldate = {2024-06-09},
  abstract = {Curso de Teoria Musical Completo},
  langid = {portuguese},
  file = {/home/baldoinov/baldoinov/PDFs/Livros/Curso de Teorial Musical Completo_Eduardo Feldberg.pdf;/home/baldoinov/Zotero/storage/TVYWXIBS/curso-de-musica.html}
}

@inproceedings{CyclicalLearningRates_2017_Smith,
  title = {Cyclical {{Learning Rates}} for {{Training Neural Networks}}},
  booktitle = {2017 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  author = {Smith, Leslie N.},
  date = {2017-03},
  pages = {464--472},
  doi = {10.1109/WACV.2017.58},
  url = {https://ieeexplore.ieee.org/abstract/document/7926641},
  urldate = {2024-04-15},
  abstract = {It is known that the learning rate is the most important hyper-parameter to tune for training deep neural networks. This paper describes a new method for setting the learning rate, named cyclical learning rates, which practically eliminates the need to experimentally find the best values and schedule for the global learning rates. Instead of monotonically decreasing the learning rate, this method lets the learning rate cyclically vary between reasonable boundary values. Training with cyclical learning rates instead of fixed values achieves improved classification accuracy without a need to tune and often in fewer iterations. This paper also describes a simple way to estimate "reasonable bounds" - linearly increasing the learning rate of the network for a few epochs. In addition, cyclical learning rates are demonstrated on the CIFAR-10 and CIFAR-100 datasets with ResNets, Stochastic Depth networks, and DenseNets, and the ImageNet dataset with the AlexNet and GoogLeNet architectures. These are practical tools for everyone who trains neural networks.},
  eventtitle = {2017 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  keywords = {Computational efficiency,Computer architecture,Neural networks,Schedules,Training,Tuning},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/machine-learning-w-pytorch-n-sklearn/Cyclical Learning Rates for Training Neural Networks_2017_Smith.pdf;/home/baldoinov/Zotero/storage/IG2JWDTS/7926641.html}
}

@article{DARTDatasetArguments_2016_BoscEtAl,
  title = {{{DART}}: A {{Dataset}} of {{Arguments}} and Their {{Relations}} on {{Twitter}}},
  author = {Bosc, Tom and Cabrio, Elena and Villata, Serena},
  date = {2016-05},
  abstract = {The problem of understanding the stream of messages exchanged on social media such as Facebook and Twitter is becoming a major challenge for automated systems. The tremendous amount of data exchanged on these platforms as well as the specific form of language adopted by social media users constitute a new challenging context for existing argument mining techniques. In this paper, we describe a resource of natural language arguments called DART (Dataset of Arguments and their Relations on Twitter) where the complete argument mining pipeline over Twitter messages is considered: (i) we identify which tweets can be considered as arguments and which cannot, and (ii) we identify what is the relation, i.e., support or attack, linking such tweets to each other.},
  langid = {english},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/projeto-heiwa/DART_2016_Bosc et al.pdf}
}

@article{DashboardDesignWhy__BrathPeters,
  title = {Dashboard {{Design}}: {{Why Design}} Is {{Important}}},
  author = {Brath, Richard and Peters, Michael},
  langid = {english},
  file = {/home/baldoinov/baldoinov/PDFs/Mackenzie/topicos-de-banco-de-dados/Dashboard Design_Brath et al.pdf}
}

@video{DataAnalystHow_2024_ChristineJiang,
  entrysubtype = {video},
  title = {Data {{Analyst}} on {{How}} to {{Turn Business Metrics}} to {{Insights}}},
  editor = {{Christine Jiang}},
  editortype = {director},
  date = {2024},
  url = {https://www.youtube.com/watch?v=xlyLxvUfTzc},
  urldate = {2024-08-08},
  abstract = {JOIN ME LIVE this Saturday for a free workshop: https://bit.ly/cjlunchandlearn We'll discuss your business metrics questions, do an open Q\&A about the job hunt, and I'll share more about the next cohort of my mentorship program and how you can become take the most lean path to working in data. THIS VIDEO One of the magic ingredients to standing out in the job hunt is knowing how to translate data to real insights. In this video I walk through a simple framework and example of going from metric to recommendation - and how to actually relay these insights to stakeholders and hiring managers, so that you can stand out in the job market. ABOUT ME If we haven’t met yet - hi! I’m Christine, a former data director and hiring manager helping ambitious career transitioners and recent grads land their first job as a data analyst. I'll be sharing the insights and strategies that empowered over 70\% of job-hunting students in the first cohort of my mentorship program landed their first data job within 6 months, in today's super-competitive job market. If you want to become a standout data analyst through applied business skills, soft skills, and strategies for every part of the job hunt process - so that you can have confidence, professional leverage, and bring real impact to work - connect below for more insights and future workshops! LINKS - Get more tips to stand out as a data analyst here - https://bit.ly/3LodfYs - Apply to my mentorship program - https://theanalyticsaccelerator.com/ - Follow on LinkedIn - ~~/~christine-jiang~~ - Instagram - ~~/~cjiaang~~ (my photography, because why not) Timestamps: 0:00 - Introduction 0:32 - What we'll cover 1:03 - Metrics are the backbone of a strong analyst 2:28 - Popular metrics 101 3:49 - A framework to understanding metrics 5:45 - Metric to insight project example 9:24 - Get mentorship + community! Video URL: ~~~•~Data~Analyst~on~How~to~Turn~Business~...}
}

@book{DatabaseDesign_2014_WattWatt,
  title = {Database {{Design}}},
  author = {Watt, Adrienne and Watt, Adrienne},
  date = {2014-10-24},
  publisher = {BCcampus},
  url = {https://opentextbc.ca/dbdesign01/},
  urldate = {2025-01-26},
  langid = {canadian},
  file = {/home/baldoinov/baldoinov/PDFs/Livros/Database Design_2014_Watt et al.pdf}
}

@book{DatabaseDesignMere_2013_Hernandez,
  title = {Database {{Design}} for {{Mere Mortals}}: A {{Hands-On Guide}} to {{Relational Database Design}}},
  shorttitle = {Database Design for Mere Mortals},
  author = {Hernandez, Michael J.},
  date = {2013},
  edition = {Third edition},
  publisher = {Addison-Wesley},
  location = {Upper Saddle River, NJ},
  isbn = {978-0-321-88449-7},
  langid = {english},
  pagetotal = {610},
  keywords = {Database design,Relational databases}
}

@article{DataCitizenshipRethinking_2020_CarmiEtAl,
  title = {Data Citizenship: Rethinking Data Literacy in the Age of Disinformation, Misinformation, and Malinformation},
  shorttitle = {Data Citizenship},
  author = {Carmi, Elinor and Yates, Simeon J. and Lockley, Eleanor and Pawluczuk, Alicja},
  date = {2020-05-28},
  journaltitle = {Internet Policy Review},
  volume = {9},
  number = {2},
  issn = {2197-6775},
  url = {https://policyreview.info/articles/analysis/data-citizenship-rethinking-data-literacy-age-disinformation-misinformation-and},
  urldate = {2024-11-13},
  abstract = {In this paper we examine what data literacy means in the age of dis-/mis-/mal-information. We examine theoretical and methodological challenges researchers face when examining these two fields and how we can move forward by sharing our own experience in designing a survey to understand UK citizens data literacies.},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Data citizenship_2020_Carmi et al.pdf}
}

@book{DataClusteringAlgorithms_2014_AggarwalReddy,
  title = {Data Clustering: Algorithms and Applications},
  shorttitle = {Data Clustering},
  editor = {Aggarwal, Charu C. and Reddy, Chandan K.},
  date = {2014},
  series = {Chapman \& {{Hall}}/{{CRC}} Data Mining and Knowledge Discovery Series},
  publisher = {CRC Press},
  location = {Boca Raton},
  abstract = {"Clustering is a diverse topic, and the underlying algorithms depend greatly on the data domain and problem scenario. This book focuses on three primary aspects of data clustering: the core methods such as probabilistic, density-based, grid-based, and spectral clustering etc; different problem domains and scenarios such as multimedia, text, biological, categorical, network, and uncertain data as well as data streams; and different detailed insights from the clustering process because of the subjectivity of the clustering process, and the many different ways in which the same data set can be clustered"--},
  isbn = {978-1-4665-5821-2},
  langid = {english},
  pagetotal = {622},
  file = {/home/baldoinov/baldoinov/PDFs/Livros/Data clustering_2014_Aggarwal et al.pdf}
}

@online{DataLooksBetter__,
  title = {Data Looks Better Naked — {{Darkhorse Analytics}} | {{Edmonton}}, {{AB}}},
  url = {https://www.darkhorseanalytics.com/blog/data-looks-better-naked/},
  urldate = {2024-10-17},
  file = {/home/baldoinov/Zotero/storage/539ARVY2/data-looks-better-naked.html}
}

@article{DataMarketDesign_2023_RavindranathEtAl,
  title = {Data {{Market Design}} through {{Deep Learning}}},
  author = {Ravindranath, Sai Srivatsa and Jiang, Yanchen and Parkes, David C.},
  date = {2023},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2310.20096},
  url = {https://arxiv.org/abs/2310.20096},
  urldate = {2024-06-11},
  abstract = {The \$\textbackslash textit\{data market design\}\$ problem is a problem in economic theory to find a set of signaling schemes (statistical experiments) to maximize expected revenue to the information seller, where each experiment reveals some of the information known to a seller and has a corresponding price [Bergemann et al., 2018]. Each buyer has their own decision to make in a world environment, and their subjective expected value for the information associated with a particular experiment comes from the improvement in this decision and depends on their prior and value for different outcomes. In a setting with multiple buyers, a buyer's expected value for an experiment may also depend on the information sold to others [Bonatti et al., 2022]. We introduce the application of deep learning for the design of revenue-optimal data markets, looking to expand the frontiers of what can be understood and achieved. Relative to earlier work on deep learning for auction design [Dütting et al., 2023], we must learn signaling schemes rather than allocation rules and handle \$\textbackslash textit\{obedience constraints\}\$ \$-\$ these arising from modeling the downstream actions of buyers \$-\$ in addition to incentive constraints on bids. Our experiments demonstrate that this new deep learning framework can almost precisely replicate all known solutions from theory, expand to more complex settings, and be used to establish the optimality of new designs for data markets and make conjectures in regard to the structure of optimal designs.},
  version = {1},
  keywords = {Artificial Intelligence (cs.AI),Computer Science and Game Theory (cs.GT),FOS: Computer and information sciences},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Data Market Design through Deep Learning_2023_Ravindranath et al.pdf}
}

@book{DataScienceEconomics_2021_ConsoliEtAl,
  title = {Data {{Science}} for {{Economics}} and {{Finance}}: {{Methodologies}} and {{Applications}}},
  shorttitle = {Data {{Science}} for {{Economics}} and {{Finance}}},
  editor = {Consoli, Sergio and Reforgiato Recupero, Diego and Saisana, Michaela},
  date = {2021},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-66891-4},
  url = {https://link.springer.com/10.1007/978-3-030-66891-4},
  urldate = {2024-04-01},
  isbn = {978-3-030-66890-7 978-3-030-66891-4},
  langid = {english}
}

@book{DataScienceScratch_2019_Grus,
  title = {Data {{Science}} from {{Scratch}}: First Principles with {{Python}}},
  shorttitle = {Data Science from {{Scratch}}},
  author = {Grus, Joel},
  date = {2019},
  edition = {Second edition},
  publisher = {O'Reilly},
  location = {Beijing Boston Farnham Sebastopol Tokyo},
  isbn = {978-1-4920-4113-9},
  langid = {english},
  pagetotal = {384}
}

@book{DataScienceZero_2021_Grus,
  title = {Data Science do zero},
  author = {Grus, Joel},
  date = {2021-10-14},
  edition = {1},
  publisher = {Alta Books},
  isbn = {978-85-508-0387-6},
  langid = {portuguese}
}

@book{DataStructuresAlgorithms_2013_GoodrichEtAl,
  title = {Data {{Structures}} and {{Algorithms}} in {{Python}}},
  author = {Goodrich, Michael T and Tamassia, Roberto and Goldwasser, Michael H},
  date = {2013-03-18},
  isbn = {978-1-118-29027-9},
  langid = {english},
  file = {/home/baldoinov/baldoinov/PDFs/Livros/Data Structures and Algorithms in Python_2013_Goodrich et al.pdf}
}

@audio{DataVisualizationData__MoniqueFemmeEtAl,
  title = {Data {{Visualization}} \& {{Data Storytelling}} - {{Data Hackers Podcast}} \#87},
  author = {{Monique Femme} and {Paulo Vasconcellos} and {Gabriel Lages} and {Letícia Pozza}},
  number = {87},
  url = {https://open.spotify.com/episode/4ZXyFa71c8hICo6cIpGO8S?si=NK824yA9Q8GDT0qWCEo9UQ},
  urldate = {2024-06-14},
  abstract = {Para explorar técnicas poderosas de como transformar conjuntos de dados complexos em histórias envolventes e insights assertivos, direto de Barcelona, convidamos {$\mkern1mu$}Letícia Pozza —{$\mkern1mu$} que teve experiência na implementação de iniciativas de análise de dados no Brasil e em pesquisas apoiadas pela Fundação Bill \& Melinda Gates e atualmente, é CEO e Co-fundadora da Odd Studio.}
}

@online{DeepARProbabilisticForecasting_2019_SalinasEtAl,
  title = {{{DeepAR}}: {{Probabilistic Forecasting}} with {{Autoregressive Recurrent Networks}}},
  shorttitle = {{{DeepAR}}},
  author = {Salinas, David and Flunkert, Valentin and Gasthaus, Jan},
  date = {2019-02-22},
  eprint = {1704.04110},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.1704.04110},
  url = {http://arxiv.org/abs/1704.04110},
  urldate = {2024-11-03},
  abstract = {Probabilistic forecasting, i.e. estimating the probability distribution of a time series' future given its past, is a key enabler for optimizing business processes. In retail businesses, for example, forecasting demand is crucial for having the right inventory available at the right time at the right place. In this paper we propose DeepAR, a methodology for producing accurate probabilistic forecasts, based on training an auto regressive recurrent network model on a large number of related time series. We demonstrate how by applying deep learning techniques to forecasting, one can overcome many of the challenges faced by widely-used classical approaches to the problem. We show through extensive empirical evaluation on several real-world forecasting data sets accuracy improvements of around 15\% compared to state-of-the-art methods.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Recovery/Modelo de Incidencia de Acao Contra/DeepAR_2019_Salinas et al.pdf;/home/baldoinov/Zotero/storage/5SZRKWMX/1704.html}
}

@book{DeepLearning_2016_Goodfellow,
  title = {Deep Learning},
  author = {Goodfellow, Ian},
  date = {2016-01-01},
  publisher = {The Mit Press},
  location = {Cambridge, Massachusetts},
  abstract = {Written by three experts in the field, Deep Learning is the only comprehensive book on the subject." -- Elon Musk, cochair of OpenAI; cofounder and CEO of Tesla and SpaceX Deep learning is a form of machine learning that enables computers to learn fr om experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hier archy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning. The text offers mathemati cal and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including de ep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recom mendation systems, bioinformatics, and videogames. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods,},
  isbn = {978-0-262-03561-3},
  langid = {Inglês}
}

@article{DeepLearningBased_2021_MinaeeEtAl,
  title = {Deep {{Learning--based Text Classification}}: {{A Comprehensive Review}}},
  shorttitle = {Deep {{Learning--based Text Classification}}},
  author = {Minaee, Shervin and Kalchbrenner, Nal and Cambria, Erik and Nikzad, Narjes and Chenaghlu, Meysam and Gao, Jianfeng},
  date = {2021-04-17},
  journaltitle = {ACM Comput. Surv.},
  volume = {54},
  number = {3},
  pages = {62:1--62:40},
  issn = {0360-0300},
  doi = {10.1145/3439726},
  url = {https://doi.org/10.1145/3439726},
  urldate = {2024-08-27},
  abstract = {Deep learning--based models have surpassed classical machine learning--based approaches in various text classification tasks, including sentiment analysis, news categorization, question answering, and natural language inference. In this article, we provide a comprehensive review of more than 150 deep learning--based models for text classification developed in recent years, and we discuss their technical contributions, similarities, and strengths. We also provide a summary of more than 40 popular datasets widely used for text classification. Finally, we provide a quantitative analysis of the performance of different deep learning models on popular benchmarks, and we discuss future research directions.},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/projeto-heiwa/Deep Learning--based Text Classification_2021_Minaee et al.pdf}
}

@online{DeepLearningEconomists_2024_Dell,
  title = {Deep {{Learning}} for {{Economists}}},
  author = {Dell, Melissa},
  date = {2024-07-21},
  eprint = {2407.15339},
  eprinttype = {arXiv},
  eprintclass = {cs, econ, q-fin},
  doi = {10.48550/arXiv.2407.15339},
  url = {http://arxiv.org/abs/2407.15339},
  urldate = {2024-08-07},
  abstract = {Deep learning provides powerful methods to impute structured information from large-scale, unstructured text and image datasets. For example, economists might wish to detect the presence of economic activity in satellite images, or to measure the topics or entities mentioned in social media, the congressional record, or firm filings. This review introduces deep neural networks, covering methods such as classifiers, regression models, generative AI, and embedding models. Applications include classification, document digitization, record linkage, and methods for data exploration in massive scale text and image corpora. When suitable methods are used, deep learning models can be cheap to tune and can scale affordably to problems involving millions or billions of data points.. The review is accompanied by a companion website, EconDL, with user-friendly demo notebooks, software resources, and a knowledge base that provides technical details and additional applications.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Economics - General Economics},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Deep Learning for Economists_2024_Dell.pdf;/home/baldoinov/Zotero/storage/L78XJN7X/2407.html}
}

@book{DeepLearningGraphs_2021_MaTang,
  title = {Deep {{Learning}} on {{Graphs}}},
  author = {Ma, Yao and Tang, Jiliang},
  date = {2021-12-09},
  edition = {1st edition},
  publisher = {Cambridge University Press},
  location = {Cambridge},
  abstract = {Deep learning on graphs has become one of the hottest topics in machine learning. The book consists of four parts to best accommodate our readers with diverse backgrounds and purposes of reading. Part 1 introduces basic concepts of graphs and deep learning; Part 2 discusses the most established methods from the basic to advanced settings; Part 3 presents the most typical applications including natural language processing, computer vision, data mining, biochemistry and healthcare; and Part 4 describes advances of methods and applications that tend to be important and promising for future research. The book is self-contained, making it accessible to a broader range of readers including (1) senior undergraduate and graduate students; (2) practitioners and project managers who want to adopt graph neural networks into their products and platforms; and (3) researchers without a computer science background who want to use graph neural networks to advance their disciplines.},
  isbn = {978-1-108-83174-1},
  langid = {english},
  pagetotal = {400},
  keywords = {notion}
}

@book{DeepLearningInterviews_2022_KashaniIvry,
  title = {Deep {{Learning Interviews}}: {{Hundreds}} of Fully Solved Job Interview Questions from a Wide Range of Key Topics in {{AI}}},
  shorttitle = {Deep {{Learning Interviews}}},
  author = {Kashani, Shlomo and Ivry, Amir},
  date = {2022-01-04},
  eprint = {2201.00650},
  eprinttype = {arXiv},
  eprintclass = {cs, math},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2201.00650},
  urldate = {2023-05-25},
  abstract = {The second edition of Deep Learning Interviews is home to hundreds of fully-solved problems, from a wide range of key topics in AI. It is designed to both rehearse interview or exam specific topics and provide machine learning MSc / PhD. students, and those awaiting an interview a well-organized overview of the field. The problems it poses are tough enough to cut your teeth on and to dramatically improve your skills-but they're framed within thought-provoking questions and engaging stories. That is what makes the volume so specifically valuable to students and job seekers: it provides them with the ability to speak confidently and quickly on any relevant topic, to answer technical questions clearly and correctly, and to fully understand the purpose and meaning of interview questions and answers. Those are powerful, indispensable advantages to have when walking into the interview room. The book's contents is a large inventory of numerous topics relevant to DL job interviews and graduate level exams. That places this work at the forefront of the growing trend in science to teach a core set of practical mathematical and computational skills. It is widely accepted that the training of every computer scientist must include the fundamental theorems of ML, and AI appears in the curriculum of nearly every university. This volume is designed as an excellent reference for graduates of such programs.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Information Theory,Computer Science - Machine Learning,notion},
  file = {/home/baldoinov/baldoinov/PDFs/Livros/Deep Learning Interviews_2022_Kashani et al.pdf}
}

@article{DeepLearningMeets_2024_SankarEtAl,
  title = {Deep {{Learning Meets Mechanism Design}}: {{Key Results}} and {{Some Novel Applications}}},
  shorttitle = {Deep {{Learning Meets Mechanism Design}}},
  author = {Sankar, V. Udaya and Rao, Vishisht Srihari and Narahari, Y.},
  date = {2024},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2401.05683},
  url = {https://arxiv.org/abs/2401.05683},
  urldate = {2024-06-11},
  abstract = {Mechanism design is essentially reverse engineering of games and involves inducing a game among strategic agents in a way that the induced game satisfies a set of desired properties in an equilibrium of the game. Desirable properties for a mechanism include incentive compatibility, individual rationality, welfare maximisation, revenue maximisation (or cost minimisation), fairness of allocation, etc. It is known from mechanism design theory that only certain strict subsets of these properties can be simultaneously satisfied exactly by any given mechanism. Often, the mechanisms required by real-world applications may need a subset of these properties that are theoretically impossible to be simultaneously satisfied. In such cases, a prominent recent approach is to use a deep learning based approach to learn a mechanism that approximately satisfies the required properties by minimizing a suitably defined loss function. In this paper, we present, from relevant literature, technical details of using a deep learning approach for mechanism design and provide an overview of key results in this topic. We demonstrate the power of this approach for three illustrative case studies: (a) efficient energy management in a vehicular network (b) resource allocation in a mobile network (c) designing a volume discount procurement auction for agricultural inputs. Section 6 concludes the paper.},
  version = {1},
  keywords = {Artificial Intelligence (cs.AI),Computer Science and Game Theory (cs.GT),FOS: Computer and information sciences},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Deep Learning Meets Mechanism Design_2024_Sankar et al.pdf}
}

@book{DeepLearningPython_2021_Chollet,
  title = {Deep {{Learning}} with {{Python}}},
  author = {Chollet, Francois},
  date = {2021-12-21},
  edition = {2nd edition},
  publisher = {Manning},
  location = {Shelter Island, NY},
  abstract = {Printed in full color! Unlock the groundbreaking advances of deep learning with this extensively revised new edition of the bestselling original. Learn directly from the creator of Keras and master practical Python deep learning techniques that are easy to apply in the real world.In Deep Learning with Python, Second Edition you will learn:  Deep learning from first principles Image classification and image segmentation Timeseries forecasting Text classification and machine translation Text generation, neural style transfer, and image generation Full color printing throughout  Deep Learning with Python has taught thousands of readers how to put the full capabilities of deep learning into action. This extensively revised full color second edition introduces deep learning using Python and Keras, and is loaded with insights for both novice and experienced ML practitioners. You’ll learn practical techniques that are easy to apply in the real world, and important theory for perfecting neural networks.  Purchase of the print book includes a free eBook in PDF, Kindle, and ePub formats from Manning Publications.  About the technology Recent innovations in deep learning unlock exciting new software capabilities like automated language translation, image recognition, and more. Deep learning is quickly becoming essential knowledge for every software developer, and modern tools like Keras and TensorFlow put it within your reach—even if you have no background in mathematics or data science. This book shows you how to get started.  About the book Deep Learning with Python, Second Edition introduces the field of deep learning using Python and the powerful Keras library. In this revised and expanded new edition, Keras creator François Chollet offers insights for both novice and experienced machine learning practitioners. As you move through this book, you’ll build your understanding through intuitive explanations, crisp color illustrations, and clear examples. You’ll quickly pick up the skills you need to start developing deep-learning applications.  What's inside  Deep learning from first principles Image classification and image segmentation Time series forecasting Text classification and machine translation Text generation, neural style transfer, and image generation Full color printing throughout  About the reader For readers with intermediate Python skills. No previous experience with Keras, TensorFlow, or machine learning is required.  About the author François Chollet is a software engineer at Google and creator of the Keras deep-learning library.  Table of Contents 1 What is deep learning? 2 The mathematical building blocks of neural networks 3 Introduction to Keras and TensorFlow 4 Getting started with neural networks: Classification and regression 5 Fundamentals of machine learning 6 The universal workflow of machine learning 7 Working with Keras: A deep dive 8 Introduction to deep learning for computer vision 9 Advanced deep learning for computer vision 10 Deep learning for timeseries 11 Deep learning for text 12 Generative deep learning 13 Best practices for the real world 14 Conclusions},
  isbn = {978-1-61729-686-4},
  langid = {english},
  pagetotal = {504},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Livros/Deep Learning with Python_2021_Chollet.pdf}
}

@book{DeepLearningPyTorch_2020_StevensEtAl,
  title = {Deep {{Learning}} with {{PyTorch}}: {{Build}}, Train, and Tune Neural Networks Using {{Python}} Tools},
  shorttitle = {Deep {{Learning}} with {{PyTorch}}},
  author = {Stevens, Eli and Antiga, Luca and Viehmann, Thomas},
  date = {2020-08-04},
  edition = {First Edition},
  publisher = {Manning},
  location = {Shelter Island, NY},
  abstract = {We finally have the definitive treatise on PyTorch! It covers the basics and abstractions in great detail. I hope this book becomes your extended reference document. —Soumith Chintala, co-creator of PyTorch Key features: Written by PyTorchs creator and key contributors Develop deep learning models in a familiar Pythonic way Use PyTorch to build an image classifier for cancer detection Diagnose problems with your neural network and improve training with data augmentation Purchase of the print book includes a free eBook in PDF, Kindle, and ePub formats from Manning Publications. About The Book Every other day we hear about new ways to put deep learning to good use: improved medical imaging, accurate credit card fraud detection, long range weather forecasting, and more. PyTorch puts these superpowers in your hands. Instantly familiar to anyone who knows Python data tools like NumPy and Scikit-learn, PyTorch simplifies deep learning without sacrificing advanced features. Its great for building quick models, and it scales smoothly from laptop to enterprise. Deep Learning with PyTorch teaches you to create deep learning and neural network systems with PyTorch. This practical book gets you to work right away building a tumor image classifier from scratch. After covering the basics, youll learn best practices for the entire deep learning pipeline, tackling advanced projects as your PyTorch skills become more sophisticated. All code samples are easy to explore in downloadable Jupyter notebooks. What You Will Learn Understanding deep learning data structures such as tensors and neural networks Best practices for the PyTorch Tensor API, loading data in Python, and visualizing results Implementing modules and loss functions Utilizing pretrained models from PyTorch Hub Methods for training networks with limited inputs Sifting through unreliable results to diagnose and fix problems in your neural network Improve your results with augmented data, better model architecture, and fine tuning This Book Is Written For For Python programmers with an interest in machine learning. No experience with PyTorch or other deep learning frameworks is required. About The Authors Eli Stevens has worked in Silicon Valley for the past 15 years as a software engineer, and the past 7 years as Chief Technical Officer of a startup making medical device software. Luca Antiga is co-founder and CEO of an AI engineering company located in Bergamo, Italy, and a regular contributor to PyTorch. Thomas Viehmann is a Machine Learning and PyTorch speciality trainer and consultant based in Munich, Germany and a PyTorch core developer. Table of Contents Part 1 - Core Pytorch. 1 Introducing deep learning and the PyTorch Library 2 Pretrained networks 3 It starts with a tensor 4 Real-world data representation using tensors 5 The mechanics of learning 6 Using a neural network to fit the data 7 Telling birds from airplanes: Learning from images 8 Using convolutions to generalize PART 2 - LEARNING FROM IMAGES IN THE REAL WORLD: EARLY DETECTION OF LUNG CANCER 9 Using PyTorch to fight cancer 10 Combining data sources into a unified dataset 11 Training a classification model to detect suspected tumors 12 Improving training with metrics and augmentation 13 Using segmentation to find suspected nodules 14 End-to-end nodule analysis, and where to go next PART 3 - DEPLOYMENT 15 Deploying to production},
  isbn = {978-1-61729-526-3},
  langid = {english},
  pagetotal = {520}
}

@online{DeepReinforcementLearning_2018_Li,
  title = {Deep {{Reinforcement Learning}}},
  author = {Li, Yuxi},
  date = {2018-10-15},
  url = {https://arxiv.org/abs/1810.06339v1},
  urldate = {2024-04-19},
  abstract = {We discuss deep reinforcement learning in an overview style. We draw a big picture, filled with details. We discuss six core elements, six important mechanisms, and twelve applications, focusing on contemporary work, and in historical contexts. We start with background of artificial intelligence, machine learning, deep learning, and reinforcement learning (RL), with resources. Next we discuss RL core elements, including value function, policy, reward, model, exploration vs. exploitation, and representation. Then we discuss important mechanisms for RL, including attention and memory, unsupervised learning, hierarchical RL, multi-agent RL, relational RL, and learning to learn. After that, we discuss RL applications, including games, robotics, natural language processing (NLP), computer vision, finance, business management, healthcare, education, energy, transportation, computer systems, and, science, engineering, and art. Finally we summarize briefly, discuss challenges and opportunities, and close with an epilogue.},
  langid = {english},
  pubstate = {prepublished},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Deep Reinforcement Learning_2018_Li.pdf}
}

@article{DeepReinforcementLearning_2020_WangEtAl,
  title = {Deep Reinforcement Learning: A Survey},
  shorttitle = {Deep Reinforcement Learning},
  author = {Wang, Hao-nan and Liu, Ning and Zhang, Yi-yun and Feng, Da-wei and Huang, Feng and Li, Dong-sheng and Zhang, Yi-ming},
  date = {2020-12-01},
  journaltitle = {Frontiers of Information Technology \& Electronic Engineering},
  shortjournal = {Front Inform Technol Electron Eng},
  volume = {21},
  number = {12},
  pages = {1726--1744},
  issn = {2095-9230},
  doi = {10.1631/FITEE.1900533},
  url = {https://doi.org/10.1631/FITEE.1900533},
  urldate = {2023-12-05},
  abstract = {Deep reinforcement learning (RL) has become one of the most popular topics in artificial intelligence research. It has been widely used in various fields, such as end-to-end control, robotic control, recommendation systems, and natural language dialogue systems. In this survey, we systematically categorize the deep RL algorithms and applications, and provide a detailed review over existing deep RL algorithms by dividing them into modelbased methods, model-free methods, and advanced RL methods. We thoroughly analyze the advances including exploration, inverse RL, and transfer RL. Finally, we outline the current representative applications, and analyze four open problems for future research.},
  langid = {english},
  keywords = {Deep reinforcement learning,notion,Reinforcement learning,Reinforcement learning applications,TP18},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Deep reinforcement learning_2020_Wang et al.pdf}
}

@article{degroot_1984,
  title = {The Impact of Bottom Trawling on Benthic Fauna of the {{North Sea}}},
  author = {De Groot, S.J.},
  date = {1984},
  journaltitle = {Ocean management},
  volume = {9},
  pages = {177--190}
}

@book{DesenhandoComLado_2006_Edwards,
  title = {Desenhando Com O Lado Direito Do Cérebro},
  author = {Edwards, Betty},
  date = {2006-01-01},
  edition = {3ª edição},
  publisher = {Ediouro},
  abstract = {Traduzido em treze idiomas mais de dois milhões e meio de exemplares vendidos "Desenhando com o Lado Direito do Cérebro" é o livro de ensino de desenho mais utilizado em todo o mundo. Tanto para os que se acham com pouco talento ou duvidam que um dia serão capazes de aprender quanto para os artistas profissionais o livro mostrará como adquirir a habilidade desejada como confiar nesta habilidade e como aprofundar a percepção artística.Minuciosamente revista e atualizada esta edição do Desenhando com o Lado Direito do Cérebro traz como novidade: progressos recentes de pesquisas sobre o cérebro relacionadas ao desenho; novas abordagens no uso de técnicas de desenho no mundo empresarial e na educação; orientações para a expressão pessoal através do desenho; maneiras de avançar do desenho em branco e preto para o colorido; informações detalhadas quanto à aplicação das cinco habilidades básicas do desenho para resolver problemas.},
  isbn = {978-85-00-00748-4},
  langid = {portuguese}
}

@book{DesenvolvimentoEconomicoUma_2013_VelosoEtAl,
  title = {Desenvolvimento {{Econômico}}: Uma {{Perspectiva Brasileira}}},
  shorttitle = {Desenvolvimento Econômico},
  editor = {Veloso, Fernando and Ferreira, Pedro Cavalcanti and Giambiagi, Fabio and Pessôa, Samuel de Abreu and Barros, Alexandre Rands},
  date = {2013},
  publisher = {Elsevier},
  location = {Rio de Janeiro, RJ, Brasil},
  isbn = {978-85-352-5155-5},
  pagetotal = {449},
  keywords = {Brazil,Economic conditions,Economic development,Economic policy}
}

@book{DesigningDataIntensiveApplications_2017_Kleppmann,
  title = {Designing {{Data-Intensive Applications}}: The Big Ideas behind Reliable, Scalable, and Maintainable Systems},
  shorttitle = {Designing Data-Intensive Applications},
  author = {Kleppmann, Martin},
  date = {2017},
  edition = {First edition},
  publisher = {O'Reilly Media},
  location = {Boston},
  abstract = {Data is at the center of many challenges in system design today. Difficult issues need to be figured out, such as scalability, consistency, reliability, efficiency, and mainteinability. In addition, we have an overwhelming variet of tools, including relational databases, NoSQL datastores, stream or batch processors, and message brokers. What are the right choices for your application? How do you make sense of all these buzzwords? In this practical and comprehensive gjuide, author Martin Kleppmann helps you navigate this diverse landscape by examining the pros and cons of various technologies for processing and storing data. Software keeps changing, but the fundamental principles remain the same. With this book, software engineers and architects will learn how to apply those ideas in practice, and how to make full use of data in modern applications},
  isbn = {978-1-4493-7332-0},
  pagetotal = {590},
  keywords = {Application software,Database management,Development,Web site development},
  annotation = {OCLC: ocn893895983}
}

@book{DesigningMachineLearning_2022_Huyen,
  title = {Designing Machine Learning Systems: An Iterative Process for Production-Ready Applications},
  shorttitle = {Designing Machine Learning Systems},
  author = {Huyen, Chip},
  date = {2022-06-21},
  edition = {1ª edição},
  publisher = {O'Reilly Media},
  location = {Beijing Boston Farnham Sebastopol Tokyo},
  abstract = {Machine learning systems are both complex and unique. Complex because they consist of many different components and involve many different stakeholders. Unique because they're data dependent, with data varying wildly from one use case to the next. In this book, you'll learn a holistic approach to designing ML systems that are reliable, scalable, maintainable, and adaptive to changing environments and business requirements.  Author Chip Huyen, co-founder of Claypot AI, considers each design decision--such as how to process and create training data, which features to use, how often to retrain models, and what to monitor--in the context of how it can help your system as a whole achieve its objectives. The iterative framework in this book uses actual case studies backed by ample references.  This book will help you tackle scenarios such as: Engineering data and choosing the right metrics to solve a business problemAutomating the process for continually developing, evaluating, deploying, and updating modelsDeveloping a monitoring system to quickly detect and address issues your models might encounter in productionArchitecting an ML platform that serves across use casesDeveloping responsible ML systems},
  isbn = {978-1-09-810796-3},
  langid = {Inglês}
}

@article{DesignScienceResearch_2020_PimentelEtAl,
  title = {Design Science Research: pesquisa científica atrelada ao design de artefatos},
  shorttitle = {Design Science Research},
  author = {Pimentel, Mariano and Filippo, Denise and family=Santos, given=Thiago Marcondes, prefix=dos, useprefix=false},
  date = {2020-05-26},
  journaltitle = {RE@D - Revista de Educação a Distância e Elearning},
  volume = {3},
  number = {1},
  pages = {37--61},
  issn = {2182-4967},
  doi = {10.34627/vol3iss1pp37-61},
  url = {https://revistas.rcaap.pt/lead_read/article/view/21898},
  urldate = {2024-08-23},
  abstract = {In Education, we develop artifacts: classes, didactic resources and educational practices. It is possible to interrelate the development of an artifact with the production of theoretical knowledge, and Design Science Research (DSR) is the approach we present on this article, used to do scientific research connected to the development of artifacts. Through this approach, the goal is to design a different reality, modified by artifacts designed to solve problems in given contexts, with the scientific knowledge being the result of the investigation of the use of the artifact in a given situation. DSR is an approach that is still under discussion; in this article, we show how our research group has been appropriating DSR. In particular, we present the DSR Model in which we synthesized the main lessons learned by our group. In order to exemplify the use of the DSR Model, we present the research on Smart Musical Mats. We consider that the DSR Model has been helping make our group’s researches more rigorous in theoretical, epistemological and methodological terms and, thus, more relevant in scientific terms. We present this model because we consider it to be a useful instrument to support other researchers that want to consider and use DSR.\&nbsp;\&nbsp;\&nbsp;\&nbsp;},
  issue = {1},
  langid = {portuguese},
  keywords = {Design Science Research,Modelo,Pesquisa-Design,Processo de pesquisa},
  file = {/home/baldoinov/baldoinov/PDFs/Diversos/Design Science Research_2020_Pimentel et al.pdf}
}

@article{DesigualdadeRendaNo_2006_BarrosEtAl,
  title = {Desigualdade de renda no Brasil: uma análise da queda recente},
  shorttitle = {Desigualdade de renda no Brasil},
  author = {Barros, Ricardo Paes de (Organizador) and Foguel, Miguel Nathan (Organizador) and Ulyssea, Gabriel (Organizador)},
  date = {2006},
  journaltitle = {http://www.ipea.gov.br},
  publisher = {Instituto de Pesquisa Econômica Aplicada (Ipea)},
  url = {https://repositorio.ipea.gov.br/handle/11058/3249},
  urldate = {2024-05-07},
  abstract = {Reúne estudos voltados para estimar a magnitude da queda recente na desigualdade e suas consequências sobre as condições de vida da população mais pobre; e aqueles cujo objetivo é identificar os principais fatores determinantes por trás desse movimento. Analisa, em detalhes, as transformações por que passaram os diversos tipos de transferências governamentais, principalmente as pensões e as aposentadorias, o Benefício de Prestação Continuada (BPC) e o Programa Bolsa Família (PBF). Trata dos fatores responsáveis pelas transformações na distribuição dos rendimentos do trabalho. Avalia o papel da educação e da experiência potencial dos trabalhadores no mercado de trabalho para a redução da desigualdade de renda. Trata do mercado de trabalho como gerador de desigualdade. A análise é centrada nos papéis desempenhados pela discriminação de gênero e de cor, bem como por três tipos de segmentação: setorial, formal-informal e espacial. Aborda os efeitos do salário mínimo sobre a desigualdade de renda por meio das remunerações pagas no mercado de trabalho, assim como das transferências governamentais a ele vinculadas, visando contribuir para o aprimoramento das políticas públicas e, dessa forma, acelerar o processo de redução da extrema desigualdade de renda que ainda prevalece no País.},
  langid = {brazilian},
  annotation = {Accepted: 2014-11-25T17:02:41Z},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Desigualdade de renda no Brasil_2006_Barros et al.pdf}
}

@article{desombre_2000,
  title = {The Experience of the {{Montreal Protocol}}: {{Particularly}} Remarkable, and Remarkably Particular},
  author = {DeSombre, E.R.},
  date = {2000},
  journaltitle = {UCLA J. Envtl. L. \& Pol'y},
  volume = {19},
  pages = {49}
}

@online{DifferentWaysDeploy_2021_Gallatin,
  title = {5 {{Different Ways}} to {{Deploy}} Your {{Machine Learning Model}} with {{AWS}}},
  author = {Gallatin, Kyle},
  date = {2021-07-19T12:15:02},
  url = {https://towardsdatascience.com/5-different-ways-to-deploy-your-machine-learning-model-with-aws-bd676ab5f8d4},
  urldate = {2024-07-04},
  abstract = {On the pros and cons of different approaches to getting your model out into the world.},
  langid = {english},
  organization = {Medium},
  file = {/home/baldoinov/Zotero/storage/PX3EGZMV/5-different-ways-to-deploy-your-machine-learning-model-with-aws-bd676ab5f8d4.html}
}

@article{DificilTarefaDefinir_2004_Munanga,
  title = {A difícil tarefa de definir quem é negro no Brasil},
  author = {Munanga, Kabengele},
  date = {2004-04-01},
  journaltitle = {Estudos Avançados},
  volume = {18},
  number = {50},
  pages = {51--66},
  issn = {1806-9592},
  url = {https://www.revistas.usp.br/eav/article/view/9968},
  urldate = {2024-04-09},
  issue = {50},
  langid = {portuguese},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/A dificil tarefa de definir quem e negro no Brasil_2004_Munanga.pdf}
}

@online{DistilBERTDistilledVersion_2020_SanhEtAl,
  title = {{{DistilBERT}}, a Distilled Version of {{BERT}}: Smaller, Faster, Cheaper and Lighter},
  shorttitle = {{{DistilBERT}}, a Distilled Version of {{BERT}}},
  author = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  date = {2020-02-29},
  eprint = {1910.01108},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1910.01108},
  url = {http://arxiv.org/abs/1910.01108},
  urldate = {2024-08-27},
  abstract = {As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40\%, while retaining 97\% of its language understanding capabilities and being 60\% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.},
  pubstate = {prepublished},
  version = {4},
  keywords = {Computer Science - Computation and Language},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/projeto-heiwa/DistilBERT, a distilled version of BERT_2020_Sanh et al.pdf;/home/baldoinov/Zotero/storage/2VM5H26L/1910.html}
}

@online{DistributedInferenceFinetuning_2023_BorzunovEtAl,
  title = {Distributed {{Inference}} and {{Fine-tuning}} of {{Large Language Models Over The Internet}}},
  author = {Borzunov, Alexander and Ryabinin, Max and Chumachenko, Artem and Baranchuk, Dmitry and Dettmers, Tim and Belkada, Younes and Samygin, Pavel and Raffel, Colin},
  date = {2023-12-13},
  eprint = {2312.08361},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2312.08361},
  url = {http://arxiv.org/abs/2312.08361},
  urldate = {2023-12-18},
  abstract = {Large language models (LLMs) are useful in many NLP tasks and become more capable with size, with the best open-source models having over 50 billion parameters. However, using these 50B+ models requires high-end hardware, making them inaccessible to most researchers. In this work, we investigate methods for cost-efficient inference and fine-tuning of LLMs, comparing local and distributed strategies. We observe that a large enough model (50B+) can run efficiently even on geodistributed devices in a consumer-grade network. This could allow running LLM efficiently by pooling together idle compute resources of multiple research groups and volunteers. We address two open problems: (1) how to perform inference and fine-tuning reliably if any device can disconnect abruptly and (2) how to partition LLMs between devices with uneven hardware, joining and leaving at will. In order to do that, we develop special fault-tolerant inference algorithms and load-balancing protocols that automatically assign devices to maximize the total system throughput. We showcase these algorithms in Petals - a decentralized system that runs Llama 2 (70B) and BLOOM (176B) over the Internet up to 10x faster than offloading for interactive generation. We evaluate the performance of our system in simulated conditions and a real-world setup spanning two continents.},
  pubstate = {prepublished},
  keywords = {Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Machine Learning,notion},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Distributed Inference and Fine-tuning of Large Language Models Over The Internet_2023_Borzunov et al.pdf}
}

@misc{doccano,
  title = {Doccano: {{Text}} Annotation Tool for Human},
  author = {Nakayama, Hiroki and Kubo, Takahiro and Kamura, Junya and Taniguchi, Yasufumi and Liang, Xu},
  date = {2018},
  url = {https://github.com/doccano/doccano}
}

@article{DoesNonviolenceWork_2016_Lehoucq,
  title = {Does {{Nonviolence Work}}?},
  author = {Lehoucq, Fabrice},
  date = {2016-01-01},
  journaltitle = {Comparative Politics},
  shortjournal = {Comp Politics},
  volume = {48},
  number = {2},
  pages = {269--287},
  issn = {00104159, 21516227},
  doi = {10.5129/001041516817037691},
  url = {http://openurl.ingenta.com/content/xref?genre=article&issn=0010-4159&volume=48&issue=2&spage=269},
  urldate = {2024-07-03},
  langid = {english},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/avaliacao-de-politicas-sociais/Does Nonviolence Work_2016_Lehoucq.pdf}
}

@article{DrAIHow_2019_Jordan,
  title = {Dr. {{AI}} or: {{How I Learned}} to {{Stop Worrying}} and {{Love Economics}}},
  shorttitle = {Dr. {{AI}} Or},
  author = {Jordan, Michael I.},
  date = {2019-07-03},
  journaltitle = {Harvard Data Science Review},
  volume = {1},
  number = {1},
  issn = {2644-2353, 2688-8513},
  doi = {10.1162/99608f92.b9006d09},
  url = {https://hdsr.mitpress.mit.edu/pub/2imtstfu/release/8},
  urldate = {2023-04-13},
  langid = {english},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Dr_2019_Jordan.pdf}
}

@article{DropoutSimpleWay_2014_SrivastavaEtAl,
  title = {Dropout: {{A Simple Way}} to {{Prevent Neural Networks}} from {{Overfitting}}},
  shorttitle = {Dropout},
  author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  date = {2014},
  journaltitle = {Journal of Machine Learning Research},
  volume = {15},
  number = {56},
  pages = {1929--1958},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v15/srivastava14a.html},
  urldate = {2023-06-12},
  abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different âthinnedâ networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Dropout_2014_Srivastava et al.pdf}
}

@article{duffy_2013,
  title = {Poverty, Poaching and Trafficking: {{What}} Are the Links?},
  author = {Duffy, R. and St John, F.},
  date = {2013}
}

@book{Duna_2015_HerbertZanini,
  title = {Duna},
  author = {Herbert, Frank and Zanini, Maria do Carmo},
  date = {2015-09-16},
  edition = {2º edição},
  publisher = {Editora Aleph},
  abstract = {A vida do jovem Paul Atreides está prestes a mudar radicalmente. Após a visita de uma mulher misteriosa, ele é obrigado a deixar seu planeta natal para sobreviver ao ambiente árido e severo de Arrakis, o Planeta Deserto. Envolvido numa intrincada teia política e religiosa, Paul divide-se entre as obrigações de herdeiro e seu treinamento nas doutrinas secretas de uma antiga irmandade, que vê nele a esperança de realização de um plano urdido há séculos. Ecos de profecias ancestrais também o cercam entre os nativos de Arrakis. Seria ele o eleito que tornaria viáveis seus sonhos e planos ocultos? Ao lado das trilogias Fundação, de Isaac Asimov, e O Senhor dos Anéis, de J. R. R. Tolkien, Duna é considerada uma das maiores obras de fantasia e ficção científica de todos os tempos. Um premiado best-seller já levado às telas de cinema pelas mãos do consagrado diretor David Lynch.},
  langid = {portuguese},
  pagetotal = {883},
  keywords = {notion}
}

@article{duncan_2002,
  title = {Prehistoric Bird Extinctions and Human Hunting},
  author = {Duncan, R.P. and Blackburn, T.M. and Worthy, T.H.},
  date = {2002},
  journaltitle = {Proceedings of the Royal Society of London. Series B: Biological Sciences},
  volume = {269},
  number = {1490},
  pages = {517--521}
}

@article{DynamicModelsSegregation_1971_Schelling,
  title = {Dynamic Models of Segregation},
  author = {Schelling, Thomas C.},
  date = {1971-07-01},
  journaltitle = {The Journal of Mathematical Sociology},
  volume = {1},
  number = {2},
  pages = {143--186},
  publisher = {Routledge},
  issn = {0022-250X},
  doi = {10.1080/0022250X.1971.9989794},
  url = {https://doi.org/10.1080/0022250X.1971.9989794},
  urldate = {2024-06-04},
  abstract = {Some segregation results from the practices of organizations, some from specialized communication systems, some from correlation with a variable that is non‐random; and some results from the interplay of individual choices. This is an abstract study of the interactive dynamics of discriminatory individual choices. One model is a simulation in which individual members of two recognizable groups distribute themselves in neighborhoods defined by reference to their own locations. A second model is analytic and deals with compartmented space. A final section applies the analytics to ‘neighborhood tipping.’ The systemic effects are found to be overwhelming: there is no simple correspondence of individual incentive to collective results. Exaggerated separation and patterning result from the dynamics of movement. Inferences about individual motives can usually not be drawn from aggregate patterns. Some unexpected phenomena, like density and vacancy, are generated. A general theory of ‘tipping’ begins to emerge.},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Dynamic models of segregation_1971_Schelling.pdf}
}

@article{DynamicsDiscriminationTheory_2019_BohrenEtAl,
  title = {The {{Dynamics}} of {{Discrimination}}: {{Theory}} and {{Evidence}}},
  shorttitle = {The {{Dynamics}} of {{Discrimination}}},
  author = {Bohren, J. Aislinn and Imas, Alex and Rosenberg, Michael},
  date = {2019-10},
  journaltitle = {American Economic Review},
  volume = {109},
  number = {10},
  pages = {3395--3436},
  issn = {0002-8282},
  doi = {10.1257/aer.20171829},
  url = {https://www.aeaweb.org/articles?id=10.1257/aer.20171829},
  urldate = {2023-10-19},
  abstract = {We model the dynamics of discrimination and show how its evolution can identify the underlying source. We test these theoretical predictions in a field experiment on a large online platform where users post content that is evaluated by other users on the platform. We assign posts to accounts that exogenously vary by gender and evaluation histories. With no prior evaluations, women face significant discrimination. However, following a sequence of positive evaluations, the direction of discrimination reverses: women's posts are favored over men's. Interpreting these results through the lens of our model, this dynamic reversal implies discrimination driven by biased beliefs.},
  langid = {english},
  keywords = {Belief,Communication,Field Experiments Search,Information and Knowledge,Learning,Non-labor Discrimination Labor Discrimination,notion,Unawareness Economics of Gender},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/The Dynamics of Discrimination_2019_Bohren et al.pdf}
}

@book{EconometriaBasica_2009_GujaratiPorter,
  title = {Econometria {{Básica}}},
  author = {Gujarati, Damodar N. and Porter, Dawn C.},
  date = {2009},
  series = {The {{McGraw-Hill}} Series {{Economics}}},
  edition = {5. ed},
  publisher = {McGraw-Hill Irwin},
  location = {Boston, Mass.},
  isbn = {978-0-07-337577-9},
  langid = {english},
  pagetotal = {922}
}

@book{EconometricAnalysisCross_2010_Wooldridge,
  title = {Econometric Analysis of Cross Section and Panel Data: Second Edition},
  shorttitle = {Econometric Analysis of Cross Section and Panel Data},
  author = {Wooldridge, Jeffrey M.},
  date = {2010-10-01},
  publisher = {MIT Press},
  location = {Cambridge, Mass.},
  abstract = {The second edition of this acclaimed graduate text provides a unified treatment of two methods used in contemporary econometric research, cross section and data panel methods. By focusing on assumptions that can be given behavioral content, the book maintains an appropriate level of rigor while emphasizing intuitive thinking. The analysis covers both linear and nonlinear models, including models with dynamics and/or individual heterogeneity. In addition to general estimation frameworks (particular methods of moments and maximum likelihood), specific linear and nonlinear methods are covered in detail, including probit and logit models and their multivariate, Tobit models, models for count data, censored and missing data schemes, causal (or treatment) effects, and duration analysis.Econometric Analysis of Cross Section and Panel Data was the first graduate econometrics text to focus on microeconomic data structures, allowing assumptions to be separated into population and sampling assumptions. This second edition has been substantially updated and revised. Improvements include a broader class of models for missing data problems; more detailed treatment of cluster problems, an important topic for empirical researchers; expanded discussion of "generalized instrumental variables" (GIV) estimation; new coverage (based on the author's own recent research) of inverse probability weighting; a more complete framework for estimating treatment effects with panel data, and a firmly established link between econometric approaches to nonlinear panel data and the "generalized estimating equation" literature popular in statistics and other fields. New attention is given to explaining when particular econometric methods can be applied; the goal is not only to tell readers what does work, but why certain "obvious" procedures do not. The numerous included exercises, both theoretical and computer-based, allow the reader to extend methods covered in the text and discover new insights.},
  isbn = {978-0-262-23258-6},
  langid = {Inglês},
  file = {/home/baldoinov/baldoinov/PDFs/Livros/Econometric Analysis of Cross Section and Panel Data_2010_Wooldridge.pdf}
}

@article{EconometricPolicyEvaluation_1976_Lucas,
  title = {Econometric Policy Evaluation: {{A}} Critique},
  shorttitle = {Econometric Policy Evaluation},
  author = {Lucas, Robert E.},
  date = {1976-01-01},
  journaltitle = {Carnegie-Rochester Conference Series on Public Policy},
  shortjournal = {Carnegie-Rochester Conference Series on Public Policy},
  volume = {1},
  pages = {19--46},
  issn = {0167-2231},
  doi = {10.1016/S0167-2231(76)80003-6},
  url = {https://www.sciencedirect.com/science/article/pii/S0167223176800036},
  urldate = {2023-09-13},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Econometric policy evaluation_1976_Lucas.pdf;/home/baldoinov/Zotero/storage/NENGWPZU/S0167223176800036.html}
}

@book{EconometricsMachineLearning_2022_ChanMatyas,
  title = {Econometrics with {{Machine Learning}}},
  editor = {Chan, Felix and Mátyás, László},
  date = {2022},
  series = {Advanced {{Studies}} in {{Theoretical}} and {{Applied Econometrics}}},
  volume = {53},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-031-15149-1},
  url = {https://link.springer.com/10.1007/978-3-031-15149-1},
  urldate = {2024-04-01},
  isbn = {978-3-031-15148-4 978-3-031-15149-1},
  langid = {english}
}

@book{EconomiaBrasileiraContemporanea_2014_Giambiagi,
  title = {Economia Brasileira Contemporânea},
  author = {Giambiagi, Fabio},
  namea = {Villela, André and Barros de Castro, Lavinia and Herman, Jennifer},
  nameatype = {collaborator},
  date = {2014-11-10},
  publisher = {Elsevier},
  isbn = {978-85-352-4863-0},
  langid = {portuguese},
  keywords = {Administração e serviços auxiliares}
}

@book{EconomiaInternacional_2015_KrugmanObstfeld,
  title = {Economia Internacional},
  author = {Krugman, Paul R. and Obstfeld, Maurice},
  date = {2015-01-01},
  edition = {10ª edição},
  publisher = {Pearson Universidades},
  abstract = {Em parceria com Maurice Obstfeld e Marc J. Merlitz, Paul Krugman traz na décima edição de Economia internacional os últimos acontecimentos da área, com uma linguagem didática, clara e analítica, que permite ao leitor analisar os quadros passados e atuais do mercado econômico. Com uma revisão rigorosamente atualizada dos capítulos, a obra se divide em duas partes – voltadas ao comércio e às questões monetárias – e discorre sobre alguns desenvolvimentos signi cativos dos lados teórico e prático, além de responder às sugestões dos leitores. Conta ainda com estudos de caso que reforçam os temas abordados, ilustrando sua aplicabilidade no mundo real e fornecendo importantes informações históricas.},
  isbn = {978-85-430-0452-5},
  langid = {portuguese},
  file = {/home/baldoinov/baldoinov/PDFs/Livros/Economia Internacional_2015_Krugman et al.pdf}
}

@book{EconomicAnalysisHealth_2012_MorrisEtAl,
  title = {Economic {{Analysis}} in {{Health Care}}},
  shorttitle = {Economic Analysis in Health Care},
  editor = {Morris, Stephen and Devlin, Nancy and Parkin, David and Spencer, Anne},
  date = {2012},
  edition = {2. ed},
  publisher = {Wiley},
  location = {Chichester, West Sussex},
  isbn = {978-1-119-95149-0},
  langid = {english},
  pagetotal = {386}
}

@article{EconomicForecastingAgentbased_2023_PolednaEtAl,
  title = {Economic Forecasting with an Agent-Based Model},
  author = {Poledna, Sebastian and Miess, Michael Gregor and Hommes, Cars and Rabitsch, Katrin},
  date = {2023-01-01},
  journaltitle = {European Economic Review},
  shortjournal = {European Economic Review},
  volume = {151},
  pages = {104306},
  issn = {0014-2921},
  doi = {10.1016/j.euroecorev.2022.104306},
  url = {https://www.sciencedirect.com/science/article/pii/S0014292122001891},
  urldate = {2023-12-05},
  abstract = {We develop the first agent-based model (ABM) that can compete with benchmark VAR and DSGE models in out-of-sample forecasting of macro variables. Our ABM for a small open economy uses micro and macro data from national accounts, sector accounts, input–output tables, government statistics, and census and business demography data. The model incorporates all economic activities as classified by the European System of Accounts (ESA 2010) and includes all economic sectors populated with millions of heterogeneous agents. In addition to being a competitive model framework for forecasts of aggregate variables, the detailed structure of the ABM allows for a breakdown into sector-level forecasts. Using this detailed structure, we demonstrate the ABM by forecasting the medium-run macroeconomic effects of lockdown measures taken in Austria to combat the COVID-19 pandemic. Potential applications of the model include stress-testing and predicting the effects of monetary or fiscal macroeconomic policies.},
  keywords = {Agent-based models,Behavioural macro,Macroeconomic forecasting,Micro data,notion},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Economic forecasting with an agent-based model_2023_Poledna et al.pdf}
}

@article{EconomicModels_1978_GibbardVarian,
  title = {Economic {{Models}}},
  author = {Gibbard, Allan and Varian, Hal R.},
  date = {1978},
  journaltitle = {The Journal of Philosophy},
  volume = {75},
  number = {11},
  eprint = {2025484},
  eprinttype = {jstor},
  pages = {664--677},
  publisher = {Journal of Philosophy, Inc.},
  issn = {0022-362X},
  url = {https://www.jstor.org/stable/2025484},
  urldate = {2024-06-03},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Economic Models_1978_Gibbard et al.pdf}
}

@article{EconomicModelsTheir_2024_KuorikoskiMarchionni,
  title = {Economic Models and Their Flexible Interpretations: A Philosophy of Science Perspective},
  shorttitle = {Economic Models and Their Flexible Interpretations},
  author = {Kuorikoski, Jaakko and Marchionni, Caterina},
  date = {2024},
  journaltitle = {Journal of Economic Methodology},
  volume = {0},
  number = {0},
  pages = {1--8},
  publisher = {Routledge},
  issn = {1350-178X},
  doi = {10.1080/1350178X.2024.2336048},
  url = {https://doi.org/10.1080/1350178X.2024.2336048},
  urldate = {2024-06-03},
  abstract = {We mobilise contemporary philosophy of science to further clarify observations on economic modelling made by Gilboa et al. (2023). We adopt a normative stance towards these modelling practices to identify the extent to which they are epistemically justified. Our message is simple: many of the distinctions proposed by Gilboa et al. (2023) are useful, but without the proper qualifications, too much flexibility in choosing the right interpretation risks downplaying the crucial role that empirical evidence should play in any modelling endeavour.},
  keywords = {Economic modelling,normative economics,scientific explanation},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Economic models and their flexible interpretations_2024_Kuorikoski et al.pdf}
}

@book{EconomicsArtificialIntelligence_2019_AgrawalEtAl,
  type = {Book},
  title = {The {{Economics}} of {{Artificial Intelligence}}: {{An Agenda}}},
  shorttitle = {The {{Economics}} of {{Artificial Intelligence}}},
  author = {Agrawal, Ajay and Gans, Joshua and Goldfarb, Avi},
  date = {2019},
  publisher = {University of Chicago Press},
  doi = {10.7208/chicago/9780226613475.001.0001},
  url = {https://www.nber.org/books-and-chapters/economics-artificial-intelligence-agenda},
  urldate = {2023-04-12},
  keywords = {notion},
  annotation = {Backup Publisher: National Bureau of Economic Research}
}

@book{EconomicsHealthHealth_2017_FollandEtAl,
  title = {The {{Economics}} of {{Health}} and {{Health Care}}},
  author = {Folland, Sherman T. and Goodman, Allen C. and Stano, Miron},
  date = {2017},
  edition = {Eighth edition, international edition},
  publisher = {Routledge, Taylor \& Francis Group},
  location = {London New York, NY},
  isbn = {978-1-138-20805-6 978-1-138-20804-9},
  langid = {english},
  pagetotal = {751}
}

@article{EconomicTheoriesTheir_2022_GilboaEtAl,
  title = {Economic Theories and Their {{Dueling}} Interpretations},
  author = {Gilboa, Itzhak and Postlewaite, Andrew and Samuelson, Larry and Schmeidler, David},
  date = {2022},
  journaltitle = {Journal of Economic Methodology},
  volume = {0},
  number = {0},
  pages = {1--20},
  publisher = {Routledge},
  issn = {1350-178X},
  doi = {10.1080/1350178X.2022.2142270},
  url = {https://doi.org/10.1080/1350178X.2022.2142270},
  urldate = {2024-06-05},
  abstract = {The interpretation of economic theories varies along several dimensions. First, models can describe reality, illustrate a recommended state of affairs, or analyze the structure and implications of a theory. Second, theories can be used for prediction or for explanation. Third, theories can relate to reality in a rule-based or case-based manner. Fourth, theories can be statements about economic reality or about the act of economic reasoning itself. Fifth, theories can offer predictions or merely critique reasoning. We argue that theories are often open to multiple interpretations which can shift depending on the context in which the theory is applied, the surrounding economic literature, and the argument made by the interpreter.},
  keywords = {A1,analogies,B4,criticism,economic methodology,Economic theory,explanation,models},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Economic theories and their Dueling interpretations_2022_Gilboa et al.pdf}
}

@article{EconomyNeedsAgentbased_2009_FarmerFoley,
  title = {The Economy Needs Agent-Based Modelling},
  author = {Farmer, J. Doyne and Foley, Duncan},
  date = {2009-08},
  journaltitle = {Nature},
  volume = {460},
  number = {7256},
  pages = {685--686},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/460685a},
  url = {https://www.nature.com/articles/460685a},
  urldate = {2024-06-03},
  abstract = {The leaders of the world are flying the economy by the seat of their pants, say J. Doyne Farmer and Duncan Foley. There is, however, a better way to help guide financial policies.},
  langid = {english},
  keywords = {Humanities and Social Sciences,multidisciplinary,Science},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/The economy needs agent-based modelling_2009_Farmer et al.pdf}
}

@article{EffectCigaretteTaxes_1997_MeierLicari,
  title = {The Effect of Cigarette Taxes on Cigarette Consumption, 1955 through 1994.},
  author = {Meier, K J and Licari, M J},
  date = {1997-07},
  journaltitle = {American Journal of Public Health},
  shortjournal = {Am J Public Health},
  volume = {87},
  number = {7},
  pages = {1126--1130},
  publisher = {American Public Health Association},
  issn = {0090-0036},
  doi = {10.2105/AJPH.87.7.1126},
  url = {https://ajph.aphapublications.org/doi/abs/10.2105/AJPH.87.7.1126},
  urldate = {2024-06-25},
  abstract = {OBJECTIVES: This study examines the effectiveness of state and federal taxes in reducing the consumption of cigarettes, estimates the impact of government health warnings, and shows how warnings and taxes interact. METHODS: By means of a pooled time-series analysis from 1955 through 1994 with the 50 states as units of analysis, the impact of excise taxes on cigarette consumption for several different models and econometric techniques is assessed. RESULTS: From 1955 through 1994, increases in state taxes were effective in reducing cigarette use. Federal tax increases, however, appear to have been more effective. This difference is partly the result of the "bootlegging" of cigarettes across state lines and the size of the increases in the federal tax. Cigarette consumption also declined when health warning labels were added. CONCLUSIONS: Increases of taxes on cigarettes are associated with declines in the consumption of tobacco. Because of inflation, increased health concerns, and the declining percentage of smokers, however, large reductions in consumption require large tax increases.},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/The effect of cigarette taxes on cigarette consumption, 1955 through 1994_1997_Meier et al.pdf}
}

@article{EffectiveDashboardDesign_2013_JanesEtAl,
  title = {Effective Dashboard Design},
  author = {Janes, Andrea and Sillitti, Alberto and Succi, Giancarlo},
  date = {2013-01-01},
  journaltitle = {Cutter IT Journal},
  shortjournal = {Cutter IT Journal},
  volume = {26},
  pages = {17--24},
  file = {/home/baldoinov/baldoinov/PDFs/Mackenzie/topicos-de-banco-de-dados/Effective dashboard design_2013_Janes et al.pdf}
}

@online{EfficientLargeLanguage_2023_WanEtAl,
  title = {Efficient {{Large Language Models}}: {{A Survey}}},
  shorttitle = {Efficient {{Large Language Models}}},
  author = {Wan, Zhongwei and Wang, Xin and Liu, Che and Alam, Samiul and Zheng, Yu and Liu, Jiachen and Qu, Zhongnan and Yan, Shen and Zhu, Yi and Zhang, Quanlu and Chowdhury, Mosharaf and Zhang, Mi},
  date = {2023-12-22},
  eprint = {2312.03863},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2312.03863},
  url = {http://arxiv.org/abs/2312.03863},
  urldate = {2023-12-31},
  abstract = {Large Language Models (LLMs) have demonstrated remarkable capabilities in important tasks such as natural language understanding, language generation, and complex reasoning and have the potential to make a substantial impact on our society. Such capabilities, however, come with the considerable resources they demand, highlighting the strong need to develop effective techniques for addressing their efficiency challenges. In this survey, we provide a systematic and comprehensive review of efficient LLMs research. We organize the literature in a taxonomy consisting of three main categories, covering distinct yet interconnected efficient LLMs topics from model-centric, data-centric, and framework-centric perspective, respectively. We have also created a GitHub repository where we compile the papers featured in this survey at https://github.com/AIoT-MLSys-Lab/EfficientLLMs, and will actively maintain this repository and incorporate new research as it emerges. We hope our survey can serve as a valuable resource to help researchers and practitioners gain a systematic understanding of the research developments in efficient LLMs and inspire them to contribute to this important and exciting field.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,notion},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Efficient Large Language Models_2023_Wan et al.pdf;/home/baldoinov/Zotero/storage/9535L2LM/2312.html}
}

@online{EfficientStreamingLanguage_2023_XiaoEtAl,
  title = {Efficient {{Streaming Language Models}} with {{Attention Sinks}}},
  author = {Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike},
  date = {2023-09-29},
  eprint = {2309.17453},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2309.17453},
  url = {http://arxiv.org/abs/2309.17453},
  urldate = {2023-10-19},
  abstract = {Deploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges. Firstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length. Window attention, where only the most recent KVs are cached, is a natural approach -- but we show that it fails when the text length surpasses the cache size. We observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a ``sink'' even if they are not semantically important. Based on the above analysis, we introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning. We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more. In addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2x speedup. Code and datasets are provided at https://github.com/mit-han-lab/streaming-llm.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,notion},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Efficient Streaming Language Models with Attention Sinks_2023_Xiao et al.pdf;/home/baldoinov/Zotero/storage/RP7WUUN3/2309.html}
}

@incollection{EfficientTwitterData_2021_MurshedEtAl,
  title = {Efficient {{Twitter Data Cleansing Model}} for {{Data Analysis}} of the {{Pandemic Tweets}}},
  booktitle = {Emerging {{Technologies During}} the {{Era}} of {{COVID-19 Pandemic}}},
  author = {Murshed, Belal Abdullah Hezam and Mallappa, Suresha and Ghaleb, Osamah A. M. and Al-ariki, Hasib Daowd Esmail},
  editor = {Arpaci, Ibrahim and Al-Emran, Mostafa and A. Al-Sharafi, Mohammed and Marques, Gonçalo},
  date = {2021},
  series = {Studies in {{Systems}}, {{Decision}} and {{Control}}},
  pages = {93--114},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-67716-9_7},
  url = {https://doi.org/10.1007/978-3-030-67716-9_7},
  urldate = {2023-04-15},
  abstract = {Twitter data generally tends to be unstructured and often very noisy, cluttered/disorganized, and clothed in informal language. In this paper, we propose an intelligent Twitter data cleansing model that can solve data quality problems associated with twitter text. This model can correct a wide variety of anomalies from slangs, typos, Elongated (repeated Characters), transposition, Concatenated words, complex spelling mistakes as unorthodox use of acronyms, manifold forms of abbreviations of same words, and word boundary errors. The effects of whole range of tasks of Twitter Data Cleansing Model (TDCM) on the performance of sentiment classification utilizing feature models and three common classifiers have been investigated and evaluated. We conducted our experiments on two sets of pandemics twitter datasets: COVID-19 and Dengue datasets. The primary objective of this paper is to both increase the accuracy and the quality of twitter data and to purify and cleanse twitter data for further analysis. The experiment results seem to indicate that the accuracy of sentiment classification increases once the data quality problems associated with the Twitter text are solved. In COVID-19 twitter dataset, the best performance obtained using Random forest classifier after cleansing the data in terms of accuracy, recall, and f1-score are found to be at 84.7\%, 88.5\%, and 86.3\% respectively. However, the best performance in terms of precision at 84.5\% was observed using SVM classifier when compared to that obtained with other classifiers. Further, in the Dengue twitter dataset, the best performance for cleansing data in terms of accuracy, precision and f1-score are observed to be 81.7\%, 83.7\% and 88.6\% respectively using Random forest classifier. The best performance in terms of recall, however, is 94.9\% and was obtained using SVM classifier when compared with those obtained with other classifiers.},
  isbn = {978-3-030-67716-9},
  langid = {english},
  keywords = {COVID-19,Data cleansing,Data quality problem,Data transformation,Elongation,Informal data,Natural language processing,notion,Slang,Twitter}
}

@article{ElementsDynamicEconomic_2017_Tesfatsion,
  title = {Elements of {{Dynamic Economic Modeling}}: {{Presentation}} and {{Analysis}}},
  shorttitle = {Elements of {{Dynamic Economic Modeling}}},
  author = {Tesfatsion, Leigh},
  date = {2017-03-01},
  journaltitle = {Eastern Economic Journal},
  shortjournal = {Eastern Econ J},
  volume = {43},
  number = {2},
  pages = {192--216},
  issn = {1939-4632},
  doi = {10.1057/eej.2016.2},
  url = {https://doi.org/10.1057/eej.2016.2},
  urldate = {2024-06-03},
  abstract = {The primary goal of these introductory notes is to promote the clear presentation and rigorous analysis of dynamic economic models, whether expressed in equation or agent-based form. A secondary goal is to promote the use of initial-value state-space modeling with its regard for historical process, for cause leading to effect without the external imposition of global coordination constraints on agent actions. Economists who claim to respect individual rationality should not be doing for their modeled economic agents what in reality these agents must do for themselves.},
  langid = {english},
  keywords = {A23,agent-based computational economics,analysis,B4,C6,differential/difference equations,presentation,state-space modeling},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Elements of Dynamic Economic Modeling_2017_Tesfatsion.pdf}
}

@book{ElementsPureEconomics_2010_Walras,
  title = {Elements of Pure Economics: Or the Theory of Social Wealth},
  shorttitle = {Elements of Pure Economics},
  author = {Walras, Léon},
  translator = {Jaffé, William},
  date = {2010},
  edition = {1ª edição},
  publisher = {Routledge},
  location = {London New York},
  abstract = {Elements of Pure Economics was one of the most influential works in the history of economics, and the single most important contribution to the marginal revolution. Walras' theory of general equilibrium remains one of the cornerstones of economic theory more than 100 years after it was first published.},
  isbn = {978-0-415-60731-5},
  langid = {Inglês}
}

@book{ElementsStatisticalLearning_2017_HastieEtAl,
  title = {The {{Elements}} of {{Statistical Learning}}: Data Mining, Inference, and Prediction},
  shorttitle = {The Elements of Statistical Learning},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome H.},
  date = {2017},
  series = {Springer Series in Statistics},
  edition = {Second edition, corrected at 12th printing 2017},
  publisher = {Springer},
  location = {New York, NY},
  doi = {10.1007/b94608},
  isbn = {978-0-387-84857-0},
  langid = {english},
  pagetotal = {745},
  file = {/home/baldoinov/baldoinov/PDFs/Livros/The Elements of Statistical Learning_2017_Hastie et al.pdf}
}

@online{EmergentAbilitiesLarge_2022_WeiEtAl,
  title = {Emergent {{Abilities}} of {{Large Language Models}}},
  author = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and Chi, Ed H. and Hashimoto, Tatsunori and Vinyals, Oriol and Liang, Percy and Dean, Jeff and Fedus, William},
  date = {2022-10-26},
  eprint = {2206.07682},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2206.07682},
  url = {http://arxiv.org/abs/2206.07682},
  urldate = {2024-03-08},
  abstract = {Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Emergent Abilities of Large Language Models_2022_Wei et al.pdf;/home/baldoinov/Zotero/storage/T7LMUEGZ/2206.html}
}

@incollection{EmpiricalValidationAgentBased_2018_LuxZwinkels,
  title = {Empirical {{Validation}} of {{Agent-Based Models}}},
  booktitle = {Handbook of {{Computational Economics}}},
  author = {Lux, Thomas and Zwinkels, Remco C. J.},
  editor = {Hommes, Cars and LeBaron, Blake},
  date = {2018-01-01},
  series = {Handbook of {{Computational Economics}}},
  volume = {4},
  pages = {437--488},
  publisher = {Elsevier},
  doi = {10.1016/bs.hescom.2018.02.003},
  url = {https://www.sciencedirect.com/science/article/pii/S1574002118300030},
  urldate = {2023-09-18},
  abstract = {The literature on agent-based models has been highly successful in replicating many stylized facts of financial and macroeconomic time series. Over the past decade, however, also advances in the estimation of such models have been made. Due to the inherent heterogeneity of agents and nonlinearity of agent-based models, fundamental choices have to be made to take the models to the data. In this chapter we provide an overview of the current literature on the empirical validation of agent-based models. We discuss potential lessons from other fields of applications of agent-based models, avenues for estimation of reduced form and ‘full-fledged’ agent-based models, estimation methods, as well as applications and results.},
  keywords = {Agent-based models,Method of moments,notion,Reduced form models,Sequential Monte Carlo,State space models,Switching mechanisms,Validation},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Empirical Validation of Agent-Based Models_2018_Lux et al.pdf;/home/baldoinov/Zotero/storage/UX7YMZDH/S1574002118300030.html}
}

@inproceedings{EndtoEndArgumentMining_2018_MorioFujita,
  title = {End-to-{{End Argument Mining}} for {{Discussion Threads Based}} on {{Parallel Constrained Pointer Architecture}}},
  booktitle = {Proceedings of the 5th {{Workshop}} on {{Argument Mining}}},
  author = {Morio, Gaku and Fujita, Katsuhide},
  date = {2018-11},
  pages = {11--21},
  publisher = {Association for Computational Linguistics},
  location = {Brussels, Belgium},
  doi = {10.18653/v1/W18-5202},
  url = {https://aclanthology.org/W18-5202},
  urldate = {2023-04-17},
  abstract = {Argument Mining (AM) is a relatively recent discipline, which concentrates on extracting claims or premises from discourses, and inferring their structures. However, many existing works do not consider micro-level AM studies on discussion threads sufficiently. In this paper, we tackle AM for discussion threads. Our main contributions are follows: (1) A novel combination scheme focusing on micro-level inner- and inter- post schemes for a discussion thread. (2) Annotation of large-scale civic discussion threads with the scheme. (3) Parallel constrained pointer architecture (PCPA), a novel end-to-end technique to discriminate sentence types, inner-post relations, and inter-post interactions simultaneously. The experimental results demonstrate that our proposed model shows better accuracy in terms of relations extraction, in comparison to existing state-of-the-art models.},
  eventtitle = {{{ArgMining}} 2018},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/End-to-End Argument Mining for Discussion Threads Based on Parallel Constrained_2018_Morio et al.pdf}
}

@book{EngenhariaSoftware_2011_Sommerville,
  title = {Engenharia de Software},
  author = {Sommerville, Ian},
  date = {2011},
  edition = {9ª edição},
  publisher = {Pearson Universidades},
  abstract = {"A 10ª edição de Engenharia de software foi extensivamente atualizada para refletir a adoção crescente de métodos ágeis na engenharia de software. Um dos destaques da nova edição é o acréscimo de conteúdo sobre a metodologia do Scrum. A divisão em quatro partes do livro foi significativamente reformulada para acomodar novos capítulos sobre engenharia de resiliência, engenharia de sistemas e sistemas de sistemas. Além disso, capítulos sobre tópicos como confiança, segurança e proteção foram completamente reorganizados.Todas essas mudanças se justificam por compreenderem questões essenciais para a engenharia de software moderna ― gerenciamento da complexidade, integração da agilidade a outros métodos e garantia de que os sistemas sejam seguros e resilientes. Obra de referência para estudantes de ciência da computação, engenharia da computação e sistemas de informação, e para qualquer profissional que deseje atualizar seus conhecimentos sobre reúso de software, arquitetura de projetos, confiabilidade e segurança e engenharia de sistemas.."},
  isbn = {978-85-7936-108-1},
  langid = {portuguese}
}

@online{Era1bitLLMs_2024_MaEtAl,
  title = {The {{Era}} of 1-Bit {{LLMs}}: {{All Large Language Models}} Are in 1.58 {{Bits}}},
  shorttitle = {The {{Era}} of 1-Bit {{LLMs}}},
  author = {Ma, Shuming and Wang, Hongyu and Ma, Lingxiao and Wang, Lei and Wang, Wenhui and Huang, Shaohan and Dong, Li and Wang, Ruiping and Xue, Jilong and Wei, Furu},
  date = {2024-02-27},
  eprint = {2402.17764},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2402.17764},
  url = {http://arxiv.org/abs/2402.17764},
  urldate = {2024-05-22},
  abstract = {Recent research, such as BitNet, is paving the way for a new era of 1-bit Large Language Models (LLMs). In this work, we introduce a 1-bit LLM variant, namely BitNet b1.58, in which every single parameter (or weight) of the LLM is ternary \{-1, 0, 1\}. It matches the full-precision (i.e., FP16 or BF16) Transformer LLM with the same model size and training tokens in terms of both perplexity and end-task performance, while being significantly more cost-effective in terms of latency, memory, throughput, and energy consumption. More profoundly, the 1.58-bit LLM defines a new scaling law and recipe for training new generations of LLMs that are both high-performance and cost-effective. Furthermore, it enables a new computation paradigm and opens the door for designing specific hardware optimized for 1-bit LLMs.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/baldoinov/Zotero/storage/R5EF5RT3/2402.html}
}

@online{EsbocoQueAprendemos_2023_Duarte,
  title = {Esboço: O que aprendemos com a luta de Jacó?},
  shorttitle = {Esboço},
  author = {Duarte, Diego},
  date = {2023-10-22T03:26:45+00:00},
  url = {https://abibliaeeu.com.br/esboco-o-que-aprendemos-com-a-luta-de-jaco/},
  urldate = {2024-09-30},
  abstract = {A luta de Jacó com o Anjo é mais do que apenas uma narrativa sobre uma batalha física; ela é uma janela para a jornada espiritual e transformadora de um homem. Vamos nos aprofundar na história e explorar suas camadas de significado. Será que Jacó realmente venceu o anjo, ou será que a verdadeira vitória...},
  langid = {brazilian},
  organization = {A Bíblia e Eu},
  file = {/home/baldoinov/Zotero/storage/H7J7QQ7T/esboco-o-que-aprendemos-com-a-luta-de-jaco.html}
}

@article{EscapingMiddleincomeTechnology_2020_AndreoniTregenna,
  title = {Escaping the Middle-Income Technology Trap: {{A}} Comparative Analysis of Industrial Policies in {{China}}, {{Brazil}} and {{South Africa}}},
  shorttitle = {Escaping the Middle-Income Technology Trap},
  author = {Andreoni, Antonio and Tregenna, Fiona},
  date = {2020-09-01},
  journaltitle = {Structural Change and Economic Dynamics},
  shortjournal = {Structural Change and Economic Dynamics},
  volume = {54},
  pages = {324--340},
  issn = {0954-349X},
  doi = {10.1016/j.strueco.2020.05.008},
  url = {https://www.sciencedirect.com/science/article/pii/S0954349X1930308X},
  urldate = {2024-03-29},
  abstract = {Instead of catching up with advanced economies, most middle-income countries have remained stuck in a middle-income trap. We identify and analyse the triple challenges of ‘breaking into’ the global economy, ‘linking up’ into global value chains while ‘linking back’ to the local production system, and ‘keeping pace’ with technological change and innovation. We focus specifically on what we term the ‘middle-income technology trap’: specific structural and institutional configurations that are not conducive to increasing domestic value addition and to sustained industrial and technological upgrading. We explore this through case studies of China, Brazil and South Africa and the analysis of the evolution of their industrial policies and specific institutions, specifically InnoFund model in China, the Embrapa system in Brazil, and the Manufacturing Competitiveness Enhancement Programme in South Africa. Industrial policy implications for middle-income countries in particular, and for developing countries more widely, are finally discussed.},
  keywords = {Industrial development,Industrial policy,Middle-income technology trap,Structural transformation,Technology upgrading},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Escaping the middle-income technology trap_2020_Andreoni et al.pdf;/home/baldoinov/Zotero/storage/VRIWEF5T/S0954349X1930308X.html}
}

@book{EspiritoIntimidade_2008_Some,
  title = {O Espírito Da Intimidade},
  author = {Somé, Sobonfu},
  date = {2008-01-14},
  publisher = {Odysseus},
  isbn = {978-85-88023-95-6},
  langid = {brazilian},
  keywords = {Religião},
  file = {/home/baldoinov/baldoinov/PDFs/Livros/O Espirito Da Intimidade_2008_Some.pdf}
}

@book{EssaysPositiveEconomics_1966_Friedman,
  title = {Essays in {{Positive Economics}}},
  author = {Friedman, Milton},
  date = {1966-08},
  publisher = {University of Chicago Press},
  location = {Chicago, IL},
  url = {https://press.uchicago.edu/ucp/books/book/chicago/E/bo25773835.html},
  urldate = {2024-06-06},
  abstract = {"Stimulating, provocative, often infuriating, but well worth reading."—Peter Newman, Economica"His critical blast blows like a north wind against the more pretentious erections of modern economics. It is however a healthy and invigorating blast, without malice and with a sincere regard for scientific objectivity."—K.E. Boulding, Political Science Quarterly"Certainly one of the most engrossing volumes that has appeared recently in economic theory."—William J. Baumol, Review of Economics and Statistics},
  isbn = {978-0-226-26403-5},
  langid = {english},
  pagetotal = {334}
}

@book{EssentialMathematicsEconomic_2016_SydsaeterEtAl,
  title = {Essential {{Mathematics}} for {{Economic Analysis}}},
  author = {Sydsæter, Knut and Hammond, Peter J. and Strøm, Arne and Carvajal, Andrés},
  date = {2016},
  edition = {Fifth edition},
  publisher = {Pearson},
  location = {Harlow, England London New York},
  isbn = {978-1-292-07461-0},
  langid = {english},
  pagetotal = {807},
  file = {/home/baldoinov/baldoinov/PDFs/Livros/Essential Mathematics for Economic Analysis_2016_Sydsaeter et al.pdf}
}

@online{EssentialTemplatesData_2023_SeattleDataGuy,
  type = {Substack newsletter},
  title = {7 {{Essential Templates}} for {{Data Analytics Consulting}}},
  author = {{SeattleDataGuy}},
  date = {2023-11-06},
  url = {https://seattledataguy.substack.com/p/7-essential-templates-for-data-analytics},
  urldate = {2024-02-23},
  abstract = {Proposals, Sales And More},
  organization = {SeattleDataGuy’s Newsletter},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Diversos/7 Essential Templates for Data Analytics Consulting_2023_SeattleDataGuy.pdf;/home/baldoinov/Zotero/storage/DDHWX9CP/7-essential-templates-for-data-analytics.html}
}

@book{EstatisticaBasica_2021_MorettinBussab,
  title = {Estatística Básica},
  author = {Morettin, Pedro Alberto and Bussab, Wilton},
  date = {2021-10-07},
  publisher = {Editora Saraiva},
  isbn = {978-85-472-2023-5},
  langid = {portuguese}
}

@book{EstigmaCorComo_2021_Monteiro,
  title = {O estigma da cor : Como o racismo fere os dois grandes mandamentos},
  shorttitle = {O estigma da cor},
  author = {Monteiro, Jacira Pontinta Vaz},
  date = {2021},
  edition = {1ª edição},
  publisher = {Editora Quitanda},
  abstract = {Nesta obra preparada com esmero, Jacira Monteiro apresenta um conjunto de reflexões sobre como o racismo fere os dois grandes mandamentos de Cristo. O texto é incisivo e atravessa temas como escravidão, colonialismo, violência contra a mulher, ressentimento e desigualdade social, mas apresenta caminhos propositivos fundamentados no arcabouço bíblico e em testemunhos eclesiais. Se muitos falsos cristãos – e falsos cristos – simplesmente ignoraram os preceitos do amor e da justiça existe, por outro lado, um legado sólido de combate às injustiças na história cristã. Estes exemplos não nos deixam desanimar. Como afirmou Coretta King, “se tudo for ‘olho por olho’, acabaremos todos cegos”.},
  langid = {portuguese},
  pagetotal = {178}
}

@online{EstimatingInputCoefficients_2023_Fukui,
  title = {Estimating {{Input Coefficients}} for {{Regional Input-Output Tables Using Deep Learning}} with {{Mixup}}},
  author = {Fukui, Shogo},
  date = {2023-05-06},
  eprint = {2305.01201},
  eprinttype = {arXiv},
  eprintclass = {econ},
  doi = {10.48550/arXiv.2305.01201},
  url = {http://arxiv.org/abs/2305.01201},
  urldate = {2023-06-22},
  abstract = {An input-output table is an important data for analyzing the economic situation of a region. Generally, the input-output table for each region (regional input-output table) in Japan is not always publicly available, so it is necessary to estimate the table. In particular, various methods have been developed for estimating input coefficients, which are an important part of the input-output table. Currently, non-survey methods are often used to estimate input coefficients because they require less data and computation, but these methods have some problems, such as discarding information and requiring additional data for estimation. In this study, the input coefficients are estimated by approximating the generation process with an artificial neural network (ANN) to mitigate the problems of the non-survey methods and to estimate the input coefficients with higher precision. To avoid over-fitting due to the small data used, data augmentation, called mixup, is introduced to increase the data size by generating virtual regions through region composition and scaling. By comparing the estimates of the input coefficients with those of Japan as a whole, it is shown that the accuracy of the method of this research is higher and more stable than that of the conventional non-survey methods. In addition, the estimated input coefficients for the three cities in Japan are generally close to the published values for each city.},
  pubstate = {prepublished},
  keywords = {Economics - Econometrics,notion},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Estimating Input Coefficients for Regional Input-Output Tables Using Deep_2023_Fukui.pdf;/home/baldoinov/Zotero/storage/7EMLFT9J/2305.html}
}

@article{EticaInteligenciaArtificial_2020_Garcia,
  title = {Ética e Inteligência Artificial},
  author = {Garcia, Ana Cristina Bicharra},
  date = {2020-11-16},
  journaltitle = {Computação Brasil},
  number = {43},
  pages = {14--22},
  issn = {2965-9728},
  doi = {10.5753/compbr.2020.43.1791},
  url = {https://journals-sol.sbc.org.br/index.php/comp-br/article/view/1791},
  urldate = {2025-02-16},
  abstract = {A transformação digital vem fomentando o uso de Inteligência Artificial (IA) por empresas e governos. O contexto da pandemia de COVID-19 impulsionou o uso de IA e o cidadão mal se dá conta que interage com sistemas inteligentes o tempo todo, seja numa simples compra de cartão de crédito, seja recebendo dicas no seu canal preferido de streaming. Este artigo discute o uso IA e os vieses sociais que podem estar contidos na enorme massa de dados utilizada pelos sistemas inteligentes e algoritmos de aprendizado de máquina. Os casos discutidos revelam que é preciso reconhecer as distorções que o emprego de técnicas de IA não só exacerba, mas perpetua, como vieses raciais e desigualdades. Dados não são neutros e registram decisões humanas. Logo, para uso consciente, faz-se indispensável uma abordagem multidisciplinar, que inclua especialistas em Ética, cientistas sociais e especialistas que entendam as nuances de cada área de aplicação de Inteligência Artificial.},
  issue = {43},
  langid = {portuguese},
  keywords = {Ética em Inteligência Artificial,Sistemas Inteligentes,Transformação Digital},
  file = {/home/baldoinov/baldoinov/PDFs/Mackenzie/aprendizado-de-maquina-iii/Etica e Inteligencia Artificial_2020_Garcia.pdf}
}

@inproceedings{EvaluationAnalysisBusiness_2019_LousaEtAl,
  title = {Evaluation and {{Analysis}} of {{Business Intelligence Data Visualization Tools}}},
  author = {Lousa, Andre and Pedrosa, Isabel and Bernardino, Jorge},
  date = {2019-06-01},
  pages = {1--6},
  doi = {10.23919/CISTI.2019.8760677},
  file = {/home/baldoinov/baldoinov/PDFs/Mackenzie/projeto-analise-de-visualizacao-de-conhecimento/Evaluation and Analysis of Business Intelligence Data Visualization Tools_2019_Lousa et al.pdf}
}

@online{EvaluationPrecisionRecall_2020_Powers,
  title = {Evaluation: From Precision, Recall and {{F-measure}} to {{ROC}}, Informedness, Markedness and Correlation},
  shorttitle = {Evaluation},
  author = {Powers, David M. W.},
  date = {2020-10-10},
  eprint = {2010.16061},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2010.16061},
  url = {http://arxiv.org/abs/2010.16061},
  urldate = {2024-02-26},
  abstract = {Commonly used evaluation measures including Recall, Precision, F-Measure and Rand Accuracy are biased and should not be used without clear understanding of the biases, and corresponding identification of chance or base case levels of the statistic. Using these measures a system that performs worse in the objective sense of Informedness, can appear to perform better under any of these commonly used measures. We discuss several concepts and measures that reflect the probability that prediction is informed versus chance. Informedness and introduce Markedness as a dual measure for the probability that prediction is marked versus chance. Finally we demonstrate elegant connections between the concepts of Informedness, Markedness, Correlation and Significance as well as their intuitive relationships with Recall and Precision, and outline the extension from the dichotomous case to the general multi-class case.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,notion,Statistics - Machine Learning,Statistics - Methodology},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/machine-learning-w-pytorch-n-sklearn/Evaluation_2020_Powers.pdf;/home/baldoinov/Zotero/storage/4RUWQSJ3/2010.html}
}

@article{EvolutionArgumentationMining_2019_LytosEtAl,
  title = {The Evolution of Argumentation Mining: {{From}} Models to Social Media and Emerging Tools},
  shorttitle = {The Evolution of Argumentation Mining},
  author = {Lytos, Anastasios and Lagkas, Thomas and Sarigiannidis, Panagiotis and Bontcheva, Kalina},
  date = {2019-11-01},
  journaltitle = {Information Processing \& Management},
  shortjournal = {Information Processing \& Management},
  volume = {56},
  number = {6},
  pages = {102055},
  issn = {0306-4573},
  doi = {10.1016/j.ipm.2019.102055},
  url = {https://www.sciencedirect.com/science/article/pii/S030645731930024X},
  urldate = {2023-04-15},
  abstract = {Argumentation mining is a rising subject in the computational linguistics domain focusing on extracting structured arguments from natural text, often from unstructured or noisy text. The initial approaches on modeling arguments was aiming to identify a flawless argument on specific fields (Law, Scientific Papers) serving specific needs (completeness, effectiveness). With the emerge of Web 2.0 and the explosion in the use of social media both the diffusion of the data and the argument structure have changed. In this survey article, we bridge the gap between theoretical approaches of argumentation mining and pragmatic schemes that satisfy the needs of social media generated data, recognizing the need for adapting more flexible and expandable schemes, capable to adjust to the argumentation conditions that exist in social media. We review, compare, and classify existing approaches, techniques and tools, identifying the positive outcome of combining tasks and features, and eventually propose a conceptual architecture framework. The proposed theoretical framework is an argumentation mining scheme able to identify the distinct sub-tasks and capture the needs of social media text, revealing the need for adopting more flexible and extensible frameworks.},
  langid = {english},
  keywords = {Argumentation mining,Argumentation models,Argumentation tools,Computational linguistics,Machine learning,notion,Social media},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/The evolution of argumentation mining_2019_Lytos et al.pdf;/home/baldoinov/Zotero/storage/R4XIGQUG/S030645731930024X.html}
}

@article{EvolutionaryMechanismDesign_2010_PhelpsEtAl,
  title = {Evolutionary Mechanism Design: A Review},
  shorttitle = {Evolutionary Mechanism Design},
  author = {Phelps, Steve and McBurney, Peter and Parsons, Simon},
  date = {2010-09-01},
  journaltitle = {Autonomous Agents and Multi-Agent Systems},
  shortjournal = {Auton Agent Multi-Agent Syst},
  volume = {21},
  number = {2},
  pages = {237--264},
  issn = {1573-7454},
  doi = {10.1007/s10458-009-9108-7},
  url = {https://doi.org/10.1007/s10458-009-9108-7},
  urldate = {2024-06-11},
  abstract = {The advent of large-scale distributed systems poses unique engineering challenges. In open systems such as the internet it is not possible to prescribe the behaviour of all of the components of the system in advance. Rather, we attempt to design infrastructure, such as network protocols, in such a way that the overall system is robust despite the fact that numerous arbitrary, non-certified, third-party components can connect to our system. Economists have long understood this issue, since it is analogous to the design of the rules governing auctions and other marketplaces, in which we attempt to achieve socially-desirable outcomes despite the impossibility of prescribing the exact behaviour of the market participants, who may attempt to subvert the market for their own personal gain. This field is known as “mechanism design”: the science of designing rules of a game to achieve a specific outcome, even though each participant may be self-interested. Although it originated in economics, mechanism design has become an important foundation of multi-agent systems (MAS) research. In a traditional mechanism design problem, analytical methods are used to prove that agents’ game-theoretically optimal strategies lead to socially desirable outcomes. In many scenarios, traditional mechanism design and auction theory yield clear-cut results; however, there are many situations in which the underlying assumptions of the theory are violated due to the messiness of the real-world. In this paper we review alternative approaches to mechanism design which treat it as an engineering problem and bring to bear engineering design principles, viz.: iterative step-wise refinement of solutions, and satisficing instead of optimization in the face of intractable complexity. We categorize these approaches under the banner of evolutionary mechanism design.},
  langid = {english},
  keywords = {Auction theory,Evolutionary game theory,Mechanism design,Multi-agent systems},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Evolutionary mechanism design_2010_Phelps et al.pdf}
}

@article{ExactConsumerSurplus_1981_Hausman,
  title = {Exact {{Consumer}}'s {{Surplus}} and {{Deadweight Loss}}},
  author = {Hausman, Jerry A.},
  date = {1981},
  journaltitle = {The American Economic Review},
  volume = {71},
  number = {4},
  eprint = {1806188},
  eprinttype = {jstor},
  pages = {662--676},
  publisher = {American Economic Association},
  issn = {0002-8282},
  url = {https://www.jstor.org/stable/1806188},
  urldate = {2023-12-07},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Exact Consumer's Surplus and Deadweight Loss_1981_Hausman.pdf}
}

@book{ExperiencingArchitecture_2005_Rasmussen,
  title = {Experiencing {{Architecture}}},
  author = {Rasmussen, Steen Eiler},
  date = {2005},
  edition = {33th printing},
  publisher = {MIT Press},
  location = {Cambridge, Mass},
  isbn = {978-0-262-68002-8},
  langid = {english},
  pagetotal = {245}
}

@incollection{ExplainableAIMethods_2022_HolzingerEtAl,
  title = {Explainable {{AI Methods}} - {{A Brief Overview}}},
  booktitle = {{{xxAI}} - {{Beyond Explainable AI}}: {{International Workshop}}, {{Held}} in {{Conjunction}} with {{ICML}} 2020, {{July}} 18, 2020, {{Vienna}}, {{Austria}}, {{Revised}} and {{Extended Papers}}},
  author = {Holzinger, Andreas and Saranti, Anna and Molnar, Christoph and Biecek, Przemyslaw and Samek, Wojciech},
  editor = {Holzinger, Andreas and Goebel, Randy and Fong, Ruth and Moon, Taesup and Müller, Klaus-Robert and Samek, Wojciech},
  date = {2022},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {13--38},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-031-04083-2_2},
  url = {https://doi.org/10.1007/978-3-031-04083-2_2},
  urldate = {2024-02-15},
  abstract = {Explainable Artificial Intelligence (xAI) is an established field with a vibrant community that has developed a variety of very successful approaches to explain and interpret predictions of complex machine learning models such as deep neural networks. In this article, we briefly introduce a few selected methods and discuss them in a short, clear and concise way. The goal of this article is to give beginners, especially application engineers and data scientists, a quick overview of the state of the art in this current topic. The following 17 methods are covered in this chapter: LIME, Anchors, GraphLIME, LRP, DTD, PDA, TCAV, XGNN, SHAP, ASV, Break-Down, Shapley Flow, Textual Explanations of Visual Models, Integrated Gradients, Causal Models, Meaningful Perturbations, and X-NeSyL.},
  isbn = {978-3-031-04083-2},
  langid = {english},
  keywords = {Evaluation,Explainable AI,Methods,notion},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Explainable AI Methods - A Brief Overview_2022_Holzinger et al.pdf}
}

@article{ExplainableArtificialIntelligence_2020_BarredoArrietaEtAl,
  title = {Explainable {{Artificial Intelligence}} ({{XAI}}): {{Concepts}}, Taxonomies, Opportunities and Challenges toward Responsible {{AI}}},
  shorttitle = {Explainable {{Artificial Intelligence}} ({{XAI}})},
  author = {Barredo Arrieta, Alejandro and Díaz-Rodríguez, Natalia and Del Ser, Javier and Bennetot, Adrien and Tabik, Siham and Barbado, Alberto and Garcia, Salvador and Gil-Lopez, Sergio and Molina, Daniel and Benjamins, Richard and Chatila, Raja and Herrera, Francisco},
  date = {2020-06-01},
  journaltitle = {Information Fusion},
  shortjournal = {Information Fusion},
  volume = {58},
  pages = {82--115},
  issn = {1566-2535},
  doi = {10.1016/j.inffus.2019.12.012},
  url = {https://www.sciencedirect.com/science/article/pii/S1566253519308103},
  urldate = {2024-09-17},
  abstract = {In the last few years, Artificial Intelligence (AI) has achieved a notable momentum that, if harnessed appropriately, may deliver the best of expectations over many application sectors across the field. For this to occur shortly in Machine Learning, the entire community stands in front of the barrier of explainability, an inherent problem of the latest techniques brought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not present in the last hype of AI (namely, expert systems and rule based models). Paradigms underlying this problem fall within the so-called eXplainable AI (XAI) field, which is widely acknowledged as a crucial feature for the practical deployment of AI models. The overview presented in this article examines the existing literature and contributions already done in the field of XAI, including a prospect toward what is yet to be reached. For this purpose we summarize previous efforts made to define explainability in Machine Learning, establishing a novel definition of explainable Machine Learning that covers such prior conceptual propositions with a major focus on the audience for which the explainability is sought. Departing from this definition, we propose and discuss about a taxonomy of recent contributions related to the explainability of different Machine Learning models, including those aimed at explaining Deep Learning methods for which a second dedicated taxonomy is built and examined in detail. This critical literature analysis serves as the motivating background for a series of challenges faced by XAI, such as the interesting crossroads of data fusion and explainability. Our prospects lead toward the concept of Responsible Artificial Intelligence, namely, a methodology for the large-scale implementation of AI methods in real organizations with fairness, model explainability and accountability at its core. Our ultimate goal is to provide newcomers to the field of XAI with a thorough taxonomy that can serve as reference material in order to stimulate future research advances, but also to encourage experts and professionals from other disciplines to embrace the benefits of AI in their activity sectors, without any prior bias for its lack of interpretability.},
  keywords = {Accountability,Comprehensibility,Data Fusion,Deep Learning,Explainable Artificial Intelligence,Fairness,Interpretability,Machine Learning,Privacy,Responsible Artificial Intelligence,Transparency},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Explainable Artificial Intelligence (XAI)_2020_Barredo Arrieta et al.pdf;/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Explainable Artificial Intelligence (XAI)_2020_Barredo Arrieta et al2.pdf;/home/baldoinov/Zotero/storage/NIEQVJIW/S1566253519308103.html}
}

@inproceedings{ExplorandoHierarquiasConceituais_2015_ZacariasFelippo,
  title = {Explorando Hierarquias Conceituais para a Seleção de Conteúdo na Sumarização Automática Multidocumento},
  booktitle = {Simpósio Brasileiro de Tecnologia da Informação e da Linguagem Humana (STIL)},
  author = {Zacarias, Andressa C. I. and Felippo, Ariani Di},
  date = {2015-11-04},
  pages = {257--264},
  publisher = {SBC},
  issn = {0000-0000},
  url = {https://sol.sbc.org.br/index.php/stil/article/view/3988},
  urldate = {2024-08-28},
  abstract = {Partindo-se de uma representação hierárquica dos conceitos de uma coleção de textos sobre determinado assunto, exploram-se medidas estatísticas para detectar os conceitos mais relevantes da coleção. Com isso, as medidas mais pertinentes podem ser usadas em métodos profundos de Sumarização Automática Multidocumento baseados em conhecimento léxico-conceitual.},
  eventtitle = {Simpósio Brasileiro de Tecnologia da Informação e da Linguagem Humana (STIL)},
  langid = {portuguese},
  file = {/home/baldoinov/baldoinov/PDFs/Mestrado/Explorando Hierarquias Conceituais para a Selecao de Conteudo na Sumarizacao_2015_Zacarias et al.pdf}
}

@book{ExploratoryDataAnalysis_1977_Tukey,
  title = {Exploratory {{Data Analysis}}},
  author = {Tukey, John Wilder},
  date = {1977},
  series = {Addison-{{Wesley}} Series in Behavioral Science},
  publisher = {Addison-Wesley Pub. Co},
  location = {Reading, Mass},
  isbn = {978-0-201-07616-5},
  pagetotal = {688},
  keywords = {Statistics}
}

@article{ExploringComplexNetworks_2001_Strogatz,
  title = {Exploring Complex Networks},
  author = {Strogatz, Steven H.},
  date = {2001-03},
  journaltitle = {Nature},
  volume = {410},
  number = {6825},
  pages = {268--276},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/35065725},
  url = {https://www.nature.com/articles/35065725},
  urldate = {2024-02-14},
  abstract = {The study of networks pervades all of science, from neurobiology to statistical physics. The most basic issues are structural: how does one characterize the wiring diagram of a food web or the Internet or the metabolic network of the bacterium Escherichia coli? Are there any unifying principles underlying their topology? From the perspective of nonlinear dynamics, we would also like to understand how an enormous network of interacting dynamical systems — be they neurons, power stations or lasers — will behave collectively, given their individual dynamics and coupling architecture. Researchers are only now beginning to unravel the structure and dynamics of complex networks.},
  issue = {6825},
  langid = {english},
  keywords = {Humanities and Social Sciences,multidisciplinary,notion,Science},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Exploring complex networks_2001_Strogatz.pdf}
}

@inproceedings{ExploringPortugueseHate_2024_AssisEtAl,
  title = {Exploring {{Portuguese Hate Speech Detection}} in {{Low-Resource Settings}}: {{Lightly Tuning Encoder Models}} or {{In-Context Learning}} of {{Large Models}}?},
  shorttitle = {Exploring {{Portuguese Hate Speech Detection}} in {{Low-Resource Settings}}},
  booktitle = {Proceedings of the 16th {{International Conference}} on {{Computational Processing}} of {{Portuguese}} - {{Vol}}. 1},
  author = {Assis, Gabriel and Amorim, Annie and Carvalho, Jonnatahn and family=Oliveira, given=Daniel, prefix=de, useprefix=true and Vianna, Daniela and Paes, Aline},
  editor = {Gamallo, Pablo and Claro, Daniela and Teixeira, António and Real, Livy and Garcia, Marcos and Oliveira, Hugo Gonçalo and Amaro, Raquel},
  date = {2024-03},
  pages = {301--311},
  publisher = {Association for Computational Lingustics},
  location = {Santiago de Compostela, Galicia/Spain},
  url = {https://aclanthology.org/2024.propor-1.31},
  urldate = {2024-09-01},
  eventtitle = {{{PROPOR}} 2024},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/projeto-heiwa/Exploring Portuguese Hate Speech Detection in Low-Resource Settings_2024_Assis et al.pdf}
}

@video{ExposicaoEmJoao_2020_EduardoBittencourt,
  entrysubtype = {video},
  title = {Exposição Em {{João}} 6:60-71},
  shorttitle = {Exposição Em {{João}} 6:60-71},
  editor = {{Eduardo Bittencourt}},
  editortype = {director},
  namea = {{Igreja Cristã Evangélica de Brasília}},
  nameatype = {collaborator},
  date = {2020-08-23},
  url = {https://www.youtube.com/watch?v=MfAReGA0dsk},
  urldate = {2025-02-02},
  abstract = {Pr. Eduardo Bittencourt - Duro é este discurso, quem pode ouvir?  - JOÃO 6:60-71 Celebração de Domingo - 23/08}
}

@article{FactsBlacknessBrazil_1998_DaSilva,
  title = {Facts of {{Blackness}}: {{Brazil}} Is Not {{Quite}} the {{United States}} … and {{Racial Politics}} in {{Brazil}}?1},
  shorttitle = {Facts of {{Blackness}}},
  author = {Da Silva, Denise Ferreira},
  date = {1998-03-01},
  journaltitle = {Social Identities},
  volume = {4},
  number = {2},
  pages = {201--234},
  publisher = {Routledge},
  issn = {1350-4630},
  doi = {10.1080/13504639851807},
  url = {https://doi.org/10.1080/13504639851807},
  urldate = {2023-10-09},
  abstract = {Studies of racial subordination in Brazil usually stress the puzzling co existence of racial inequality with Brazil s self image as a lracial democracy . Frequently, they identify the absence of racial conflict and a clear white black distinc tion as explanationsfor the low level of black political mobilisation. In doing this, these studies unreflectedly take the United Sates as a universal model of racial subordina tion of which Brazilian difference is a mere variation. What seems to escape these analysts is that the Brazilian construction of race was set against the view that lracial differences identify distinct groups, a view which still prevails in the United States and in sociological constructions of race. Actually, an analysis of writings on Brazilian subjectivity suggests that the texts which write blackness do so by deploying various modern categories of lbeing race, nation, gender, and class both in the narratives which have produced blacks as subordinate subjects in modernity and in the texts which aim to foster black emancipation.},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Facts of Blackness_1998_Da Silva.pdf}
}

@inproceedings{FairnessEqualityPower_2021_KasyAbebe,
  title = {Fairness, {{Equality}}, and {{Power}} in {{Algorithmic Decision-Making}}},
  booktitle = {Proceedings of the 2021 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Kasy, Maximilian and Abebe, Rediet},
  date = {2021-03-01},
  series = {{{FAccT}} '21},
  pages = {576--586},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3442188.3445919},
  url = {https://dl.acm.org/doi/10.1145/3442188.3445919},
  urldate = {2023-10-09},
  abstract = {Much of the debate on the impact of algorithms is concerned with fairness, defined as the absence of discrimination for individuals with the same "merit." Drawing on the theory of justice, we argue that leading notions of fairness suffer from three key limitations: they legitimize inequalities justified by "merit;" they are narrowly bracketed, considering only differences of treatment within the algorithm; and they consider between-group and not within-group differences. We contrast this fairness-based perspective with two alternate perspectives: the first focuses on inequality and the causal impact of algorithms and the second on the distribution of power. We formalize these perspectives drawing on techniques from causal inference and empirical economics, and characterize when they give divergent evaluations. We present theoretical results and empirical examples which demonstrate this tension. We further use these insights to present a guide for algorithmic auditing and discuss the importance of inequality- and power-centered frameworks in algorithmic decision-making.},
  isbn = {978-1-4503-8309-7},
  keywords = {Algorithmic fairness,auditing,empirical economics,inequality,notion,power},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Fairness, Equality, and Power in Algorithmic Decision-Making_2021_Kasy et al.pdf}
}

@online{FaithFateLimits_2023_DziriEtAl,
  title = {Faith and {{Fate}}: {{Limits}} of {{Transformers}} on {{Compositionality}}},
  shorttitle = {Faith and {{Fate}}},
  author = {Dziri, Nouha and Lu, Ximing and Sclar, Melanie and Li, Xiang Lorraine and Jiang, Liwei and Lin, Bill Yuchen and West, Peter and Bhagavatula, Chandra and Bras, Ronan Le and Hwang, Jena D. and Sanyal, Soumya and Welleck, Sean and Ren, Xiang and Ettinger, Allyson and Harchaoui, Zaid and Choi, Yejin},
  date = {2023-06-01},
  eprint = {2305.18654},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2305.18654},
  url = {http://arxiv.org/abs/2305.18654},
  urldate = {2023-06-17},
  abstract = {Transformer large language models (LLMs) have sparked admiration for their exceptional performance on tasks that demand intricate multi-step reasoning. Yet, these models simultaneously show failures on surprisingly trivial problems. This begs the question: Are these errors incidental, or do they signal more substantial limitations? In an attempt to demystify Transformers, we investigate the limits of these models across three representative compositional tasks -- multi-digit multiplication, logic grid puzzles, and a classic dynamic programming problem. These tasks require breaking problems down into sub-steps and synthesizing these steps into a precise answer. We formulate compositional tasks as computation graphs to systematically quantify the level of complexity, and break down reasoning steps into intermediate sub-procedures. Our empirical findings suggest that Transformers solve compositional tasks by reducing multi-step compositional reasoning into linearized subgraph matching, without necessarily developing systematic problem-solving skills. To round off our empirical study, we provide theoretical arguments on abstract multi-step reasoning problems that highlight how Transformers' performance will rapidly decay with increased task complexity.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,notion},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Faith and Fate_2023_Dziri et al.pdf;/home/baldoinov/Zotero/storage/MNBTRR6S/2305.html}
}

@report{filho_o_2022,
  type = {Working paper},
  title = {O Desenvolvimento Da Agricultura Do {{Brasil}} e o Papel Da {{Embrapa}}},
  author = {Filho, Vieira and Ribeiro, José Eustáquio},
  date = {2022},
  number = {2748},
  institution = {Texto para Discussão},
  doi = {10.38116/td2748},
  url = {https://www.econstor.eu/handle/10419/265267},
  urldate = {2023-12-11},
  abstract = {This paper debates on modernization of Brazilian agriculture, presents the construction process of the sector's national innovation system and the role of Empresa Brasileira de Pesquisa Agropecuária (Embrapa), as well as shows Brazil's competitive insertion in the international market of agricultural goods. In between economics and pragmatism, the institutional organization of this successful agriculture's trajectory depended on the efforts of many people, but its implementation required the leadership of a visionary. Eliseu Alves is a man ahead of his time. He dared to dream and thought about the new. He faced resistances. However, wherever he has worked, he left his legacy. He transformed agricultural research through induced institutional innovation. He contributed to the formation of a generation of researchers, as well as created the modern science and technology system of the Brazilian agricultural sector. Thus, he contributed to tropical agriculture development, which expanded the supply of agricultural goods, decreased food prices and preserved the environment.}
}

@video{FinancialModelingFull_2024_QuintEdge,
  entrysubtype = {video},
  title = {Financial {{Modeling Full Course}} for {{Beginners}} [{{With Practical Case Study}}]},
  editor = {{QuintEdge}},
  editortype = {director},
  date = {2024-02-24},
  url = {https://www.youtube.com/watch?app=desktop&v=Rf2QhfF9LgA&ab_channel=QuintEdge},
  urldate = {2025-08-17},
  abstract = {Unlock the secrets of financial modeling with our comprehensive tutorial designed specifically for beginners. Dive deep into the core concepts and techniques of financial modeling through our detailed tutorials and a practical case study that brings theory to life.}
}

@article{FindingGeneralEquilibria_2021_CurryEtAl,
  title = {Finding {{General Equilibria}} in {{Many-Agent Economic Simulations}} Using {{Deep Reinforcement Learning}}},
  author = {Curry, Michael and Trott, Alexander R. and Phade, Soham and Bai, Yu and Zheng, Stephan},
  date = {2021-10-06},
  url = {https://openreview.net/forum?id=d5IQ3k7ed__},
  urldate = {2023-11-06},
  abstract = {Real economies can be seen as a sequential imperfect-information game with many heterogeneous, interacting strategic agents of various agent types, such as consumers, firms, and governments. Dynamic general equilibrium models are common economic tools to model the economic activity, interactions, and outcomes in such systems. However, existing analytical and computational methods struggle to find explicit equilibria when all agents are strategic and interact, while joint learning is unstable and challenging. Amongst others, a key reason is that the actions of one economic agent may change the reward function of another agent, e.g., a consumer's expendable income changes when firms change prices or governments change taxes. We show that multi-agent deep reinforcement learning (RL) can discover stable solutions that are \$\textbackslash epsilon\$-Nash equilibria for a meta-game over agent types, in economic simulations with many agents, through the use of structured learning curricula and efficient GPU-only simulation and training.Conceptually, our approach is more flexible and does not need unrealistic assumptions, e.g., market clearing, that are commonly used for analytical tractability. Our GPU implementation enables training and analyzing economies with a large number of agents within reasonable time frames, e.g., training completes within a day. We demonstrate our approach in real-business-cycle models, a representative family of DGE models, with 100 worker-consumers, 10 firms, and a government who taxes and redistributes. We validate the learned meta-game \$\textbackslash epsilon\$-Nash equilibria through approximate best-response analyses, show that RL policies align with economic intuitions, and that our approach is constructive, e.g., by explicitly learning a spectrum of meta-game \$\textbackslash epsilon\$-Nash equilibria in open economic models.},
  langid = {english},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Finding General Equilibria in Many-Agent Economic Simulations using Deep_2021_Curry et al.pdf}
}

@inproceedings{FineGrainedSpoilerDetection_2019_WanEtAl,
  title = {Fine-{{Grained Spoiler Detection}} from {{Large-Scale Review Corpora}}},
  booktitle = {Proceedings of the 57th {{Conference}} of the {{Association}} for {{Computational Linguistics}}, {{ACL}} 2019, {{Florence}}, {{Italy}}, {{July}} 28- {{August}} 2, 2019, {{Volume}} 1: {{Long Papers}}},
  author = {Wan, Mengting and Misra, Rishabh and Nakashole, Ndapa and McAuley, Julian J.},
  editor = {Korhonen, Anna and Traum, David R. and Màrquez, Lluís},
  date = {2019},
  pages = {2605--2610},
  publisher = {Association for Computational Linguistics},
  doi = {10.18653/V1/P19-1248},
  url = {https://doi.org/10.18653/v1/p19-1248},
  urldate = {2024-08-16},
  file = {/home/baldoinov/baldoinov/PDFs/Mackenzie/projeto-aplicado-iii/Fine-Grained Spoiler Detection from Large-Scale Review Corpora_2019_Wan et al.pdf}
}

@online{FinetuningLanguageModels_2023_TianEtAl,
  title = {Fine-Tuning {{Language Models}} for {{Factuality}}},
  author = {Tian, Katherine and Mitchell, Eric and Yao, Huaxiu and Manning, Christopher D. and Finn, Chelsea},
  date = {2023-11-14},
  url = {https://arxiv.org/abs/2311.08401v1},
  urldate = {2023-12-31},
  abstract = {The fluency and creativity of large pre-trained language models (LLMs) have led to their widespread use, sometimes even as a replacement for traditional search engines. Yet language models are prone to making convincing but factually inaccurate claims, often referred to as 'hallucinations.' These errors can inadvertently spread misinformation or harmfully perpetuate misconceptions. Further, manual fact-checking of model responses is a time-consuming process, making human factuality labels expensive to acquire. In this work, we fine-tune language models to be more factual, without human labeling and targeting more open-ended generation settings than past work. We leverage two key recent innovations in NLP to do so. First, several recent works have proposed methods for judging the factuality of open-ended text by measuring consistency with an external knowledge base or simply a large model's confidence scores. Second, the direct preference optimization algorithm enables straightforward fine-tuning of language models on objectives other than supervised imitation, using a preference ranking over possible model responses. We show that learning from automatically generated factuality preference rankings, generated either through existing retrieval systems or our novel retrieval-free approach, significantly improves the factuality (percent of generated claims that are correct) of Llama-2 on held-out topics compared with RLHF or decoding strategies targeted at factuality. At 7B scale, compared to Llama-2-chat, we observe 58\% and 40\% reduction in factual error rate when generating biographies and answering medical questions, respectively.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Fine-tuning Language Models for Factuality_2023_Tian et al.pdf}
}

@article{FirmsInformalityDevelopment_2018_Ulyssea,
  title = {Firms, {{Informality}}, and {{Development}}: {{Theory}} and {{Evidence}} from {{Brazil}}},
  shorttitle = {Firms, {{Informality}}, and {{Development}}},
  author = {Ulyssea, Gabriel},
  date = {2018-08},
  journaltitle = {American Economic Review},
  volume = {108},
  number = {8},
  pages = {2015--2047},
  issn = {0002-8282},
  doi = {10.1257/aer.20141745},
  url = {https://www.aeaweb.org/articles?id=10.1257/aer.20141745},
  urldate = {2024-05-07},
  abstract = {This paper develops and estimates an equilibrium model where heterogeneous firms can exploit two margins of informality: (i) not register their business, the extensive margin; and (ii) hire workers "off the books," the intensive margin. The model encompasses the main competing frameworks for understanding informality and provides a natural setting to infer their empirical relevance. The counterfactual analysis shows that once the intensive margin is accounted for, firm and labor informality need not move in the same direction as a result of policy changes. Lower informality can be, but is not necessarily associated with higher output, TFP, or welfare.},
  langid = {english},
  keywords = {Choice of Technology Formal and Informal Sectors,Firm Behavior: Empirical Analysis Informal Economy,Institutional Arrangements,Manufacturing and Service Industries,Shadow Economy,Underground Economy Tax Evasion and Avoidance Informal Labor Markets Industrialization},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Firms, Informality, and Development_2018_Ulyssea.pdf;/home/baldoinov/PDFs/Feausp/monografia/Firms, Informality, and Development_2018_Ulyssea2.pdf}
}

@book{FisicaConceitual_2021_Hewitt,
  title = {Física Conceitual},
  author = {Hewitt, Paul G.},
  date = {2021-05-20},
  publisher = {Bookman},
  isbn = {978-85-8260-341-3},
  langid = {portuguese}
}

@book{FlageloEconomiaPrivilegios_2021_Barbosa,
  title = {O Flagelo da Economia de Privilégios. Brasil, 1947-2020. Crescimento, Crise Fiscal e Estagnação},
  author = {Barbosa, Fernando De Holanda},
  date = {2021-11-25},
  publisher = {FGV},
  location = {Rio de Janeiro, RJ, Brasil},
  abstract = {A economia de privilégios é um produto da cultura brasileira. Um grupo bastante organizado e importante, composto por empresários obtendo subsídios, transferências e tratamento fiscal diferenciado; trabalhadores com tratamentos especiais inclusive de impostos; funcionários públicos dos três poderes com salários acima do setor privado e até anistiados com aposentadorias e pensões especiais, procura, por vários mecanismos, extrair renda do Estado. O resultado desse ataque predatório nas finanças públicas produz déficit porque uma parte da população não aceita aumento de impostos para pagar a conta. A crise fiscal resulta desse conflito social. A obra traz alternativas e análises para o fim do flagelo da economia de privilégios, que depende de um pacto político da sociedade brasileira que estabeleça o princípio de regras universais para todo e qualquer cidadão.},
  isbn = {9786556520940},
  langid = {portuguese}
}

@book{FluentPythonClear_2022_Ramalho,
  title = {Fluent {{Python}}: Clear, Concise, and Effective Programming},
  shorttitle = {Fluent {{Python}}},
  author = {Ramalho, Luciano},
  date = {2022},
  edition = {Second edition},
  publisher = {O'Reilly},
  location = {Beijing Boston Farnham Sebastopol Tokyo},
  abstract = {Don't waste time bending Python to fit patterns you've learned in other languages. Python's simplicity lets you become productive quickly, but often this means you aren't using everything the language has to offer. With the updated edition of this hands-on guide, you'll learn how to write effective, modern Python 3 code by leveraging its best ideas. Discover and apply idiomatic Python 3 features beyond your past experience. Author Luciano Ramalho guides you through Python's core language features and libraries and teaches you how to make your code shorter, faster, and more readable},
  isbn = {978-1-4920-5635-5},
  langid = {english},
  pagetotal = {983}
}

@online{ForecastEvaluationData_2022_HewamalageEtAl,
  title = {Forecast {{Evaluation}} for {{Data Scientists}}: {{Common Pitfalls}} and {{Best Practices}}},
  shorttitle = {Forecast {{Evaluation}} for {{Data Scientists}}},
  author = {Hewamalage, Hansika and Ackermann, Klaus and Bergmeir, Christoph},
  date = {2022-04-04},
  eprint = {2203.10716},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2203.10716},
  url = {http://arxiv.org/abs/2203.10716},
  urldate = {2024-02-07},
  abstract = {Machine Learning (ML) and Deep Learning (DL) methods are increasingly replacing traditional methods in many domains involved with important decision making activities. DL techniques tailor-made for specific tasks such as image recognition, signal processing, or speech analysis are being introduced at a fast pace with many improvements. However, for the domain of forecasting, the current state in the ML community is perhaps where other domains such as Natural Language Processing and Computer Vision were at several years ago. The field of forecasting has mainly been fostered by statisticians/econometricians; consequently the related concepts are not the mainstream knowledge among general ML practitioners. The different non-stationarities associated with time series challenge the data-driven ML models. Nevertheless, recent trends in the domain have shown that with the availability of massive amounts of time series, ML techniques are quite competent in forecasting, when related pitfalls are properly handled. Therefore, in this work we provide a tutorial-like compilation of the details of one of the most important steps in the overall forecasting process, namely the evaluation. This way, we intend to impart the information of forecast evaluation to fit the context of ML, as means of bridging the knowledge gap between traditional methods of forecasting and state-of-the-art ML techniques. We elaborate on the different problematic characteristics of time series such as non-normalities and non-stationarities and how they are associated with common pitfalls in forecast evaluation. Best practices in forecast evaluation are outlined with respect to the different steps such as data partitioning, error calculation, statistical testing, and others. Further guidelines are also provided along selecting valid and suitable error measures depending on the specific characteristics of the dataset at hand.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,notion,Statistics - Methodology},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Forecast Evaluation for Data Scientists_2022_Hewamalage et al.pdf;/home/baldoinov/Zotero/storage/J7GULZX9/2203.html}
}

@article{ForecastingDynamicPanel_2020_LiuEtAl,
  title = {Forecasting with {{Dynamic Panel Data Models}}},
  author = {Liu, Laura and Moon, Hyungsik Roger and Schorfheide, Frank},
  date = {2020},
  journaltitle = {Econometrica},
  volume = {88},
  number = {1},
  eprint = {48584337},
  eprinttype = {jstor},
  pages = {171--201},
  publisher = {[Wiley, The Econometric Society]},
  issn = {0012-9682},
  url = {https://www.jstor.org/stable/48584337},
  urldate = {2024-10-11},
  abstract = {This paper considers the problem of forecasting a collection of short time series using cross-sectional information in panel data. We construct point predictors using Tweedie’s formula for the posterior mean of heterogeneous coefficients under a correlated random effects distribution. This formula utilizes cross-sectional information to transform the unit-specific (quasi) maximum likelihood estimator into an approximation of the posterior mean under a prior distribution that equals the population distribution of the random coefficients. We show that the risk of a predictor based on a nonparametric kernel estimate of the Tweedie correction is asymptotically equivalent to the risk of a predictor that treats the correlated random effects distribution as known (ratio optimality). Our empirical Bayes predictor performs well compared to various competitors in a Monte Carlo study. In an empirical application, we use the predictor to forecast revenues for a large panel of bank holding companies and compare forecasts that condition on actual and severely adverse macroeconomic conditions.},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Recovery/Modelo de Incidencia de Acao Contra/Forecasting with Dynamic Panel Data Models_2020_Liu et al.pdf}
}

@online{ForecastingManyTime_2021_Dancho,
  title = {Forecasting {{Many Time Series}} ({{Using NO For-Loops}})},
  author = {Dancho, Matt},
  date = {2021-07-19},
  url = {https://www.business-science.io/code-tools/2021/07/19/modeltime-panel-data.html},
  urldate = {2024-10-11},
  abstract = {I'm super excited to introduce the new panel data forecasting functionality in modeltime. It's perfect for making many forecasts at once without for-loops.},
  langid = {english},
  organization = {Business Science},
  file = {/home/baldoinov/Zotero/storage/ZNMDQHQK/modeltime-panel-data.html}
}

@article{ForecastingPanelData_2007_Baltagi,
  title = {Forecasting with {{Panel Data}}},
  author = {Baltagi, Badi},
  date = {2007-01-01},
  journaltitle = {Center for Policy Research},
  url = {https://surface.syr.edu/cpr/74},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Recovery/Modelo de Incidencia de Acao Contra/Forecasting with Panel Data_2007_Baltagi.pdf}
}

@unpublished{FormacaoEconomicaSocial_2019_Mauro,
  title = {Formação Econômica e Social do Brasil: uma síntese},
  author = {Mauro, Gabriel Galeti},
  date = {2019},
  langid = {portuguese},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/formacao-economica-e-social-do-brasil-i/Formacao Economica e Social do Brasil_2019_Mauro.pdf}
}

@online{FormalAlgorithmsTransformers_2022_PhuongHutter,
  title = {Formal {{Algorithms}} for {{Transformers}}},
  author = {Phuong, Mary and Hutter, Marcus},
  date = {2022-07-19},
  eprint = {2207.09238},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2207.09238},
  url = {http://arxiv.org/abs/2207.09238},
  urldate = {2023-12-31},
  abstract = {This document aims to be a self-contained, mathematically precise overview of transformer architectures and algorithms (*not* results). It covers what transformers are, how they are trained, what they are used for, their key architectural components, and a preview of the most prominent models. The reader is assumed to be familiar with basic ML terminology and simpler neural network architectures such as MLPs.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,notion},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Formal Algorithms for Transformers_2022_Phuong et al.pdf;/home/baldoinov/Zotero/storage/WGR9P65C/2207.html}
}

@report{FrameworkOfflineEvaluation_2017_ChulyadyoLeray,
  type = {Technical Report},
  title = {A {{Framework}} for {{Offline Evaluation}} of {{Recommender Systems}} Based on {{Probabilistic Relational Models}}},
  author = {Chulyadyo, Rajani and Leray, Philippe},
  date = {2017-12},
  institution = {Laboratoire des Sciences du Numérique de Nantes ; Capacités SAS},
  url = {https://hal.science/hal-01666117},
  urldate = {2024-11-06},
  abstract = {Recommender systems and their evaluation have been widely studied topics since more than past two decades. Implementation of such systems can be found in numerous commercial and non-commercial software. However, most of the existing open-source/free libraries for recommender systems still deal with single-table data whereas recent studies on recom-mender systems focus on the use of relational (multi-table, multi-entity) data. In our earlier work (Chulyadyo and Leray [2014]), we had presented a personalized recommender system that works with relational data, and is based on Probabilistic Relational Models (PRM), a framework for mod-eling uncertainties present on relational data. With the aim to benefit from existing software for evaluating recommender systems, we propose a framework for evaluating such PRM-based recommender systems in this report. The basic idea is to delegate the tasks of evaluation to an external library while the focus for the implementation of the recommender system under study is only on learning a recommendation model and making recommendations from it. Our proposed evaluation framework is generic, and should not be limited only to PRM-based recommender systems. Any recommender systems dealing with relational data can follow this evaluation framework.},
  file = {/home/baldoinov/baldoinov/PDFs/Mackenzie/projeto-aplicado-iii/A Framework for Offline Evaluation of Recommender Systems based on_2017_Chulyadyo et al.pdf}
}

@incollection{FrontMatter_2021_Kissell,
  title = {Front {{Matter}}},
  booktitle = {Algorithmic {{Trading Methods}} ({{Second Edition}})},
  editor = {Kissell, Robert L.},
  date = {2021-01-01},
  pages = {iii},
  publisher = {Academic Press},
  doi = {10.1016/B978-0-12-815630-8.01001-2},
  url = {https://www.sciencedirect.com/science/article/pii/B9780128156308010012},
  urldate = {2025-04-21},
  isbn = {978-0-12-815630-8}
}

@book{FundamentalMethodsMathematical_2005_ChiangWainwright,
  title = {Fundamental {{Methods}} of {{Mathematical Economics}}},
  author = {Chiang, Alpha C. and Wainwright, Kevin},
  date = {2005},
  edition = {4th ed},
  publisher = {McGraw-Hill/Irwin},
  location = {Boston, Mass},
  isbn = {978-0-07-010910-0},
  pagetotal = {688},
  keywords = {Economics Mathematical}
}

@book{FundamentosMatematicaElementar_2022_DolcePompeo,
  title = {Fundamentos de Matemática Elementar Volume 10:  Geometria espacial posição e métrica},
  shorttitle = {Fundamentos de matemática elementar volume 10},
  author = {Dolce, Osvaldo and Pompeo, José Nicolau},
  date = {2022-03-31},
  publisher = {Editora Atual},
  isbn = {978-85-357-1758-7},
  langid = {portuguese}
}

@book{FundamentosMatematicaElementar_2022_DolcePompeoa,
  title = {Fundamentos de Matemática Elementar Volume 9: Geometria Plana},
  shorttitle = {Fundamentos de Matemática Elementar Volume 9},
  author = {Dolce, Osvaldo and Pompeo, José Nicolau},
  date = {2022-03-31},
  publisher = {Editora Atual},
  isbn = {978-85-357-1686-3},
  langid = {portuguese}
}

@book{FundamentosMatematicaElementar_2022_Hazzani,
  title = {Fundamentos de Matemática Elementar Volume 5: Combinatória e Probabilidade},
  shorttitle = {Fundamentos de matemática elementar volume 5},
  author = {Hazzani, Samuel},
  date = {2022-03-31},
  publisher = {Editora Atual},
  isbn = {978-85-357-1750-1},
  langid = {portuguese}
}

@book{FundamentosMatematicaElementar_2022_Iezzi,
  title = {Fundamentos de Matemática Elementar Volume 6: Complexos, Polinômios e Equações},
  shorttitle = {Fundamentos de matemática elementar volume 6},
  author = {Iezzi, Gelson},
  date = {2022-03-31},
  publisher = {Editora Atual},
  isbn = {978-85-357-1752-5},
  langid = {portuguese}
}

@book{FundamentosMatematicaElementar_2022_Iezzia,
  title = {Fundamentos de Matemática Elementar Volume 7: Geometria Analítica},
  shorttitle = {Fundamentos de matemática elementar volume 7},
  author = {Iezzi, Gelson},
  date = {2022-03-31},
  publisher = {Editora Atual},
  isbn = {978-85-357-1754-9},
  langid = {portuguese}
}

@book{FundamentosMatematicaElementar_2022_Iezzib,
  title = {Fundamentos de Matemática Elementar Volume 3: Trigonometria},
  author = {Iezzi, Gelson},
  date = {2022-03-31},
  publisher = {Editora Atual},
  isbn = {978-85-357-1684-9},
  langid = {portuguese}
}

@book{FundamentosMatematicaElementar_2022_IezziEtAl,
  title = {Fundamentos de Matemática Elementar Volume 11: Matemática Comercial, Matemática Financeira e Estatística Descritiva},
  shorttitle = {Fundamentos de matemática elementar volume 11},
  author = {Iezzi, Gelson and Hazzani, Samuel and Degenszajn, David},
  date = {2022-03-31},
  publisher = {Editora Atual},
  isbn = {978-85-357-1760-0},
  langid = {portuguese}
}

@book{FundamentosMatematicaElementar_2022_IezziEtAla,
  title = {Fundamentos de Matemática Elementar Volume 8: Limites, Derivadas e Noções de Integral},
  shorttitle = {Fundamentos de matemática elementar volume 8},
  author = {Iezzi, Gelson and Murakami, Carlos and Machado, Nilson José},
  date = {2022-03-31},
  publisher = {Editora Atual},
  isbn = {978-85-357-1756-3},
  langid = {portuguese}
}

@book{FundamentosMatematicaElementar_2022_IezziEtAlb,
  title = {Fundamentos de Matemática Elementar Volume 2: Logaritmos},
  shorttitle = {Fundamentos de matemática elementar  volume 2},
  author = {Iezzi, Gelson and Dolce, Osvaldo and Murakami, Carlos},
  date = {2022-03-31},
  publisher = {Editora Atual},
  isbn = {978-85-357-1682-5},
  langid = {portuguese}
}

@book{FundamentosMatematicaElementar_2022_IezziHazzani,
  title = {Fundamentos de Matemática Elementar Volume 4: Sequências, Matrizes, Determinantes, Sistemas},
  shorttitle = {Fundamentos de matemática elementar volume 4},
  author = {Iezzi, Gelson and Hazzani, Samuel},
  date = {2022-03-31},
  publisher = {Editora Atual},
  isbn = {978-85-357-1748-8},
  langid = {portuguese}
}

@book{FundamentosMatematicaElementar_2022_IezziMurakami,
  title = {Fundamentos de Matemática Elementar Volume 1: Conjuntos e Funções},
  shorttitle = {Fundamentos de matemática elementar volume 1},
  author = {Iezzi, Gelson and Murakami, Carlos},
  date = {2022-03-31},
  publisher = {Editora Atual},
  isbn = {978-85-357-1680-1},
  langid = {brazilian}
}

@book{FundamentosMatematicosPara_2016_Gersting,
  title = {Fundamentos Matemáticos para a Ciência da Computação: Matemática Discreta e suas Aplicações},
  shorttitle = {Fundamentos matemáticos para a ciência da computação},
  author = {Gersting, Judith L.},
  date = {2016-07-26},
  publisher = {Ltc-Livros Tecnicos E Cientificos Editora Lda},
  isbn = {978-85-216-3259-7},
  langid = {brazilian}
}

@book{GamesStrategy_2015_DixitEtAl,
  title = {Games of {{Strategy}}},
  author = {Dixit, Avinash K. and Skeath van Mulbregt, Susan Emily and Reiley, David H.},
  date = {2015},
  edition = {4., international student ed},
  publisher = {Norton},
  location = {New York, NY},
  isbn = {978-0-393-91968-4 978-0-393-12444-6 978-0-393-92075-8},
  langid = {english},
  pagetotal = {732}
}

@article{GARNetGraphAttention_2022_XuEtAl,
  title = {{{GAR-Net}}: {{A Graph Attention Reasoning Network}} for Conversation Understanding},
  shorttitle = {{{GAR-Net}}},
  author = {Xu, Hua and Yuan, Ziqi and Zhao, Kang and Xu, Yunfeng and Zou, Jiyun and Gao, Kai},
  date = {2022-03-15},
  journaltitle = {Knowledge-Based Systems},
  shortjournal = {Knowledge-Based Systems},
  volume = {240},
  pages = {108055},
  issn = {0950-7051},
  doi = {10.1016/j.knosys.2021.108055},
  url = {https://www.sciencedirect.com/science/article/pii/S0950705121011400},
  urldate = {2023-12-14},
  abstract = {Conversation understanding, as a necessary step for many applications, including social media, education, and argumentation mining, has been gaining increasing attention from the research community. Reasoning over long-term dependent contextual information is the key to utterance-level conversation understanding. Aiming to emphasize the importance of contextual reasoning, an end-to-end graph attention reasoning network which takes both word-level and utterance-level context into concern is proposed. To be specific, a word-level encoder with a novel convolution gate is first built to filter out irrelevant contextual information. Based on the representation extracted by word-level encoder, a graph reasoning network is designed to utilize the context among utterance-level, where the entire conversation is treated as a fully connected graph, utterances as nodes, and attention scores between utterances as edges. The proposed model is a general framework for conversation understanding tasks, which can be flexibly applied on most conversation datasets without changing the network architecture. Furthermore, we revise the tensor fusion network by adding a residual connection to explore cross-modal conversation understanding. For uni-modal scene (text modality), experiments show that the proposed method surpasses current state-of-the-art methods on emotion recognition, intent classification, and dialogue act identification tasks. For cross-modal scenes (text modality and audio modality), experiments on IEMOCAP and MELD datasets show that the proposed method can use cross-modal information to improve model performance.22The code will be available at https://github.com/thuiar/GAR-Net.},
  keywords = {Conversation understanding,Convolution gate,Cross-modal,Graph reasoning,notion},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/projeto-heiwa-revisao-de-literatura/GAR-Net_2022_Xu et al.pdf}
}

@online{GeneralizationTransformerNetworks_2021_DwivediBresson,
  title = {A {{Generalization}} of {{Transformer Networks}} to {{Graphs}}},
  author = {Dwivedi, Vijay Prakash and Bresson, Xavier},
  date = {2021-01-24},
  eprint = {2012.09699},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2012.09699},
  url = {http://arxiv.org/abs/2012.09699},
  urldate = {2023-07-24},
  abstract = {We propose a generalization of transformer neural network architecture for arbitrary graphs. The original transformer was designed for Natural Language Processing (NLP), which operates on fully connected graphs representing all connections between the words in a sequence. Such architecture does not leverage the graph connectivity inductive bias, and can perform poorly when the graph topology is important and has not been encoded into the node features. We introduce a graph transformer with four new properties compared to the standard model. First, the attention mechanism is a function of the neighborhood connectivity for each node in the graph. Second, the positional encoding is represented by the Laplacian eigenvectors, which naturally generalize the sinusoidal positional encodings often used in NLP. Third, the layer normalization is replaced by a batch normalization layer, which provides faster training and better generalization performance. Finally, the architecture is extended to edge feature representation, which can be critical to tasks s.a. chemistry (bond type) or link prediction (entity relationship in knowledge graphs). Numerical experiments on a graph benchmark demonstrate the performance of the proposed graph transformer architecture. This work closes the gap between the original transformer, which was designed for the limited case of line graphs, and graph neural networks, that can work with arbitrary graphs. As our architecture is simple and generic, we believe it can be used as a black box for future applications that wish to consider transformer and graphs.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,notion},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/A Generalization of Transformer Networks to Graphs_2021_Dwivedi et al.pdf;/home/baldoinov/Zotero/storage/Z73NME2L/2012.html}
}

@online{GenerativeAgentsInteractive_2023_ParkEtAl,
  title = {Generative {{Agents}}: {{Interactive Simulacra}} of {{Human Behavior}}},
  shorttitle = {Generative {{Agents}}},
  author = {Park, Joon Sung and O'Brien, Joseph C. and Cai, Carrie J. and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S.},
  date = {2023-08-05},
  eprint = {2304.03442},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2304.03442},
  url = {http://arxiv.org/abs/2304.03442},
  urldate = {2023-09-28},
  abstract = {Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents--computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent's experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors: for example, starting with only a single user-specified notion that one agent wants to throw a Valentine's Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture--observation, planning, and reflection--each contribute critically to the believability of agent behavior. By fusing large language models with computational, interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,notion},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Generative Agents_2023_Park et al.pdf;/home/baldoinov/Zotero/storage/ZGBWRC9B/2304.html}
}

@article{GentleIntroductionGraph_2021_Sanchez-LengelingEtAl,
  title = {A {{Gentle Introduction}} to {{Graph Neural Networks}}},
  author = {Sanchez-Lengeling, Benjamin and Reif, Emily and Pearce, Adam and Wiltschko, Alexander B.},
  date = {2021-09-02},
  journaltitle = {Distill},
  shortjournal = {Distill},
  volume = {6},
  number = {9},
  pages = {e33},
  issn = {2476-0757},
  doi = {10.23915/distill.00033},
  url = {https://distill.pub/2021/gnn-intro},
  urldate = {2023-06-02},
  abstract = {What components are needed for building learning algorithms that leverage the structure and properties of graphs?},
  langid = {english},
  keywords = {notion},
  file = {/home/baldoinov/Zotero/storage/LAVJ3FP4/gnn-intro.html}
}

@online{GentleIntroductionState_2024_JorgeCardete,
  title = {Gentle Introduction to {{State Space Models}} | by {{Jorgecardete}} | in {{The Deep Hub}} - {{Freedium}}},
  author = {{Jorge Cardete}},
  date = {2024-03-15},
  url = {https://freedium.cfd/https://medium.com/thedeephub/gentle-introduction-to-state-space-models-e8cd7501e0cf?utm_source=substack&utm_medium=email},
  urldate = {2024-06-11},
  file = {/home/baldoinov/Zotero/storage/TLS2I5GB/gentle-introduction-to-state-space-models-e8cd7501e0cf.html}
}

@online{GetYourWork_2019_Evans,
  title = {Get Your Work Recognized: Write a Brag Document},
  shorttitle = {Get Your Work Recognized},
  author = {Evans, Julia},
  date = {2019-06-28},
  url = {https://jvns.ca/blog/brag-documents/},
  urldate = {2024-03-01},
  abstract = {Get your work recognized: write a brag document},
  langid = {english},
  organization = {Julia Evans},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Diversos/Get your work recognized_2019_Evans.pdf;/home/baldoinov/Zotero/storage/LJJ4XLI8/brag-documents.html}
}

@online{GNNRAGGraphNeural_2024_MavromatisKarypis,
  title = {{{GNN-RAG}}: {{Graph Neural Retrieval}} for {{Large Language Model Reasoning}}},
  shorttitle = {{{GNN-RAG}}},
  author = {Mavromatis, Costas and Karypis, George},
  date = {2024-05-30},
  eprint = {2405.20139},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2405.20139},
  url = {http://arxiv.org/abs/2405.20139},
  urldate = {2024-06-02},
  abstract = {Knowledge Graphs (KGs) represent human-crafted factual knowledge in the form of triplets (head, relation, tail), which collectively form a graph. Question Answering over KGs (KGQA) is the task of answering natural questions grounding the reasoning to the information provided by the KG. Large Language Models (LLMs) are the state-of-the-art models for QA tasks due to their remarkable ability to understand natural language. On the other hand, Graph Neural Networks (GNNs) have been widely used for KGQA as they can handle the complex graph information stored in the KG. In this work, we introduce GNN-RAG, a novel method for combining language understanding abilities of LLMs with the reasoning abilities of GNNs in a retrieval-augmented generation (RAG) style. First, a GNN reasons over a dense KG subgraph to retrieve answer candidates for a given question. Second, the shortest paths in the KG that connect question entities and answer candidates are extracted to represent KG reasoning paths. The extracted paths are verbalized and given as input for LLM reasoning with RAG. In our GNN-RAG framework, the GNN acts as a dense subgraph reasoner to extract useful graph information, while the LLM leverages its natural language processing ability for ultimate KGQA. Furthermore, we develop a retrieval augmentation (RA) technique to further boost KGQA performance with GNN-RAG. Experimental results show that GNN-RAG achieves state-of-the-art performance in two widely used KGQA benchmarks (WebQSP and CWQ), outperforming or matching GPT-4 performance with a 7B tuned LLM. In addition, GNN-RAG excels on multi-hop and multi-entity questions outperforming competing approaches by 8.9--15.5\% points at answer F1.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/GNN-RAG_2024_Mavromatis et al.pdf;/home/baldoinov/Zotero/storage/CWQ6URIY/2405.html}
}

@article{gonzalez_2015,
  title = {The {{Montreal Protocol}}: How Today’s Successes Offer a Pathway to the Future},
  author = {Gonzalez, M. and Taddonio, K.N. and Sherman, N.J.},
  date = {2015},
  journaltitle = {Journal of Environmental Studies and Sciences},
  volume = {5},
  pages = {122--129}
}

@article{GoodBadJustifications_2023_Sugden,
  title = {Good and Bad Justifications of Analytical Modelling},
  author = {Sugden, Robert},
  date = {2023},
  journaltitle = {Journal of Economic Methodology},
  volume = {0},
  number = {0},
  pages = {1--11},
  publisher = {Routledge},
  issn = {1350-178X},
  doi = {10.1080/1350178X.2023.2275584},
  url = {https://doi.org/10.1080/1350178X.2023.2275584},
  urldate = {2024-06-05},
  abstract = {Gilboa, Postlewaite, Samuelson and Schmeidler (2022, Economic theories and their dueling interpretations. Journal of Economic Methodology, 1–20. https://doi.org/10.1080/1350178X.2022.2142270; henceforth GPSS) give a ‘sociological’ account of various ways in which economists claim to find value in ‘analytical’ models – i.e. models that investigate formal relationships between concepts without deriving substantive empirical or normative conclusions. In this paper, I argue that some of the claims that GPSS report economists as making are defensible, but that others are used in support of modelling strategies that have little or no scientific value.},
  keywords = {economic methodology,economics,modelling,Models},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Good and bad justifications of analytical modelling_2023_Sugden.pdf}
}

@incollection{GPUComputingEconomics_2014_Aldrich,
  title = {{{GPU Computing}} in {{Economics}}},
  booktitle = {Handbook of {{Computational Economics}}},
  author = {Aldrich, Eric M.},
  editor = {Schmedders, Karl and Judd, Kenneth L.},
  date = {2014-01-01},
  series = {Handbook of {{Computational Economics Vol}}. 3},
  volume = {3},
  pages = {557--598},
  publisher = {Elsevier},
  doi = {10.1016/B978-0-444-52980-0.00010-4},
  url = {https://www.sciencedirect.com/science/article/pii/B9780444529800000104},
  urldate = {2023-09-18},
  abstract = {This paper discusses issues related to GPU for economic problems. It highlights new methodologies and resources that are available for solving and estimating economic models and emphasizes situations when they are useful and others where they are impractical. Two examples illustrate the different ways these GPU parallel methods can be employed to speed computation.},
  keywords = {Asset pricing,Dynamic programming,General equilibrium,GPU computing,Heterogeneous beliefs,notion,Parallel computing},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/GPU Computing in Economics_2014_Aldrich.pdf;/home/baldoinov/Zotero/storage/GA2CMPH7/B9780444529800000104.html}
}

@online{GRAGGraphRetrievalAugmented_2024_HuEtAl,
  title = {{{GRAG}}: {{Graph Retrieval-Augmented Generation}}},
  shorttitle = {{{GRAG}}},
  author = {Hu, Yuntong and Lei, Zhihan and Zhang, Zheng and Pan, Bo and Ling, Chen and Zhao, Liang},
  date = {2024-05-26},
  eprint = {2405.16506},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2405.16506},
  url = {http://arxiv.org/abs/2405.16506},
  urldate = {2024-06-02},
  abstract = {While Retrieval-Augmented Generation (RAG) enhances the accuracy and relevance of responses by generative language models, it falls short in graph-based contexts where both textual and topological information are important. Naive RAG approaches inherently neglect the structural intricacies of textual graphs, resulting in a critical gap in the generation process. To address this challenge, we introduce \$\textbackslash textbf\{Graph Retrieval-Augmented Generation (GRAG)\}\$, which significantly enhances both the retrieval and generation processes by emphasizing the importance of subgraph structures. Unlike RAG approaches that focus solely on text-based entity retrieval, GRAG maintains an acute awareness of graph topology, which is crucial for generating contextually and factually coherent responses. Our GRAG approach consists of four main stages: indexing of \$k\$-hop ego-graphs, graph retrieval, soft pruning to mitigate the impact of irrelevant entities, and generation with pruned textual subgraphs. GRAG's core workflow-retrieving textual subgraphs followed by soft pruning-efficiently identifies relevant subgraph structures while avoiding the computational infeasibility typical of exhaustive subgraph searches, which are NP-hard. Moreover, we propose a novel prompting strategy that achieves lossless conversion from textual subgraphs to hierarchical text descriptions. Extensive experiments on graph multi-hop reasoning benchmarks demonstrate that in scenarios requiring multi-hop reasoning on textual graphs, our GRAG approach significantly outperforms current state-of-the-art RAG methods while effectively mitigating hallucinations.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/GRAG_2024_Hu et al.pdf;/home/baldoinov/Zotero/storage/QK644QQY/2405.html}
}

@book{GrandesObrasPoliticas_2023_,
  title = {As Grandes Obras Politicas De Maquiavel a Nossos Dias},
  date = {2023-10-11},
  publisher = {Agir Editora Ltda},
  isbn = {978-85-220-0348-8},
  langid = {brazilian},
  file = {/home/baldoinov/baldoinov/PDFs/Livros/As Grandes Obras Politicas De Maquiavel a Nossos Dias_2023_.pdf}
}

@article{GreedyFunctionApproximation_2001_Friedman,
  title = {Greedy Function Approximation: {{A}} Gradient Boosting Machine.},
  shorttitle = {Greedy Function Approximation},
  author = {Friedman, Jerome H.},
  date = {2001-10},
  journaltitle = {The Annals of Statistics},
  volume = {29},
  number = {5},
  pages = {1189--1232},
  publisher = {Institute of Mathematical Statistics},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/aos/1013203451},
  url = {https://projecteuclid.org/journals/annals-of-statistics/volume-29/issue-5/Greedy-function-approximation-A-gradient-boosting-machine/10.1214/aos/1013203451.full},
  urldate = {2024-04-16},
  abstract = {Function estimation/approximation is viewed from the perspective of numerical optimization in function space, rather than parameter space. A connection is made between stagewise additive expansions and steepest-descent minimization. A general gradient descent “boosting” paradigm is developed for additive expansions based on any fitting criterion.Specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification. Special enhancements are derived for the particular case where the individual additive components are regression trees, and tools for interpreting such “TreeBoost” models are presented. Gradient boosting of regression trees produces competitive, highly robust, interpretable procedures for both regression and classification, especially appropriate for mining less than clean data. Connections between this approach and the boosting methods of Freund and Shapire and Friedman, Hastie and Tibshirani are discussed.},
  keywords = {62-02,62-07,62-08,62G08,62H30,68T10,boosting,decision trees,Function estimation,robust nonparametric regression},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/machine-learning-w-pytorch-n-sklearn/Greedy function approximation_2001_Friedman.pdf}
}

@book{GrokkingDeepLearning_2019_Trask,
  title = {Grokking {{Deep Learning}}},
  author = {Trask, Andrew W.},
  date = {2019},
  publisher = {Manning},
  location = {Shelter Island, New York},
  isbn = {978-1-61729-370-2},
  langid = {english},
  pagetotal = {309}
}

@book{GuideEconometrics_2008_Kennedy,
  title = {A {{Guide}} to {{Econometrics}}},
  author = {Kennedy, Peter Elliott},
  date = {2008},
  edition = {6th ed},
  publisher = {Wiley-Blackwell},
  location = {Malden},
  isbn = {978-1-4051-8258-4},
  langid = {english}
}

@incollection{GuideNewcomersAgentBased_2006_AxelrodTesfatsion,
  title = {A {{Guide}} for {{Newcomers}} to {{Agent-Based Modeling}} in the {{Social Sciences}}},
  booktitle = {Handbook of {{Computational Economics}}},
  author = {Axelrod, Robert and Tesfatsion, Leigh},
  editor = {Tesfatsion, L. and Judd, K. L.},
  date = {2006-01-01},
  volume = {2},
  pages = {1647--1659},
  publisher = {Elsevier},
  doi = {10.1016/S1574-0021(05)02044-7},
  url = {https://www.sciencedirect.com/science/article/pii/S1574002105020447},
  urldate = {2023-09-18},
  abstract = {This guide provides pointers to introductory readings, software, and other materials to help newcomers become acquainted with agent-based modeling in the social sciences.},
  keywords = {agent-based modeling,collective behavior,complexity,emergence,evolution,institutional design,learning,markets,networks,norms,notion},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/A Guide for Newcomers to Agent-Based Modeling in the Social Sciences_2006_Axelrod et al.pdf;/home/baldoinov/Zotero/storage/X8XEXRAD/S1574002105020447.html}
}

@report{GuideStudentsInstructors__BurriEtAl,
  title = {A Guide for Students and Instructors {{Version}}: 17 {{July}} 2024},
  author = {Burri, Marc and Kaufmann, Daniel and Ostovan, Nima},
  abstract = {This report documents the use and misuse of generative artificial intelligence in academic economic research and provides guidelines for university students and instructors. It primarily addresses students and instructors in a master’s program in economics; however, the use cases and guidelines may be useful in other fields and academic research in general.},
  langid = {english},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/A guide for students and instructors Version_Burri et al.pdf}
}

@book{HandsonMachineLearning_2019_Geron,
  title = {Hands-on {{Machine Learning}} with {{Scikit-Learn}}, {{Keras}}, and {{TensorFlow}}: {{Concepts}}, {{Tools}}, and {{Techniques}} to {{Build Intelligent Systems}}},
  shorttitle = {Hands-on Machine Learning with {{Scikit-Learn}}, {{Keras}}, and {{TensorFlow}}},
  author = {Géron, Aurélien},
  date = {2019},
  series = {Covid-19 Collection},
  edition = {Second edition},
  publisher = {O'Reilly},
  location = {Beijing Boston Farnham Sebastopol Tokyo},
  isbn = {978-1-4920-3264-9 978-1-09-812597-4},
  langid = {english},
  pagetotal = {819}
}

@book{HandsOnProgramming_2014_Grolemund,
  title = {Hands-{{On Programming}} with {{R}}},
  author = {Grolemund, Garrett},
  date = {2014},
  edition = {First edition},
  publisher = {O'Reilly},
  location = {Sebastopol, CA},
  isbn = {978-1-4493-5901-0},
  langid = {english},
  pagetotal = {230},
  keywords = {Data processing,Handbooks manuals etc,Mathematical statistics,Programmeertalen,R (Computer program language),Statistiek},
  annotation = {OCLC: ocn887746093}
}

@book{hawken_2017,
  title = {Drawdown: {{The}} Most Comprehensive Plan Ever Proposed to Reverse Global Warming},
  author = {Hawken, Paul},
  date = {2017}
}

@article{hayha_2018,
  title = {Operationalizing the Concept of a Safe Operating Space at the {{EU}} Level – First Steps and Explorations},
  author = {Häyhä, T. and Cornell, S.E. and Hoff, H. and Lucas, P. and family=Vuuren, given=D., prefix=van, useprefix=true},
  date = {2018},
  journaltitle = {Stockholm Resilience Centre}
}

@book{HealthEconomics_2009_ZweifelEtAl,
  title = {Health {{Economics}}},
  author = {Zweifel, Peter and Breyer, Friedrich and Kifmann, Mathias},
  date = {2009},
  publisher = {Springer Berlin Heidelberg},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-68540-1},
  url = {http://link.springer.com/10.1007/978-3-540-68540-1},
  urldate = {2024-04-01},
  isbn = {978-3-540-27804-7 978-3-540-68540-1},
  langid = {english}
}

@book{HealthEconomics_2018_Phelps,
  title = {Health {{Economics}}},
  author = {Phelps, Charles E.},
  date = {2018},
  edition = {6th edition},
  publisher = {Routledge/Taylor \& Francis Group},
  location = {New York, NY},
  isbn = {978-1-138-20798-1},
  langid = {english},
  pagetotal = {508},
  keywords = {economics,Economics Medical,Financial Management Hospital,Health Policy,Health Services Needs and Demand,Insurance Health,Marketing of Health Services,United States}
}

@book{HealthEconomicsInternational_2008_McPakeNormand,
  title = {Health {{Economics}}: An {{International Perspective}}},
  shorttitle = {Health Economics},
  author = {McPake, Barbara and Normand, Charles E. M.},
  date = {2008},
  edition = {2. ed., repr},
  publisher = {Routledge},
  location = {Abingdon, Oxon},
  isbn = {978-0-415-39129-0 978-0-203-99524-2 978-0-415-39132-0},
  langid = {english},
  pagetotal = {292}
}

@book{HealthMedicalProfession_1998_Zweifel,
  title = {Health, the {{Medical Profession}}, and {{Regulation}}},
  editor = {Zweifel, Peter},
  editora = {Zweifel, Peter and Frech, H. E.},
  editoratype = {redactor},
  date = {1998},
  series = {Developments in {{Health Economics}} and {{Public Policy}}},
  volume = {6},
  publisher = {Springer US},
  location = {Boston, MA},
  doi = {10.1007/978-1-4615-5681-7},
  url = {http://link.springer.com/10.1007/978-1-4615-5681-7},
  urldate = {2024-04-11},
  isbn = {978-1-4613-7601-9 978-1-4615-5681-7},
  langid = {english}
}

@article{herberz_2020,
  title = {Sustainability Assessment of a Single-Use Plastics Ban},
  author = {Herberz, T. and Barlow, C.Y. and Finkbeiner, M.},
  date = {2020},
  journaltitle = {Sustainability},
  volume = {12},
  number = {9}
}

@book{HistoriaEconomicaSocial_2016_KleinLuna,
  title = {História econômica e social do Brasil: O Brasil desde a República},
  shorttitle = {História econômica e social do Brasil},
  author = {Klein, Herbert S. and Luna, Francisco Vidal},
  date = {2016-03-16},
  edition = {1ª edição},
  publisher = {Saraiva Uni},
  isbn = {978-85-472-0776-2},
  langid = {portuguese}
}

@mvbook{HistoriaGeralAfrica_2010_Ajayi,
  title = {História Geral da África VI: África do século XIX à década de 1880},
  author = {Ajayi, J. G. Ade},
  date = {2010-08-09},
  series = {História Geral da África},
  volume = {6},
  publisher = {UNESCO Brasil},
  isbn = {978-85-7652-128-0},
  langid = {portuguese},
  volumes = {8},
  keywords = {Ciências sociais},
  file = {/home/baldoinov/baldoinov/PDFs/Livros/Historia Geral da Africa VI_2010_Ajayi.pdf}
}

@mvbook{HistoriaGeralAfrica_2010_AliA.Mazrui,
  title = {História Geral da África VIII: África desde 1935},
  editor = {{Ali A. Mazrui}},
  date = {2010-08-09},
  series = {História Geral da África},
  volume = {8},
  publisher = {UNESCO Brasil},
  isbn = {978-85-7652-130-3},
  langid = {portuguese},
  volumes = {8},
  keywords = {Ciências sociais},
  file = {/home/baldoinov/baldoinov/PDFs/Livros/Historia Geral da Africa VIII_2010_Ali A. Mazrui.pdf}
}

@mvbook{HistoriaGeralAfrica_2010_Boahen,
  title = {História Geral da África VII: África sob dominação colonial, 1880-1935},
  editor = {Boahen, Albert Adu},
  date = {2010-08-09},
  series = {História Geral da África},
  volume = {7},
  publisher = {UNESCO Brasil},
  isbn = {978-85-7652-129-7},
  langid = {portuguese},
  volumes = {8},
  keywords = {Ciências sociais},
  file = {/home/baldoinov/baldoinov/PDFs/Livros/Historia Geral da Africa VII_2010_Boahen.pdf}
}

@mvbook{HistoriaGeralAfrica_2010_Fasi,
  title = {História Geral da África III: África do século VII ao XI},
  editor = {Fasi, Mohammed El},
  date = {2010-08-09},
  series = {História Geral da África},
  volume = {3},
  publisher = {UNESCO Brasil},
  isbn = {978-85-7652-125-9},
  langid = {portuguese},
  volumes = {8},
  keywords = {Ciências sociais},
  file = {/home/baldoinov/baldoinov/PDFs/Livros/Historia Geral da Africa III_2010_Fasi.pdf}
}

@mvbook{HistoriaGeralAfrica_2010_Ki-Zerbo,
  title = {História Geral da África I: Metodologia e pré-história da África},
  editor = {Ki-Zerbo, Joseph},
  date = {2010-08-09},
  series = {História Geral da África},
  volume = {1},
  publisher = {UNESCO Brasil},
  isbn = {978-85-7652-123-5},
  langid = {portuguese},
  volumes = {8},
  keywords = {Ciências sociais},
  file = {/home/baldoinov/baldoinov/PDFs/Livros/Historia Geral da Africa I_2010_Ki-Zerbo.pdf}
}

@mvbook{HistoriaGeralAfrica_2010_Mokhtar,
  title = {História Geral da África II: África Antiga},
  editor = {Mokhtar, Gamal},
  date = {2010-08-09},
  series = {História Geral da África},
  volume = {2},
  publisher = {UNESCO Brasil},
  isbn = {978-85-7652-124-2},
  langid = {portuguese},
  volumes = {8},
  keywords = {Ciências sociais},
  file = {/home/baldoinov/baldoinov/PDFs/Livros/Historia Geral da Africa II_2010_Mokhtar.pdf}
}

@mvbook{HistoriaGeralAfrica_2010_Niane,
  title = {História Geral da África IV: África do século XII ao XVI},
  editor = {Niane, Djibril Tamsir},
  date = {2010-08-09},
  series = {História Geral da África},
  volume = {4},
  publisher = {UNESCO Brasil},
  isbn = {978-85-7652-126-6},
  langid = {portuguese},
  volumes = {8},
  keywords = {Ciências sociais},
  file = {/home/baldoinov/baldoinov/PDFs/Livros/Historia Geral da Africa IV_2010_Niane.pdf}
}

@mvbook{HistoriaGeralAfrica_2010_Ogot,
  title = {História Geral da África V: África do século XVI ao XVIII},
  editor = {Ogot, Bethwell Allan},
  date = {2010-08-09},
  series = {História Geral da África},
  volume = {5},
  publisher = {UNESCO Brasil},
  isbn = {978-85-7652-127-3},
  langid = {portuguese},
  volumes = {8},
  keywords = {Ciências sociais},
  file = {/home/baldoinov/baldoinov/PDFs/Livros/Historia Geral da Africa V_2010_Ogot.pdf}
}

@inproceedings{HowBuildValid_2019_Law,
  title = {How to {{Build Valid}} and {{Credible Simulation Models}}},
  booktitle = {2019 {{Winter Simulation Conference}} ({{WSC}})},
  author = {Law, Averill M.},
  date = {2019-12},
  pages = {1402--1414},
  issn = {1558-4305},
  doi = {10.1109/WSC40007.2019.9004789},
  url = {https://ieeexplore.ieee.org/document/9004789},
  urldate = {2024-06-23},
  abstract = {In this tutorial we present techniques for building valid and credible simulation models. Ideas to be discussed include the importance of a definitive problem formulation, discussions with subject-matter experts, interacting with the decision-maker on a regular basis, development of a written assumptions document, structured walk-through of the assumptions document, use of sensitivity analysis to determine important model factors, and comparison of model and system output data for an existing system (if any). Each idea will be illustrated by one or more real-world examples. We will also discuss the difficulty in using formal statistical techniques (e.g., confidence intervals) to validate simulation models.},
  eventtitle = {2019 {{Winter Simulation Conference}} ({{WSC}})},
  keywords = {Accreditation,Analytical models,Animation,Buildings,Computational modeling,Data models,Tutorials},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/How to Build Valid and Credible Simulation Models_2019_Law.pdf;/home/baldoinov/Zotero/storage/SCKEN4VD/9004789.html}
}

@online{HowCreateYour_2021_Nighania,
  title = {How to Create Your Own Deep Learning Rig: {{A}} Complete Hardware Guide.},
  shorttitle = {How to Create Your Own Deep Learning Rig},
  author = {Nighania, Kartik},
  date = {2021-01-22T20:00:16},
  url = {https://towardsdatascience.com/how-to-create-your-own-deep-learning-rig-a-complete-hardware-guide-2bba792b001b},
  urldate = {2023-10-09},
  abstract = {and how it’s faster and cheaper than cloud solutions in the long run.},
  langid = {english},
  organization = {Medium},
  keywords = {notion},
  file = {/home/baldoinov/Zotero/storage/F2MPPP2N/how-to-create-your-own-deep-learning-rig-a-complete-hardware-guide-2bba792b001b.html}
}

@article{HowDidEconomics_1997_Solow,
  title = {How {{Did Economics Get That Way}} and {{What Way Did It Get}}?},
  author = {Solow, Robert M.},
  date = {1997},
  journaltitle = {Daedalus},
  volume = {126},
  number = {1},
  eprint = {20027408},
  eprinttype = {jstor},
  pages = {39--58},
  publisher = {The MIT Press},
  issn = {0011-5266},
  url = {https://www.jstor.org/stable/20027408},
  urldate = {2023-12-31},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/How Did Economics Get That Way and What Way Did It Get_1997_Solow.pdf}
}

@online{HowDuolingoReignited_2024_Mazal,
  title = {How {{Duolingo}} Reignited User Growth},
  author = {Mazal, Jorge},
  date = {2024-02-20},
  url = {https://www.lennysnewsletter.com/p/how-duolingo-reignited-user-growth},
  urldate = {2024-07-17},
  abstract = {The story behind Duolingo's 350\% growth acceleration, leaderboards, streaks, notifications, and their innovative growth model},
  langid = {english},
  file = {/home/baldoinov/baldoinov/PDFs/Diversos/How Duolingo reignited user growth_2024_Mazal.pdf;/home/baldoinov/Zotero/storage/EKZP68XG/how-duolingo-reignited-user-growth.html}
}

@article{HowFindRead_2022_Gosztyla,
  title = {How to Find, Read and Organize Papers},
  author = {Gosztyla, Maya},
  date = {2022-07-07},
  journaltitle = {Nature},
  publisher = {Nature Publishing Group},
  doi = {10.1038/d41586-022-01878-7},
  url = {https://www.nature.com/articles/d41586-022-01878-7},
  urldate = {2023-10-27},
  abstract = {Maya Gosztyla decided to rethink her approach to research papers after she had trouble keeping track of the published literature.},
  langid = {english},
  keywords = {Careers,Lab life,notion,Research management},
  annotation = {Bandiera\_abtest: a\\
Cg\_type: Career Column\\
Subject\_term: Careers, Research management, Lab life}
}

@online{HowFineTune_2022_Beliasau,
  title = {How to Fine Tune {{VERY}} Large Model If It Doesn’t Fit on Your {{GPU}}},
  author = {Beliasau, Stanislau},
  date = {2022-10-03T12:17:13},
  url = {https://bestasoff.medium.com/how-to-fine-tune-very-large-model-if-it-doesnt-fit-on-your-gpu-3561e50859af},
  urldate = {2024-08-20},
  abstract = {Memory-efficient techniques to defeat the problem of “CUDA memory error..” during training},
  langid = {english},
  organization = {Medium},
  file = {/home/baldoinov/Zotero/storage/AABG9A23/how-to-fine-tune-very-large-model-if-it-doesnt-fit-on-your-gpu-3561e50859af.html}
}

@inproceedings{HowFineTuneBERT_2019_SunEtAl,
  title = {How to {{Fine-Tune BERT}} for {{Text Classification}}?},
  booktitle = {Chinese {{Computational Linguistics}}},
  author = {Sun, Chi and Qiu, Xipeng and Xu, Yige and Huang, Xuanjing},
  editor = {Sun, Maosong and Huang, Xuanjing and Ji, Heng and Liu, Zhiyuan and Liu, Yang},
  date = {2019},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {194--206},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-32381-3_16},
  abstract = {Language model pre-training has proven to be useful in learning universal language representations. As a state-of-the-art language model pre-training model, BERT (Bidirectional Encoder Representations from Transformers) has achieved amazing results in many language understanding tasks. In this paper, we conduct exhaustive experiments to investigate different fine-tuning methods of BERT on text classification task and provide a general solution for BERT fine-tuning. Finally, the proposed solution obtains new state-of-the-art results on eight widely-studied text classification datasets.},
  isbn = {978-3-030-32381-3},
  langid = {english},
  keywords = {BERT,notion,Text classification,Transfer learning},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/How to Fine-Tune BERT for Text Classification_2019_Sun et al.pdf}
}

@article{HowInterpretStatistical_2024_Arel-BundockEtAl,
  title = {How to {{Interpret Statistical Models Using}} Marginaleffects for {{R}} and {{Python}}},
  author = {Arel-Bundock, Vincent and Greifer, Noah and Heiss, Andrew},
  date = {2024-11-30},
  journaltitle = {Journal of Statistical Software},
  volume = {111},
  pages = {1--32},
  issn = {1548-7660},
  doi = {10.18637/jss.v111.i09},
  url = {https://doi.org/10.18637/jss.v111.i09},
  urldate = {2025-05-13},
  abstract = {The parameters of a statistical model can sometimes be difficult to interpret substantively, especially when that model includes nonlinear components, interactions, or transformations. Analysts who fit such complex models often seek to transform raw parameter estimates into quantities that are easier for domain experts and stakeholders to understand. This article presents a simple conceptual framework to describe a vast array of such quantities of interest, which are reported under imprecise and inconsistent terminology across disciplines: predictions, marginal predictions, marginal means, marginal effects, conditional effects, slopes, contrasts, risk ratios, etc. We introduce marginaleffects, a package for R and Python which offers a simple and powerful interface to compute all of those quantities, and to conduct (non-)linear hypothesis and equivalence tests on them. marginaleffects is lightweight; extensible; it works well in combination with other R and Python packages; and it supports over 100 classes of models, including linear, generalized linear, generalized additive, mixed effects, Bayesian, and several machine learning models.},
  langid = {english},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/How to Interpret Statistical Models Using marginaleffects for R and Python_2024_Arel-Bundock et al.pdf}
}

@online{HowPlayMultiple__,
  title = {How Do {{I}} Play Multiple Guitar Songs on One Guitar?},
  url = {https://www.quora.com/How-do-I-play-multiple-guitar-songs-on-one-guitar},
  urldate = {2024-05-29},
  abstract = {Answer (1 of 5): Dire Straits is the perfect example for this question. Seriously. You couldn't have chosen a better one. Dire Straits frontman Mark Knopfler is one of the best, most unique guitarists in rock history, and his sound is huuuuuuugely difficult to copy. Why? His musical background. ...},
  langid = {english},
  organization = {Quora},
  file = {/home/baldoinov/Zotero/storage/RD4BDRE3/How-do-I-play-multiple-guitar-songs-on-one-guitar.html}
}

@online{HowPythonAte_2014_izaromanowska,
  title = {How the {{Python Ate}} the {{Turtle}}},
  author = {{izaromanowska}},
  date = {2014-05-26T14:20:36+00:00},
  url = {https://simulatingcomplexity.wordpress.com/2014/05/26/how-the-python-eat-the-turtle/},
  urldate = {2024-06-10},
  abstract = {In the first blogpost dealing with the modelling ~tools of trade Ben Davies argued in favour~of NetLogo. I join the debate to argue for the simplest of commonly used programming languages – P…},
  langid = {english},
  organization = {simulatingcomplexity},
  file = {/home/baldoinov/Zotero/storage/XC28SVX6/how-the-python-eat-the-turtle.html}
}

@book{HowResearch_2008_BlaxterEtAl,
  title = {How to Research},
  author = {Blaxter, Loraine and Hughes, Christina and Tight, Malcolm},
  date = {2008},
  edition = {3},
  publisher = {Open Univ. Press},
  location = {Maidenhead},
  isbn = {978-0-335-21746-5},
  langid = {english},
  pagetotal = {287}
}

@online{HowWorkLanguage__Morales,
  title = {How {{To Work}} with {{Language Data}} in {{Python}} 3 Using the {{Natural Language Toolkit}} ({{NLTK}}) | {{DigitalOcean}}},
  author = {Morales, Michelle},
  url = {https://www.digitalocean.com/community/tutorials/how-to-work-with-language-data-in-python-3-using-the-natural-language-toolkit-nltk},
  urldate = {2023-09-21},
  abstract = {This tutorial will provide an introduction to using the Natural Language Toolkit (NLTK): a Natural Language Processing tool for Python. NLP is a field of com…},
  langid = {english},
  keywords = {notion},
  file = {/home/baldoinov/Zotero/storage/ZL86H3PN/how-to-work-with-language-data-in-python-3-using-the-natural-language-toolkit-nltk.html}
}

@article{HumanlevelControlDeep_2015_MnihEtAl,
  title = {Human-Level Control through Deep Reinforcement Learning},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  date = {2015-02},
  journaltitle = {Nature},
  volume = {518},
  number = {7540},
  pages = {529--533},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature14236},
  url = {https://www.nature.com/articles/nature14236},
  urldate = {2024-06-25},
  abstract = {An artificial agent is developed that learns to play~a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a~performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.},
  langid = {english},
  keywords = {Computer science},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Human-level control through deep reinforcement learning_2015_Mnih et al.pdf}
}

@incollection{IdeaAgentBasedModeling_2019_Gilbert,
  title = {The Idea of Agent-Based Modeling},
  booktitle = {Agent-Based Models},
  author = {Gilbert, Nigel},
  date = {2019-12-24},
  edition = {2nd ed. edição},
  publisher = {Sage Publications, Inc},
  location = {Thousand Oaks, California},
  abstract = {Agent-based simulation has become increasingly popular as a modeling approach in the social sciences because it enables researchers to build models where individual entities and their interactions are directly represented. The Second Edition of Nigel Gilbert′s Agent-Based Models introduces this technique; considers a range of methodological and theoretical issues; shows how to design an agent-based model, with a simple example; offers some practical advice about developing, verifying and validating agent-based models; and finally discusses how to plan an agent-based modelling project, publish the results and apply agent-based modeling to formulate and evaluate social and economic policies. A website to accompany the book includes an annotated exemplar model using NetLogo.},
  isbn = {978-1-5063-5560-3},
  langid = {Inglês},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/The Idea of Agent-Based Modeling_2019_Gilbert.pdf}
}

@article{IfMultiagentLearning_2007_ShohamEtAl,
  title = {If Multi-Agent Learning Is the Answer, What Is the Question?},
  author = {Shoham, Yoav and Powers, Rob and Grenager, Trond},
  date = {2007-05},
  journaltitle = {Artificial Intelligence},
  shortjournal = {Artificial Intelligence},
  volume = {171},
  number = {7},
  pages = {365--377},
  issn = {00043702},
  doi = {10.1016/j.artint.2006.02.006},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0004370207000495},
  urldate = {2024-05-29},
  abstract = {The area of learning in multi-agent systems is today one of the most fertile grounds for interaction between game theory and artificial intelligence. We focus on the foundational questions in this interdisciplinary area, and identify several distinct agendas that ought to, we argue, be separated. The goal of this article is to start a discussion in the research community that will result in firmer foundations for the area.1 © 2007 Published by Elsevier B.V.},
  langid = {english},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/If multi-agent learning is the answer, what is the question_2007_Shoham et al.pdf}
}

@online{IllustratedTransformer__Alammar,
  title = {The {{Illustrated Transformer}}},
  author = {Alammar, Jay},
  url = {http://jalammar.github.io/illustrated-transformer/},
  urldate = {2023-09-21},
  abstract = {In the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.},
  keywords = {notion},
  file = {/home/baldoinov/Zotero/storage/4TCF54FT/illustrated-transformer.html}
}

@book{IluminacaoNoDesign_2014_InnesSalvaterra,
  title = {Iluminação no design de interiores},
  author = {Innes, Malcolm and Salvaterra, Alexandre},
  date = {2014-06-15},
  edition = {1ª edição},
  publisher = {Editora Gustavo Gili},
  abstract = {A luz é uma das principais ferramentas para o designer de interiores e pode transformar a maneira pela qual um espaço é percebido. Com mais de 300 ilustrações e um texto direto e claro, este livro tem uma abordagem detalhada e prática sobre iluminação em design de interiores, dando aos estudantes todos os conhecimentos e informações essenciais para que possam ter sucesso em seus projetos. -Cobre tanto os princípios de iluminação técnicos como os de projeto; -Diagramas e sequências de fotografias especialmente criados para o livro explicam a física da luz; -Estudos de caso de obras de especialistas em luminotécnica mostram o que deve ser iluminado e de que modo; -Plantas e desenhos detalhados demonstram como representar esquemas de iluminação por meio do CAD e das maquetes eletrônicas.},
  isbn = {978-85-65985-37-6},
  langid = {portuguese}
}

@online{ImpactIlliteracyImportance_2021_Miranda,
  title = {The {{Impact}} of {{Illiteracy}} and the {{Importance}} of {{Early}} ...},
  author = {Miranda, Nicola},
  date = {2021-07-23T12:27:47+00:00},
  url = {https://worldliteracyfoundation.org/early-intervention-reduces-illiteracy/},
  urldate = {2024-09-11},
  abstract = {There are significant economic, social and health impact associated with illiteracy. Early intervention approach is key to reduce illiteracy...},
  langid = {american},
  file = {/home/baldoinov/Zotero/storage/CYVJ937Y/early-intervention-reduces-illiteracy.html}
}

@incollection{ImpactMachineLearning_2018_Athey,
  title = {The {{Impact}} of {{Machine Learning}} on {{Economics}}},
  booktitle = {The {{Economics}} of {{Artificial Intelligence}}: {{An Agenda}}},
  author = {Athey, Susan},
  date = {2018-01},
  pages = {507--547},
  publisher = {University of Chicago Press},
  url = {https://www.nber.org/books-and-chapters/economics-artificial-intelligence-agenda/impact-machine-learning-economics},
  urldate = {2023-04-13},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/The Impact of Machine Learning on Economics_2018_Athey.pdf}
}

@article{ImpactoUnidadesPronto_2016_RochaFernandes,
  title = {O Impacto das Unidades de Pronto Atendimento (UPAs) 24h sobre indicadores de mortalidade: evidências para o Rio de Janeiro},
  shorttitle = {O Impacto das Unidades de Pronto Atendimento (UPAs) 24h sobre indicadores de mortalidade},
  author = {Rocha, Rudi and Fernandes, Lucas Merenfeld da Silva},
  date = {2016-12},
  journaltitle = {http://ppe.ipea.gov.br},
  shortjournal = {The Impact of emergency care units on mortality : evidence from Rio de Janeiro},
  publisher = {Instituto de Pesquisa Econômica Aplicada (Ipea)},
  url = {https://repositorio.ipea.gov.br/handle/11058/7503},
  urldate = {2024-07-02},
  abstract = {Este artigo avalia o impacto das Unidades de Pronto Atendimento (UPAs) 24h sobre as taxas de mortalidade nos municípios do estado do Rio de Janeiro entre 2000 e 2011. Para tanto, estimou-se um modelo de dados em painel ao nível do município-mês, no qual se identificou o efeito das UPAs sobre as taxas de mortalidade por município de residência, local de ocorrência e causa do óbito. Observou-se que as UPAs têm um efeito negativo, porém não significativo sobre a taxa geral de mortalidade. Ao caracterizar este efeito com mais detalhes, por local de ocorrência e causa do óbito, observou-se uma redução significativa dos óbitos em hospitais (-16\%) e na rua (-27\%), mas um aumento de óbitos ocorridos em outros estabelecimentos de saúde (em que as UPAs estão classificadas). Isso sugere realocação parcial de óbitos entre locais de ocorrência. Ao examinar efeitos sobre a mortalidade em hospitais, observou-se um efeito negativo sobre óbitos por doenças circulatórias e endócrinas, bem como por causas externas.},
  langid = {brazilian},
  annotation = {Accepted: 2017-03-03T20:04:58Z},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/econometria-ii/O Impacto das Unidades de Pronto Atendimento (UPAs) 24h sobre indicadores de_2016_Rocha et al.pdf}
}

@article{ImpactoUnidadesPronto_2020_SilvaEtAl,
  title = {Impacto das Unidades de Pronto Atendimento 24h sobre indicadores de morbimortalidade: uma análise com dados em painel para o estado do Rio Grande do Norte e região metropolitana de Natal no período 2010-2016},
  shorttitle = {Impacto das Unidades de Pronto Atendimento 24h sobre indicadores de morbimortalidade},
  author = {family=Silva, given=Mavigson Francisco, prefix=da, useprefix=false and Santos, Joelson Oliveira and Alves, Janaina da Silva},
  date = {2020-08-31},
  journaltitle = {Revista Meta: Avaliação},
  number = {36},
  issn = {2175-2753},
  doi = {10.22347/2175-2753v12i36.2517},
  url = {https://revistas.cesgranrio.org.br/index.php/metaavaliacao/article/view/2517},
  abstract = {O modelo de assistência à saúde em todo o mundo tem enfrentado desafios, sendo um deles a superlotação nos serviços de urgências e emergências em hospitais e seu impacto nos indicadores de saúde. No Brasil, o processo de reestruturação do sistema de atenção às urgências se deu através da introdução das Unidades de Pronto-Atendimento (UPA). Dessa forma, esse trabalho tem por objetivo avaliar o impacto das UPA 24h sobre as taxas de mortalidade nos municípios do estado do Rio Grande do Norte e na Região Metropolitana de Natal durante o período 2010-2016. Para tanto, seguindo a metodologia de Rocha e Fernandes, estimou-se um modelo de dados em painel com efeitos fixos, ao âmbito do município-ano, por local de ocorrência e causa do óbito. De acordo com os resultados obtidos no estudo, observou-se que a UPA tem um impacto negativo, porém não significativo sobre a taxa geral de mortalidade norte-rio-grandense, resultado esse que corrobora com a literatura mais recente. Ao verificar esse impacto com mais detalhes, por local de ocorrência e causa do óbito, observou-se redução significativa da taxa de mortalidade em alguns cenários, no entanto houve aumento de óbitos ocorridos em outros estabelecimentos de saúde (em que as UPA estão classificadas). Esse resultado sugere a realocação parcial dos óbitos por local de ocorrência e que, portanto, essas unidades estariam cumprindo o papel de hospitais em vez de funcionar como UPA em um sistema de atendimento integrado.},
  langid = {portuguese},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/econometria-ii/Impacto das Unidades de Pronto Atendimento 24h sobre indicadores de_2020_Silva et al.pdf}
}

@online{ImplementationSCD2Slowly_2022_Kalgal,
  title = {Implementation of {{SCD-2}} ({{Slowly Changing Dimension}}) with {{Apache Hudi}}},
  author = {Kalgal, Jayasheel},
  date = {2022-08-29T05:04:59},
  url = {https://medium.com/walmartglobaltech/implementation-of-scd-2-slowly-changing-dimension-with-apache-hudi-465e0eb94a5},
  urldate = {2025-03-09},
  abstract = {Authored \& Contributed by~: Jayasheel Kalgal, Esha Dhing, Prashant Mishra},
  langid = {english},
  organization = {Walmart Global Tech Blog},
  file = {/home/baldoinov/Zotero/storage/KV8YN7NJ/implementation-of-scd-2-slowly-changing-dimension-with-apache-hudi-465e0eb94a5.html}
}

@inproceedings{ImprovingImplicitStance_2019_SchaeferStede,
  title = {Improving {{Implicit Stance Classification}} in {{Tweets Using Word}} and {{Sentence Embeddings}}},
  booktitle = {{{KI}} 2019: {{Advances}} in {{Artificial Intelligence}}},
  author = {Schaefer, Robin and Stede, Manfred},
  editor = {Benzmüller, Christoph and Stuckenschmidt, Heiner},
  date = {2019},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {299--307},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-30179-8_26},
  abstract = {Argumentation Mining aims at finding components of arguments, as well as relations between them, in text. One of the largely unsolved problems is implicitness, where the text invites the reader to infer a missing component, such as the claim or a supporting statement. In the work of Wojatzki and Zesch (2016), an interesting implicitness problem is addressed on a Twitter data set. They showed that implicit stances toward a claim can be found with some success using just token and character n-grams. Using the same dataset, we show that results for this task can be improved using word and sentence embeddings, but that not all embedding variants perform alike. Specifically, we compare fastText, GloVe, and Universal Sentence Encoder (USE); and we find that, to our knowledge, USE yields state-of-the-art results for this task.},
  isbn = {978-3-030-30179-8},
  langid = {english},
  keywords = {Argumentation mining,notion,Social media,Stance classification},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/projeto-heiwa-revisao-de-literatura/Improving Implicit Stance Classification in Tweets Using Word and Sentence_2019_Schaefer et al.pdf}
}

@article{ImprovingLanguageUnderstanding__RadfordEtAl,
  title = {Improving {{Language Understanding}} by {{Generative Pre-Training}}},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
  langid = {english},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Improving Language Understanding by Generative Pre-Training_Radford et al.pdf}
}

@incollection{Index_2021_Kissell,
  title = {Index},
  booktitle = {Algorithmic {{Trading Methods}} ({{Second Edition}})},
  editor = {Kissell, Robert L.},
  date = {2021-01-01},
  pages = {577--588},
  publisher = {Academic Press},
  doi = {10.1016/B978-0-12-815630-8.20001-X},
  url = {https://www.sciencedirect.com/science/article/pii/B978012815630820001X},
  urldate = {2025-04-21},
  isbn = {978-0-12-815630-8}
}

@article{InductiveReasoningBounded_1994_Arthur,
  title = {Inductive {{Reasoning}} and {{Bounded Rationality}}},
  author = {Arthur, W. Brian},
  date = {1994},
  journaltitle = {The American Economic Review},
  volume = {84},
  number = {2},
  eprint = {2117868},
  eprinttype = {jstor},
  pages = {406--411},
  publisher = {American Economic Association},
  issn = {0002-8282},
  url = {https://www.jstor.org/stable/2117868},
  urldate = {2023-09-24},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Inductive Reasoning and Bounded Rationality_1994_Arthur.pdf}
}

@book{IndustrialOrganizationStrategic_2000_ChurchWare,
  title = {Industrial {{Organization}}: A Strategic Approach},
  shorttitle = {Industrial Organization},
  author = {Church, Jeffrey R. and Ware, Roger},
  date = {2000},
  publisher = {Irwin McGraw Hill},
  location = {New York},
  isbn = {978-0-256-20571-8},
  langid = {english},
  pagetotal = {926},
  file = {/home/baldoinov/baldoinov/PDFs/Livros/Industrial Organization_2000_Church et al.pdf}
}

@book{InformationTheoryInference_2019_MacKay,
  title = {Information {{Theory}}, {{Inference}}, and {{Learning Algorithms}}},
  author = {MacKay, David J. C.},
  date = {2019},
  edition = {22nd printing},
  publisher = {Cambridge University Press},
  location = {Cambridge},
  isbn = {978-0-521-64298-9},
  langid = {english},
  pagetotal = {628},
  file = {/home/baldoinov/baldoinov/PDFs/Livros/Information Theory, Inference, and Learning Algorithms_2019_MacKay.pdf}
}

@online{InnateValuesdrivenReinforcementLearning_2024_Yang,
  title = {Innate-{{Values-driven Reinforcement Learning}} for {{Cooperative Multi-Agent Systems}}},
  author = {Yang, Qin},
  date = {2024-01-10},
  eprint = {2401.05572},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2401.05572},
  url = {http://arxiv.org/abs/2401.05572},
  urldate = {2024-01-31},
  abstract = {Innate values describe agents' intrinsic motivations, which reflect their inherent interests and preferences to pursue goals and drive them to develop diverse skills satisfying their various needs. The essence of reinforcement learning (RL) is learning from interaction based on reward-driven (such as utilities) behaviors, much like natural agents. It is an excellent model to describe the innate-values-driven (IV) behaviors of AI agents. Especially in multi-agent systems (MAS), building the awareness of AI agents to balance the group utilities and system costs and satisfy group members' needs in their cooperation is a crucial problem for individuals learning to support their community and integrate human society in the long term. This paper proposes a hierarchical compound intrinsic value reinforcement learning model -- innate-values-driven reinforcement learning termed IVRL to describe the complex behaviors of multi-agent interaction in their cooperation. We implement the IVRL architecture in the StarCraft Multi-Agent Challenge (SMAC) environment and compare the cooperative performance within three characteristics of innate value agents (Coward, Neutral, and Reckless) through three benchmark multi-agent RL algorithms: QMIX, IQL, and QTRAN. The results demonstrate that by organizing individual various needs rationally, the group can achieve better performance with lower costs effectively.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems,Computer Science - Robotics,notion},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Innate-Values-driven Reinforcement Learning for Cooperative Multi-Agent Systems_2024_Yang.pdf;/home/baldoinov/Zotero/storage/DBH9IU4X/2401.html}
}

@article{InovacaoTecnologicaNo_2021_LealFigueiredo,
  title = {Inovação tecnológica no Brasil: desafios e insumos para políticas públicas},
  shorttitle = {Inovação tecnológica no Brasil},
  author = {Leal, Carlos Ivan Simonsen and Figueiredo, Paulo N.},
  date = {2021-07-09},
  journaltitle = {Revista de Administração Pública},
  shortjournal = {Rev. Adm. Pública},
  volume = {55},
  pages = {512--537},
  publisher = {Fundação Getulio Vargas},
  issn = {0034-7612, 1982-3134},
  doi = {10.1590/0034-761220200583},
  url = {https://www.scielo.br/j/rap/a/th4kPMNYksKFkZDwSdWs7Zj/?lang=pt},
  urldate = {2024-03-29},
  abstract = {Resumo Este artigo oferece uma breve reflexão sobre a natureza do investimento em pesquisa e desenvolvimento (P\&D) no Brasil. Seu objetivo é proporcionar alguns insumos para avançar no debate sobre esse tema na sociedade brasileira. Desde 1999, o Brasil tem aumentado de maneira consistente o seu investimento em P\&D, considerado um dos insumos para inovação e produtividade. Porém, tal esforço tem gerado resultados limitados. Esses resultados limitados não parecem refletir mera insuficiência de investimentos em inovação no Brasil, mas a maneira e a eficácia de sua implementação.},
  langid = {portuguese},
  keywords = {Brasil,capacidade tecnológica,crescimento econômico,desenvolvimento tecnológico,inovação,políticas públicas},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Inovacao tecnologica no Brasil_2021_Leal et al.pdf}
}

@book{InputOutputAnalysisFoundations_2009_MillerBlair,
  title = {Input-{{Output Analysis}}: {{Foundations}} and {{Extensions}}},
  shorttitle = {Input-{{Output Analysis}}},
  author = {Miller, Ronald E. and Blair, Peter D.},
  date = {2009},
  edition = {2},
  publisher = {Cambridge University Press},
  location = {Cambridge},
  doi = {10.1017/CBO9780511626982},
  url = {https://www.cambridge.org/core/books/inputoutput-analysis/69827DA658E766CD1E17B1A47BA2B9C3},
  urldate = {2023-10-31},
  abstract = {This edition of Ronald Miller and Peter Blair's classic textbook is an essential reference for students and scholars in the input-output research and applications community. The book has been fully revised and updated to reflect important developments in the field since its original publication. New topics covered include SAMs (and extended input-output models) and their connection to input-output data, structural decomposition analysis (SDA), multiplier decompositions, identifying important coefficients, and international input-output models. A major new feature of this edition is that it is also supported by an accompanying website with solutions to all problems, wide-ranging real-world data sets, and appendices with further information for more advanced readers. Input-Output Analysis is an ideal introduction to the subject for advanced undergraduate and graduate students in a wide variety of fields, including economics, regional science, regional economics, city, regional and urban planning, environmental planning, public policy analysis and public management.},
  keywords = {notion}
}

@inproceedings{InputOutputMatrixesAgentBased_2010_AndradeEtAl,
  title = {From {{Input-Output Matrixes}} to {{Agent-Based Models}}: {{A Case Study}} on {{Carbon Credits}} in a {{Local Economy}}},
  shorttitle = {From {{Input-Output Matrixes}} to {{Agent-Based Models}}},
  booktitle = {2010 {{Second Brazilian Workshop}} on {{Social Simulation}}},
  author = {family=Andrade, given=Pedro Ribeiro, prefix=de, useprefix=false and Monteiro, Antonio Miguel Vieira and Camara, Gilberto},
  date = {2010-10},
  pages = {58--65},
  doi = {10.1109/BWSS.2010.16},
  abstract = {Analytical macroeconomic scenarios are currently the most common approach to assist in the development and evaluation of economic policies. Reproducing and evaluating the results found by analytic models is one major hurdle to be overcome by social simulation on its early development in any specific knowledge area. This work describes an initial step toward moving from algebraic input-output economic models to agent-based models, in order to get more flexibility, adding decision capabilities to the agents and exploring more complex scenarios. We study economic scenarios of carbon credits for reducing deforestation in a region of Par´a state, in the Brazilian Amazonia. The objective is to investigate the underlying assumptions of the analytic model through an agent-based approach, deriving new challenges to be tackled by future agent-based models.},
  eventtitle = {2010 {{Second Brazilian Workshop}} on {{Social Simulation}}},
  keywords = {Analytical models,Biological system modeling,Carbon,Economics,Finance,Industries,notion,Remuneration},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/From Input-Output Matrixes to Agent-Based Models_2010_Andrade et al.pdf;/home/baldoinov/Zotero/storage/VIJ2PTG5/6030015.html}
}

@online{InstantMeshEfficient3D_2024_XuEtAl,
  title = {{{InstantMesh}}: {{Efficient 3D Mesh Generation}} from a {{Single Image}} with {{Sparse-view Large Reconstruction Models}}},
  shorttitle = {{{InstantMesh}}},
  author = {Xu, Jiale and Cheng, Weihao and Gao, Yiming and Wang, Xintao and Gao, Shenghua and Shan, Ying},
  date = {2024-04-14},
  eprint = {2404.07191},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2404.07191},
  url = {http://arxiv.org/abs/2404.07191},
  urldate = {2024-04-17},
  abstract = {We present InstantMesh, a feed-forward framework for instant 3D mesh generation from a single image, featuring state-of-the-art generation quality and significant training scalability. By synergizing the strengths of an off-the-shelf multiview diffusion model and a sparse-view reconstruction model based on the LRM architecture, InstantMesh is able to create diverse 3D assets within 10 seconds. To enhance the training efficiency and exploit more geometric supervisions, e.g, depths and normals, we integrate a differentiable iso-surface extraction module into our framework and directly optimize on the mesh representation. Experimental results on public datasets demonstrate that InstantMesh significantly outperforms other latest image-to-3D baselines, both qualitatively and quantitatively. We release all the code, weights, and demo of InstantMesh, with the intention that it can make substantial contributions to the community of 3D generative AI and empower both researchers and content creators.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/InstantMesh_2024_Xu et al.pdf;/home/baldoinov/Zotero/storage/X8SN5JLR/2404.html}
}

@online{InstitutionsFundamentalCause_2004_AcemogluEtAl,
  type = {Working Paper},
  title = {Institutions as the {{Fundamental Cause}} of {{Long-Run Growth}}},
  author = {Acemoglu, Daron and Johnson, Simon and Robinson, James},
  date = {2004-05},
  series = {Working {{Paper Series}}},
  number = {10481},
  eprint = {10481},
  eprinttype = {National Bureau of Economic Research},
  doi = {10.3386/w10481},
  url = {https://www.nber.org/papers/w10481},
  urldate = {2024-04-11},
  abstract = {This paper develops the empirical and theoretical case that differences in economic institutions are the fundamental cause of differences in economic development. We first document the empirical importance of institutions by focusing on two 'quasi-natural experiments' in history, the division of Korea into two parts with very different economic institutions and the colonization of much of the world by European powers starting in the fifteenth century. We then develop the basic outline of a framework for thinking about why economic institutions differ across countries. Economic institutions determine the incentives of and the constraints on economic actors, and shape economic outcomes. As such, they are social decisions, chosen for their consequences. Because different groups and individuals typically benefit from different economic institutions, there is generally a conflict over these social choices, ultimately resolved in favor of groups with greater political power. The distribution of political power in society is in turn determined by political institutions and the distribution of resources. Political institutions allocate de jure political power, while groups with greater economic might typically possess greater de facto political power. We therefore view the appropriate theoretical framework as a dynamic one with political institutions and the distribution of resources as the state variables. These variables themselves change over time because prevailing economic institutions affect the distribution of resources, and because groups with de facto political power today strive to change political institutions in order to increase their de jure political power in the future. Economic institutions encouraging economic growth emerge when political institutions allocate power to groups with interests in broad-based property rights enforcement, when they create effective constraints on power-holders, and when there are relatively few rents to be captured by power-holders. We illustrate the assumptions, the workings and the implications of this framework using a number of historical examples.},
  pubstate = {prepublished},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/economia-brasileira-ii/Institutions as the Fundamental Cause of Long-Run Growth_2004_Acemoglu et al.pdf}
}

@inproceedings{InstructionReinforcementLearning_2010_WatanabeSawa,
  title = {Instruction for Reinforcement Learning Agent Based on Sub-Rewards and Forgetting},
  booktitle = {International {{Conference}} on {{Fuzzy Systems}}},
  author = {Watanabe, Toshihiko and Sawa, Toru},
  date = {2010-07},
  pages = {1--7},
  issn = {1098-7584},
  doi = {10.1109/FUZZY.2010.5584788},
  abstract = {In order to realize intelligent agent such as autonomous mobile robots, Reinforcement Learning is one of the necessary techniques in control system. It is desirable in terms of knowledge or skill acquisition of agent that reinforcement learning is based only upon rewards concept instead of teaching signal. However, there exist many problems to apply reinforcement learning to actual problem. The most severe problem is huge iterations in learning process. On the other hand, several methods such as intrinsically motivated reinforcement learning have been studied. The methods are based on internal rewards to formulate behavioral rules abstracted from the results of reinforcement learning expressed as action rules. They are promising techniques for task decomposition of complicated task of agent. In the abstraction process, segmentation of learning is an indispensable and essential technique. Our motivation is to utilize appropriately instructions that we can give to the reinforcement learning agent along with main rewards in order to haste the learning process and to attain valid learning performance for preparation of segmentation. In this study, we propose instruction approach for reinforcement learning agent based on sub-reward and forgetting mechanism. Through numerical experiments of grid world task and mountain car task, we show validness of the proposed approach in terms of learning speed and accuracy.},
  eventtitle = {International {{Conference}} on {{Fuzzy Systems}}},
  keywords = {Artificial neural networks,Classification algorithms,Education,Gravity,Learning,notion,Planning},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Instruction for reinforcement learning agent based on sub-rewards and forgetting_2010_Watanabe et al.pdf;/home/baldoinov/Zotero/storage/R2VGF55T/5584788.html}
}

@book{IntermediateMicroeconomicsCalculus_2014_Varian,
  title = {Intermediate {{Microeconomics}}: With {{Calculus}}},
  shorttitle = {Intermediate Microeconomics},
  author = {Varian, Hal R.},
  date = {2014},
  edition = {First edition},
  publisher = {W.W. Norton \& Company},
  location = {New York},
  isbn = {978-0-393-12398-2 978-0-393-92394-0},
  langid = {english},
  pagetotal = {761},
  keywords = {Mathematical models,Microeconomics,Textbooks},
  annotation = {OCLC: ocn884922812}
}

@book{IntermediateStatisticsDummies_2007_Rumsey,
  title = {Intermediate {{Statistics}} for {{Dummies}}},
  author = {Rumsey, Deborah},
  date = {2007},
  series = {For Dummies: {{Bestselling}} Book Series for Beginners},
  publisher = {Wiley},
  location = {Hoboken, NJ},
  isbn = {978-0-470-04520-6},
  langid = {english},
  pagetotal = {362}
}

@thesis{IntermittentDemandForecasting_2021_SarloAntonioFilho,
  type = {MESTRE EM ENGENHARIA ELÉTRICA},
  title = {Intermittent Demand Forecasting in Retail: Applications of the GAS Framework},
  shorttitle = {INTERMITTENT DEMAND FORECASTING IN RETAIL},
  author = {Sarlo Antonio Filho, Rodrigo},
  date = {2021-06-10},
  institution = {PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO},
  location = {Rio de Janeiro, Brazil},
  doi = {10.17771/PUCRio.acad.55086},
  url = {http://www.maxwell.vrac.puc-rio.br/Busca_etds.php?strSecao=resultado&nrSeq=55086@2},
  urldate = {2024-10-21},
  abstract = {Antonio Filho, Rodrigo; Fernandes, Cristiano (Advisor). Intermittent demand forecasting in retail: applications of the GAS framework. Rio de Janeiro, 2021. 86p. Dissertação de Mestrado – Departamento de Engenharia Elétrica, Pontifícia Universidade Católica do Rio de Janeiro.},
  langid = {portuguese},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Recovery/Modelo de Incidencia de Acao Contra/Intermittent Demand Forecasting in Retail_2021_Sarlo Antonio Filho.pdf}
}

@online{IntermittentTimeSeries_2022_R,
  title = {Intermittent {{Time Series Forecasting}}},
  author = {R, Karthikeswaren},
  date = {2022-04-11T17:02:39},
  url = {https://medium.com/codex/intermittent-time-series-forecasting-be530f819880},
  urldate = {2024-10-18},
  abstract = {We surveyed various methods that can be used to forecast intermittent time series. Our work got published and accepted at the International…},
  langid = {english},
  organization = {CodeX},
  file = {/home/baldoinov/Zotero/storage/QKNT2SQI/intermittent-time-series-forecasting-be530f819880.html}
}

@incollection{InterpretableMachineLearning_2020_MolnarEtAl,
  title = {Interpretable {{Machine Learning}} -- {{A Brief History}}, {{State-of-the-Art}} and {{Challenges}}},
  author = {Molnar, Christoph and Casalicchio, Giuseppe and Bischl, Bernd},
  date = {2020},
  volume = {1323},
  eprint = {2010.09337},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  pages = {417--431},
  doi = {10.1007/978-3-030-65965-3_28},
  url = {http://arxiv.org/abs/2010.09337},
  urldate = {2024-02-21},
  abstract = {We present a brief history of the field of interpretable machine learning (IML), give an overview of state-of-the-art interpretation methods, and discuss challenges. Research in IML has boomed in recent years. As young as the field is, it has over 200 years old roots in regression modeling and rule-based machine learning, starting in the 1960s. Recently, many new IML methods have been proposed, many of them model-agnostic, but also interpretation techniques specific to deep learning and tree-based ensembles. IML methods either directly analyze model components, study sensitivity to input perturbations, or analyze local or global surrogate approximations of the ML model. The field approaches a state of readiness and stability, with many methods not only proposed in research, but also implemented in open-source software. But many important challenges remain for IML, such as dealing with dependent features, causal interpretation, and uncertainty estimation, which need to be resolved for its successful application to scientific problems. A further challenge is a missing rigorous definition of interpretability, which is accepted by the community. To address the challenges and advance the field, we urge to recall our roots of interpretable, data-driven modeling in statistics and (rule-based) ML, but also to consider other areas such as sensitivity analysis, causal inference, and the social sciences.},
  keywords = {Computer Science - Machine Learning,notion,Statistics - Machine Learning},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Interpretable Machine Learning -- A Brief History, State-of-the-Art and_2020_Molnar et al.pdf;/home/baldoinov/Zotero/storage/ZRX2XZVN/2010.html}
}

@book{IntroducaoAcessibilidadeUrbana_2023_Herszenhut,
  title = {Introdução à acessibilidade urbana: um guia prático em R},
  shorttitle = {Introdução à acessibilidade urbana},
  author = {Herszenhut, Daniel},
  namea = {Pereira, Rafael H. M.},
  nameatype = {collaborator},
  date = {2023-08-02},
  publisher = {Ipea},
  location = {Rio de Janeiro, RJ},
  isbn = {9786556350547},
  langid = {portuguese},
  keywords = {acessibilidade,Economia,GTFS,Ipea,Politicas,Públicas,público,Transporte,Urbana},
  file = {/home/baldoinov/baldoinov/PDFs/Livros/Introducao a acessibilidade urbana_2023_Herszenhut.pdf}
}

@book{IntroducaoAnaliseRedes_2017_Recuero,
  title = {Introdução à análise de redes sociais online},
  author = {Recuero, Raquel},
  date = {2017},
  edition = {1},
  publisher = {Edufba},
  url = {https://edufba.ufba.br/livros-publicados/catalogo/introducao-analise-de-redes-sociais-online},
  urldate = {2023-12-06},
  isbn = {978-85-232-1669-6},
  langid = {portuguese},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Livros/Introducao a analise de redes sociais online_2017_Recuero.pdf}
}

@book{IntroducaoEconomia_2021_Mankiw,
  title = {Introdução à Economia},
  shorttitle = {Introdução à economia},
  author = {Mankiw, Gregory},
  date = {2021-07-31},
  publisher = {Cengage Learning},
  isbn = {978-85-221-1273-9},
  langid = {brazilian}
}

@book{IntroducaoSistemasBancos_2017_Date,
  title = {Introdução a Sistemas de Bancos de Dados},
  author = {Date, C. J.},
  date = {2017-07-26},
  publisher = {Elsevier},
  isbn = {978-85-352-8445-4},
  langid = {portuguese},
  keywords = {Administração e serviços auxiliares}
}

@online{IntroducingMetaLlama_2024_MetaAI,
  title = {Introducing {{Meta Llama}} 3: {{The}} Most Capable Openly Available {{LLM}} to Date},
  shorttitle = {Introducing {{Meta Llama}} 3},
  author = {{Meta AI}},
  date = {2024-04-18},
  url = {https://ai.meta.com/blog/meta-llama-3/},
  urldate = {2024-08-20},
  abstract = {Today, we’re introducing Meta Llama 3, the next generation of our state-of-the-art open source large language model. In the coming months, we expect to share new capabilities, additional model sizes, and more.},
  langid = {english},
  organization = {Meta AI}
}

@book{IntroducingMLOpsHow_2021_TreveilEtAl,
  title = {Introducing MLOps: How to Scale Machine Learning in the Enterprise},
  shorttitle = {Introducing MLOps},
  author = {Treveil, Mark and Omont, Nicolas and Stenac, Clément and LeFevre, Kenji and Phan, Du and Zentici, Joachim and Lavoillotte, Adrien and Miyazaki, Makoto and Heidmann, Lynn},
  date = {2021-01-05},
  edition = {1ª edição},
  publisher = {O'Reilly Media},
  location = {Beijing ; Boston},
  abstract = {More than half of the analytics and machine learning (ML) models created by organizations today never make it into production. Some of the challenges and barriers to operationalization are technical, but others are organizational. Either way, the bottom line is that models not in production can't provide business impact.  This book introduces the key concepts of MLOps to help data scientists and application engineers not only operationalize ML models to drive real business change but also maintain and improve those models over time. Through lessons based on numerous MLOps applications around the world, nine experts in machine learning provide insights into the five steps of the model life cycle--Build, Preproduction, Deployment, Monitoring, and Governance--uncovering how robust MLOps processes can be infused throughout.  This book helps you: Fulfill data science value by reducing friction throughout ML pipelines and workflows Refine ML models through retraining, periodic tuning, and complete remodeling to ensure long-term accuracy Design the MLOps life cycle to minimize organizational risks with models that are unbiased, fair, and explainable Operationalize ML models for pipeline deployment and for external business systems that are more complex and less standardized},
  isbn = {978-1-4920-8329-0},
  langid = {Inglês}
}

@book{IntroductionAgentBasedModeling_2015_WilenskyRand,
  title = {An {{Introduction}} to {{Agent-Based Modeling}}: {{Modeling Natural}}, {{Social}}, and {{Engineered Complex Systems}} with {{NetLogo}}},
  shorttitle = {An {{Introduction}} to {{Agent-Based Modeling}}},
  author = {Wilensky, Uri and Rand, William},
  date = {2015-04-03},
  publisher = {The MIT Press},
  location = {Cambridge (Mass.)},
  abstract = {A comprehensive and hands-on introduction to the core concepts, methods, and applications of agent-based modeling, including detailed NetLogo examples.The advent of widespread fast computing has enabled us to work on more complex problems and to build and analyze more complex models. This book provides an introduction to one of the primary methodologies for research in this new field of knowledge. Agent-based modeling (ABM) offers a new way of doing science: by conducting computer-based experiments. ABM is applicable to complex systems embedded in natural, social, and engineered contexts, across domains that range from engineering to ecology. An Introduction to Agent-Based Modeling offers a comprehensive description of the core concepts, methods, and applications of ABM. Its hands-on approach—with hundreds of examples and exercises using NetLogo—enables readers to begin constructing models immediately, regardless of experience or discipline.The book first describes the nature and rationale of agent-based modeling, then presents the methodology for designing and building ABMs, and finally discusses how to utilize ABMs to answer complex questions. Features in each chapter include step-by-step guides to developing models in the main text; text boxes with additional information and concepts; end-of-chapter explorations; and references and lists of relevant reading. There is also an accompanying website with all the models and code.},
  isbn = {978-0-262-73189-8},
  langid = {english},
  pagetotal = {504},
  keywords = {notion}
}

@book{IntroductionAlgorithms_2009_Cormen,
  title = {Introduction to {{Algorithms}}},
  editor = {Cormen, Thomas H.},
  date = {2009},
  edition = {3rd ed},
  publisher = {MIT Press},
  location = {Cambridge, Mass},
  isbn = {978-0-262-03384-8 978-0-262-53305-8},
  langid = {english},
  pagetotal = {1292},
  keywords = {Computer algorithms,Computer programming},
  annotation = {OCLC: ocn311310321}
}

@book{IntroductionAppliedLinear_2018_BoydVandenberghe,
  title = {Introduction to {{Applied Linear Algebra}}: {{Vectors}}, {{Matrices}}, and {{Least Squares}}},
  shorttitle = {Introduction to {{Applied Linear Algebra}}},
  author = {Boyd, Stephen and Vandenberghe, Lieven},
  date = {2018-06-07},
  edition = {1},
  publisher = {Cambridge University Press \& Assessment},
  doi = {10.1017/9781108583664},
  url = {https://www.cambridge.org/highereducation/product/9781108583664/book},
  urldate = {2024-04-10},
  abstract = {This groundbreaking textbook combines straightforward explanations with a wealth of practical examples to offer an innovative approach to teaching linear algebra. Requiring no prior knowledge of the subject, it covers the aspects of linear algebra - vectors, matrices, and least squares - that are needed for engineering applications, discussing examples across data science, machine learning and artificial intelligence, signal and image processing, tomography, navigation, control, and finance. The numerous practical exercises throughout allow students to test their understanding and translate their knowledge into solving real-world problems, with lecture slides, additional computational exercises in Julia and MATLAB®, and data sets accompanying the book online. Suitable for both one-semester and one-quarter courses, as well as self-study, this self-contained text provides beginning students with the foundation they need to progress to more advanced study.},
  isbn = {978-1-108-58366-4 978-1-316-51896-0},
  langid = {english},
  file = {/home/baldoinov/baldoinov/PDFs/Livros/Introduction to Applied Linear Algebra_2018_Boyd et al.pdf}
}

@book{IntroductionComputableGeneral_2017_Burfisher,
  title = {Introduction to {{Computable General Equilibrium Models}}},
  author = {Burfisher, Mary E.},
  date = {2017},
  edition = {2},
  publisher = {Cambridge University Press},
  location = {Cambridge},
  doi = {10.1017/9781316450741},
  url = {https://www.cambridge.org/core/books/introduction-to-computable-general-equilibrium-models/0BE974BE5210FCD352290A6295F45211},
  urldate = {2023-09-27},
  abstract = {This book provides an accessible, undergraduate-level introduction to computable general equilibrium (CGE) models, a class of model that has come to play an important role in government policy decisions. The book uses a graphical approach to explain the economic theory that underlies a CGE model, and provides results from simple, small-scale CGE models to illustrate the links between theory and model outcomes. The book includes eleven guided, hands-on exercises that introduce modeling techniques that are applied to real-world economic problems. Students will learn how to integrate their separate fields of economic study into a comprehensive, general equilibrium perspective as they develop their skills as producers or consumers of CGE-based analysis.},
  keywords = {notion}
}

@online{IntroductionGraphMachine__,
  title = {Introduction to {{Graph Machine Learning}}},
  url = {https://huggingface.co/blog/intro-graphml},
  urldate = {2023-06-02},
  abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
  keywords = {notion},
  file = {/home/baldoinov/Zotero/storage/9Y3WATWY/intro-graphml.html}
}

@book{IntroductionLinearAlgebra_2005_Strang,
  title = {Introduction to {{Linear Algebra}}},
  author = {Strang, Gilbert},
  date = {2005},
  edition = {3. ed., rev. International ed., [Nachdr.]},
  publisher = {Wellesley-Cambridge Pr},
  location = {Wellesley, Mass},
  isbn = {978-0-9614088-9-3 978-0-9614088-4-8},
  langid = {english},
  pagetotal = {567}
}

@book{IntroductionStatisticalLearning_2017_JamesEtAl,
  title = {An {{Introduction}} to {{Statistical Learning}}: With Applications in {{R}}},
  shorttitle = {An Introduction to Statistical Learning},
  author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
  date = {2017},
  series = {Springer Texts in Statistics},
  edition = {Corrected at 8th printing},
  publisher = {Springer},
  location = {New York Heidelberg Dordrecht London},
  doi = {10.1007/978-1-4614-7138-7},
  isbn = {978-1-4614-7137-0},
  langid = {english},
  pagetotal = {426},
  file = {/home/baldoinov/baldoinov/PDFs/Livros/An Introduction to Statistical Learning_2017_James et al.pdf}
}

@book{IntroductionStatisticalLearning_2023_JamesEtAl,
  title = {An {{Introduction}} to {{Statistical Learning}}: With {{Applications}} in {{Python}}},
  shorttitle = {An {{Introduction}} to {{Statistical Learning}}},
  author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert and Taylor, Jonathan},
  date = {2023},
  series = {Springer {{Texts}} in {{Statistics}}},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-031-38747-0},
  url = {https://link.springer.com/10.1007/978-3-031-38747-0},
  urldate = {2024-03-21},
  isbn = {978-3-031-38746-3 978-3-031-38747-0},
  langid = {english},
  file = {/home/baldoinov/baldoinov/PDFs/Livros/An Introduction to Statistical Learning_2023_James et al.pdf}
}

@book{IntroductionStatisticsPython_2016_Haslwanter,
  title = {An {{Introduction}} to {{Statistics}} with {{Python}}},
  author = {Haslwanter, Thomas},
  date = {2016},
  series = {Statistics and {{Computing}}},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-28316-6},
  url = {http://link.springer.com/10.1007/978-3-319-28316-6},
  urldate = {2024-04-01},
  isbn = {978-3-319-28315-9 978-3-319-28316-6},
  langid = {english},
  file = {/home/baldoinov/baldoinov/PDFs/Livros/An Introduction to Statistics with Python_2016_Haslwanter.pdf}
}

@video{IntroductionUpliftModeling_2022_JuanOrduz,
  entrysubtype = {video},
  title = {Introduction to {{Uplift Modeling}}},
  shorttitle = {Dr. {{Juan Orduz}}},
  editor = {{Juan Orduz}},
  editortype = {director},
  date = {2022-05-12},
  url = {https://www.youtube.com/watch?v=VWjsi-5yc3w},
  urldate = {2025-08-17},
  abstract = {Speaker:: Dr. Juan Orduz Track: PyData: Machine Learning \& Stats}
}

@online{IntroductionUpliftModeling_2022_Strong,
  title = {An {{Introduction}} to {{Uplift Modeling}}},
  author = {Strong, Emily},
  date = {2022-06-22T21:07:39},
  url = {https://medium.com/the-data-nerd/an-introduction-to-uplift-modeling-737d0d78e3e},
  urldate = {2025-08-17},
  abstract = {Marketing campaigns can be expensive. You want to contact only the people who you think will be persuaded to do whatever it is your…},
  langid = {english},
  organization = {The Data Nerd},
  file = {/home/baldoinov/Zotero/storage/7VZ4CRCX/an-introduction-to-uplift-modeling-737d0d78e3e.html}
}

@online{IntroductionVisionLanguageModeling_2024_BordesEtAl,
  title = {An {{Introduction}} to {{Vision-Language Modeling}}},
  author = {Bordes, Florian and Pang, Richard Yuanzhe and Ajay, Anurag and Li, Alexander C. and Bardes, Adrien and Petryk, Suzanne and Mañas, Oscar and Lin, Zhiqiu and Mahmoud, Anas and Jayaraman, Bargav and Ibrahim, Mark and Hall, Melissa and Xiong, Yunyang and Lebensold, Jonathan and Ross, Candace and Jayakumar, Srihari and Guo, Chuan and Bouchacourt, Diane and Al-Tahan, Haider and Padthe, Karthik and Sharma, Vasu and Xu, Hu and Tan, Xiaoqing Ellen and Richards, Megan and Lavoie, Samuel and Astolfi, Pietro and Hemmat, Reyhane Askari and Chen, Jun and Tirumala, Kushal and Assouel, Rim and Moayeri, Mazda and Talattof, Arjang and Chaudhuri, Kamalika and Liu, Zechun and Chen, Xilun and Garrido, Quentin and Ullrich, Karen and Agrawal, Aishwarya and Saenko, Kate and Celikyilmaz, Asli and Chandra, Vikas},
  date = {2024-05-27},
  eprint = {2405.17247},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2405.17247},
  url = {http://arxiv.org/abs/2405.17247},
  urldate = {2024-06-02},
  abstract = {Following the recent popularity of Large Language Models (LLMs), several attempts have been made to extend them to the visual domain. From having a visual assistant that could guide us through unfamiliar environments to generative models that produce images using only a high-level text description, the vision-language model (VLM) applications will significantly impact our relationship with technology. However, there are many challenges that need to be addressed to improve the reliability of those models. While language is discrete, vision evolves in a much higher dimensional space in which concepts cannot always be easily discretized. To better understand the mechanics behind mapping vision to language, we present this introduction to VLMs which we hope will help anyone who would like to enter the field. First, we introduce what VLMs are, how they work, and how to train them. Then, we present and discuss approaches to evaluate VLMs. Although this work primarily focuses on mapping images to language, we also discuss extending VLMs to videos.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/An Introduction to Vision-Language Modeling_2024_Bordes et al.pdf;/home/baldoinov/Zotero/storage/BSN777XZ/2405.html}
}

@book{IntroductoryEconometricsModern_2016_Wooldridge,
  title = {Introductory {{Econometrics}}: {{A Modern Approach}}},
  shorttitle = {Introductory Econometrics},
  author = {Wooldridge, Jeffrey M.},
  date = {2016},
  edition = {Sixth edition, student edition},
  publisher = {Cengage Learning},
  location = {Boston, MA},
  isbn = {978-1-305-27010-7},
  langid = {english},
  pagetotal = {789}
}

@article{ipcc_2014,
  title = {Climate Change 2014: {{Synthesis}} Report},
  author = {{IPCC}},
  date = {2014},
  journaltitle = {Contribution of Working Groups I, II and III to the Fifth Assessment Report of the Intergovernmental Panel on Climate Change}
}

@book{IslamicGardensLandscapes_2008_Ruggles,
  title = {Islamic Gardens and Landscapes},
  author = {Ruggles, D. Fairchild},
  date = {2008},
  series = {Penn Studies in Landscape Architecture},
  publisher = {University of Pennsylvania press},
  location = {Philadelphia},
  isbn = {978-0-8122-4025-2},
  langid = {english}
}

@article{ISOIECIEEE_2017_,
  title = {{{ISO}}/{{IEC}}/{{IEEE International Standard}} - {{Systems}} and Software Engineering – {{Software}} Life Cycle Processes},
  date = {2017-11},
  journaltitle = {ISO/IEC/IEEE 12207:2017(E) First edition 2017-11},
  pages = {1--157},
  doi = {10.1109/IEEESTD.2017.8100771},
  url = {https://ieeexplore.ieee.org/document/8100771},
  urldate = {2025-02-27},
  eventtitle = {{{ISO}}/{{IEC}}/{{IEEE}} 12207:2017({{E}}) {{First}} Edition 2017-11},
  keywords = {acquisition,agreement,architecture,assessment,audit,configuration management,Decision making,decision management,design,development,disposal,enabling system,IEC Standards,IEEE Standards,implementation,information management,infrastructure,integration,ISO Standards,life cycle,life cycle model,life cycle stages,maintenance,measurement,operation,planning,portfolio,process,process improvement,process reference model,process tailoring,process view,product,Product life cycle management,quality management,requirements,retirement,risk management,service,software,Software engineering,Software maintenance,stages,stakeholder requirements,supply,system,system structure,system-of-interest,Systems engineering and theory,tailoring,transition,validation,verification},
  file = {/home/baldoinov/baldoinov/PDFs/Mackenzie/topicos-de-engenharia-de-software/ISO-IEC-IEEE International Standard - Systems and software engineering –_2017_.pdf;/home/baldoinov/Zotero/storage/AA5WXNFX/8100771.html}
}

@online{ItauAbreHackathon_2021_Ventura,
  title = {Itaú abre hackathon de open banking com bancos internacionais • Tecnoblog},
  author = {Ventura, Felipe},
  date = {2021-08},
  url = {https://tecnoblog.net/noticias/itau-abre-hackathon-de-open-banking-com-bancos-internacionais/},
  urldate = {2024-10-30},
  abstract = {Desafio Global de Open Finance reúne Itaú e bancos do Canadá, Austrália e Reino Unido; participantes terão acesso a sandbox que imita infraestrutura financeira},
  langid = {brazilian},
  organization = {Tecnoblog},
  file = {/home/baldoinov/Zotero/storage/9S6DLZ9P/itau-abre-hackathon-de-open-banking-com-bancos-internacionais.html}
}

@online{ItauAnunciaBatalha_2024_Lima,
  title = {Itaú anuncia Batalha de Dados 2024; veja como se inscrever},
  author = {Lima, Ramalho},
  date = {2024-06-17},
  url = {https://www.tecmundo.com.br/mercado/286016-itau-anuncia-batalha-dados-2024-veja-inscrever.htm},
  urldate = {2024-10-30},
  abstract = {Hackathon do Itaú será realizado em julho, e será aberto a todos os profissionais que trabalham com dados.},
  langid = {brazilian},
  file = {/home/baldoinov/Zotero/storage/ZHPGX26D/286016-itau-anuncia-batalha-dados-2024-veja-inscrever.html}
}

@online{ItauDigitalLab_2019_,
  title = {Itaú Digital Lab: conheça a inovação do maior banco brasileiro aliando a experiência do cliente, Pesquisa de Mercado e Metodologia Agile. - Ecglobal},
  shorttitle = {Itaú Digital Lab},
  date = {2019-08-30T16:18:01-03:00},
  url = {https://business.ecglobal.com/itau-digital-lab-conheca-a-inovacao-do-maior-banco-brasileiro-aliando-a-experiencia-do-cliente-pesquisa-de-mercado-e-metodologia-agile/},
  urldate = {2024-10-30},
  abstract = {Em um segmento sólido, como o de serviços financeiros, a expectativa é que a modernidade seja aplicada no oferecimento de recursos diferenciados, se},
  langid = {brazilian},
  file = {/home/baldoinov/Zotero/storage/T67PK6LE/itau-digital-lab-conheca-a-inovacao-do-maior-banco-brasileiro-aliando-a-experiencia-do-cliente-.html}
}

@online{ItauDigitalLab_2019_Guimaraes,
  title = {Itaú Digital Lab: a ferramenta do banco para ouvir continuamente seus clientes},
  shorttitle = {Itaú Digital Lab},
  author = {Guimarães, Leonardo},
  date = {2019-09-10T17:44:11+00:00},
  url = {https://consumidormoderno.com.br/itau-digital-lab-clientes/},
  urldate = {2024-10-30},
  abstract = {eCGobal ajudou o Itaú a criar a plataforma que promove diálogo contínuo com mais de 800 pessoas. Alguns produtos surgiram dentro das comunidades},
  langid = {brazilian},
  organization = {Consumidor Moderno},
  file = {/home/baldoinov/Zotero/storage/73DJV5QM/itau-digital-lab-clientes.html}
}

@inproceedings{ItemRecommendationMonotonic_2018_WanMcAuley,
  title = {Item Recommendation on Monotonic Behavior Chains},
  booktitle = {Proceedings of the 12th {{ACM Conference}} on {{Recommender Systems}}, {{RecSys}} 2018, {{Vancouver}}, {{BC}}, {{Canada}}, {{October}} 2-7, 2018},
  author = {Wan, Mengting and McAuley, Julian J.},
  editor = {Pera, Sole and Ekstrand, Michael D. and Amatriain, Xavier and O'Donovan, John},
  date = {2018},
  pages = {86--94},
  publisher = {ACM},
  doi = {10.1145/3240323.3240369},
  file = {/home/baldoinov/baldoinov/PDFs/Mackenzie/projeto-aplicado-iii/Item recommendation on monotonic behavior chains_2018_Wan et al.pdf}
}

@article{jackson_2001,
  title = {Historical Overfishing and the Recent Collapse of Coastal Ecosystems},
  author = {Jackson, J.B. and Kirby, M.X. and Berger, W.H. and Bjorndal, K.A. and Botsford, L.W. and Bourque, B.J. and Bradbury, R.H. and Cooke, R. and Erlandson, J. and Estes, J.A. and Hughes, T.P.},
  date = {2001},
  journaltitle = {Science (New York, N.Y.)},
  shortjournal = {Science},
  volume = {293},
  number = {5530},
  pages = {629--637}
}

@video{Joao44653_2019_TassosLycurgo,
  entrysubtype = {video},
  title = {João 4:46-53 | {{Os Três Estágios}} Da {{Fé}}},
  shorttitle = {João 4},
  editor = {{Tassos Lycurgo}},
  editortype = {director},
  date = {2019-09-29},
  url = {https://www.youtube.com/watch?v=EtPN3IlPYOE},
  urldate = {2025-06-29},
  abstract = {DEFESA DA FÉ: aqui é proibido não pensar! Culto da Palavra | 29.09.2019   Série: OS MILAGRES DE JESUS}
}

@book{JoaoComentariosExpositivos_2015_Lopes,
  title = {João - Comentários Expositivos Hagnos: As glórias dos filhos de Deus},
  shorttitle = {João - Comentários Expositivos Hagnos},
  author = {Lopes, Hernandes Dias},
  date = {2015-01-01},
  edition = {Portugues edição},
  publisher = {Hagnos},
  abstract = {O evangelho de João é um reservatório inesgotável de onde jorra abundantemente o conhecimento de Jesus. Foi escrito para conhecer aquele que se apresentou como o pão da vida, a luz do mundo, a porta, o bom pastor, o caminho, e a verdade, e a vida, a ressurreição e a vida, e, a videira verdadeira. Aqui, sete milagres são registrados para confi rmar a divindade de Jesus Cristo. Ele transformou água em vinho, curou o ofi cial do rei, levantou o paralítico de Betesda, multiplicou pães e peixes, acalmou a tempestade, curou em cego de nascença e ressuscitou Lázaro.Diante dessas evidências irrefutáveis, só nos restam três opções: Ou Jesus é um mentiroso, pois afi rmou ser quem não é, ou ele é um lunático, pois pensou ser quem não é, ou, então, ele é o Filho de Deus. Para provar que Jesus é o Filho de Deus, e que só nele temos a vida eterna, é que este evangelho foi escrito. Sacie sua alma nessa fonte bendita! Boa leitura!},
  isbn = {978-85-7742-173-2},
  langid = {portuguese}
}

@online{KANKolmogorovArnoldNetworks_2024_LiuEtAl,
  title = {{{KAN}}: {{Kolmogorov-Arnold Networks}}},
  shorttitle = {{{KAN}}},
  author = {Liu, Ziming and Wang, Yixuan and Vaidya, Sachin and Ruehle, Fabian and Halverson, James and Soljačić, Marin and Hou, Thomas Y. and Tegmark, Max},
  date = {2024-04-30},
  eprint = {2404.19756},
  eprinttype = {arXiv},
  eprintclass = {cond-mat, stat},
  doi = {10.48550/arXiv.2404.19756},
  url = {http://arxiv.org/abs/2404.19756},
  urldate = {2024-05-02},
  abstract = {Inspired by the Kolmogorov-Arnold representation theorem, we propose Kolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer Perceptrons (MLPs). While MLPs have fixed activation functions on nodes ("neurons"), KANs have learnable activation functions on edges ("weights"). KANs have no linear weights at all -- every weight parameter is replaced by a univariate function parametrized as a spline. We show that this seemingly simple change makes KANs outperform MLPs in terms of accuracy and interpretability. For accuracy, much smaller KANs can achieve comparable or better accuracy than much larger MLPs in data fitting and PDE solving. Theoretically and empirically, KANs possess faster neural scaling laws than MLPs. For interpretability, KANs can be intuitively visualized and can easily interact with human users. Through two examples in mathematics and physics, KANs are shown to be useful collaborators helping scientists (re)discover mathematical and physical laws. In summary, KANs are promising alternatives for MLPs, opening opportunities for further improving today's deep learning models which rely heavily on MLPs.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Condensed Matter - Disordered Systems and Neural Networks,Statistics - Machine Learning},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/KAN_2024_Liu et al.pdf;/home/baldoinov/Zotero/storage/5FUUL6KA/2404.html}
}

@article{KeepThemReading_2010_Vanneman,
  title = {Keep {{Them Reading}}},
  author = {Vanneman, Susan},
  date = {2010-12},
  journaltitle = {School Library Monthly},
  volume = {27},
  number = {3},
  pages = {21--22},
  publisher = {Libraries Unlimited},
  issn = {2166-160X},
  abstract = {The relentless quest of the school librarian is finding the right book for each student, searching for the home-run book for every child, and keeping students reading book after book after book. A considerable amount of the school librarian's time, effort, and creativity are devoted not only to motivating students to read, but to keeping them reading. Keeping students reading is the result of conscientious and constant effort on the part of the school librarian. Special events may provide brief spurts of interest in reading, but students evolve into real readers, in part, as the result of the school librarian's everyday attention to students and the library collection. In this article, the author offers some tips for success.},
  langid = {english},
  keywords = {Adolescent Literature,Librarian Teacher Cooperation,Librarians,Library Role,Middle School Students,Planning,Reading Material Selection,Reading Materials,Reading Motivation,School Libraries},
  annotation = {ERIC Number: EJ906678},
  file = {/home/baldoinov/Zotero/storage/2Y79E4SK/eric.ed.gov.html}
}

@article{KnowledgePowerOpenworld_2024_ZhengEtAl,
  title = {Knowledge Is Power: {{Open-world}} Knowledge Representation Learning for Knowledge-Based Visual Reasoning},
  shorttitle = {Knowledge Is Power},
  author = {Zheng, Wenbo and Yan, Lan and Wang, Fei-Yue},
  date = {2024-08-01},
  journaltitle = {Artificial Intelligence},
  shortjournal = {Artificial Intelligence},
  volume = {333},
  pages = {104147},
  issn = {0004-3702},
  doi = {10.1016/j.artint.2024.104147},
  url = {https://www.sciencedirect.com/science/article/pii/S0004370224000833},
  urldate = {2024-05-17},
  abstract = {Knowledge-based visual reasoning requires the ability to associate outside knowledge that is not present in a given image for cross-modal visual understanding. Two deficiencies of the existing approaches are that (1) they only employ or construct elementary and explicit but superficial knowledge graphs while lacking complex and implicit but indispensable cross-modal knowledge for visual reasoning, and (2) they also cannot reason new/unseen images or questions in open environments and are often violated in real-world applications. How to represent and leverage tacit multimodal knowledge for open-world visual reasoning scenarios has been less studied. In this paper, we propose a novel open-world knowledge representation learning method to not only construct implicit knowledge representations from the given images and their questions but also enable knowledge transfer from a known given scene to an unknown scene for answer prediction. Extensive experiments conducted on six benchmarks demonstrate the superiority of our approach over other state-of-the-art methods. We apply our approach to other visual reasoning tasks, and the experimental results show that our approach, with its good performance, can support related reasoning applications.},
  keywords = {Graph model,Knowledge representation learning,Open-world learning,Visual reasoning},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Knowledge is power_2024_Zheng et al.pdf;/home/baldoinov/Zotero/storage/XZE3HV4Z/S0004370224000833.html}
}

@article{lambin_global_2011,
  title = {Global Land Use Change, Economic Globalization, and the Looming Land Scarcity},
  author = {Lambin, Eric F. and Meyfroidt, Patrick},
  date = {2011-03},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {108},
  number = {9},
  pages = {3465--3472},
  doi = {10.1073/pnas.1100480108},
  url = {https://www.pnas.org/doi/10.1073/pnas.1100480108},
  urldate = {2023-12-11},
  abstract = {A central challenge for sustainability is how to preserve forest ecosystems and the services that they provide us while enhancing food production. This challenge for developing countries confronts the force of economic globalization, which seeks cropland that is shrinking in availability and triggers deforestation. Four mechanisms—the displacement, rebound, cascade, and remittance effects—that are amplified by economic globalization accelerate land conversion. A few developing countries have managed a land use transition over the recent decades that simultaneously increased their forest cover and agricultural production. These countries have relied on various mixes of agricultural intensification, land use zoning, forest protection, increased reliance on imported food and wood products, the creation of off-farm jobs, foreign capital investments, and remittances. Sound policies and innovations can therefore reconcile forest preservation with food production. Globalization can be harnessed to increase land use efficiency rather than leading to uncontrolled land use expansion. To do so, land systems should be understood and modeled as open systems with large flows of goods, people, and capital that connect local land use with global-scale factors.}
}

@inproceedings{LanguageModelsAre_2020_BrownEtAl,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  date = {2020},
  volume = {33},
  pages = {1877--1901},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html},
  urldate = {2024-03-08},
  abstract = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting.  For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model.  GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora.},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Language Models are Few-Shot Learners_2020_Brown et al.pdf}
}

@online{LanguageModelsByte_2024_WuEtAl,
  title = {Beyond {{Language Models}}: {{Byte Models}} Are {{Digital World Simulators}}},
  shorttitle = {Beyond {{Language Models}}},
  author = {Wu, Shangda and Tan, Xu and Wang, Zili and Wang, Rui and Li, Xiaobing and Sun, Maosong},
  date = {2024-02-29},
  eprint = {2402.19155},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2402.19155},
  url = {http://arxiv.org/abs/2402.19155},
  urldate = {2024-03-04},
  abstract = {Traditional deep learning often overlooks bytes, the basic units of the digital world, where all forms of information and operations are encoded and manipulated in binary format. Inspired by the success of next token prediction in natural language processing, we introduce bGPT, a model with next byte prediction to simulate the digital world. bGPT matches specialized models in performance across various modalities, including text, audio, and images, and offers new possibilities for predicting, simulating, and diagnosing algorithm or hardware behaviour. It has almost flawlessly replicated the process of converting symbolic music data, achieving a low error rate of 0.0011 bits per byte in converting ABC notation to MIDI format. In addition, bGPT demonstrates exceptional capabilities in simulating CPU behaviour, with an accuracy exceeding 99.99\% in executing various operations. Leveraging next byte prediction, models like bGPT can directly learn from vast binary data, effectively simulating the intricate patterns of the digital world.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,notion},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Beyond Language Models_2024_Wu et al.pdf;/home/baldoinov/Zotero/storage/4CTF2KZH/2402.html}
}

@online{LargeLanguageModels_2023_YangEtAl,
  title = {Large {{Language Models}} as {{Optimizers}}},
  author = {Yang, Chengrun and Wang, Xuezhi and Lu, Yifeng and Liu, Hanxiao and Le, Quoc V. and Zhou, Denny and Chen, Xinyun},
  date = {2023-09-06},
  eprint = {2309.03409},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2309.03409},
  url = {http://arxiv.org/abs/2309.03409},
  urldate = {2023-09-18},
  abstract = {Optimization is ubiquitous. While derivative-based algorithms have been powerful tools for various problems, the absence of gradient imposes challenges on many real-world applications. In this work, we propose Optimization by PROmpting (OPRO), a simple and effective approach to leverage large language models (LLMs) as optimizers, where the optimization task is described in natural language. In each optimization step, the LLM generates new solutions from the prompt that contains previously generated solutions with their values, then the new solutions are evaluated and added to the prompt for the next optimization step. We first showcase OPRO on linear regression and traveling salesman problems, then move on to prompt optimization where the goal is to find instructions that maximize the task accuracy. With a variety of LLMs, we demonstrate that the best prompts optimized by OPRO outperform human-designed prompts by up to 8\% on GSM8K, and by up to 50\% on Big-Bench Hard tasks.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,notion},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Large Language Models as Optimizers_2023_Yang et al.pdf;/home/baldoinov/Zotero/storage/FWPA487X/2309.html}
}

@online{LargeLanguageModels_2024_TanEtAl,
  title = {Large {{Language Models}} for {{Data Annotation}}: {{A Survey}}},
  shorttitle = {Large {{Language Models}} for {{Data Annotation}}},
  author = {Tan, Zhen and Beigi, Alimohammad and Wang, Song and Guo, Ruocheng and Bhattacharjee, Amrita and Jiang, Bohan and Karami, Mansooreh and Li, Jundong and Cheng, Lu and Liu, Huan},
  date = {2024-02-20},
  eprint = {2402.13446},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2402.13446},
  url = {http://arxiv.org/abs/2402.13446},
  urldate = {2024-03-01},
  abstract = {Data annotation is the labeling or tagging of raw data with relevant information, essential for improving the efficacy of machine learning models. The process, however, is labor-intensive and expensive. The emergence of advanced Large Language Models (LLMs), exemplified by GPT-4, presents an unprecedented opportunity to revolutionize and automate the intricate process of data annotation. While existing surveys have extensively covered LLM architecture, training, and general applications, this paper uniquely focuses on their specific utility for data annotation. This survey contributes to three core aspects: LLM-Based Data Annotation, Assessing LLM-generated Annotations, and Learning with LLM-generated annotations. Furthermore, the paper includes an in-depth taxonomy of methodologies employing LLMs for data annotation, a comprehensive review of learning strategies for models incorporating LLM-generated annotations, and a detailed discussion on primary challenges and limitations associated with using LLMs for data annotation. As a key guide, this survey aims to direct researchers and practitioners in exploring the potential of the latest LLMs for data annotation, fostering future advancements in this critical domain. We provide a comprehensive papers list at \textbackslash url\{https://github.com/Zhen-Tan-dmml/LLM4Annotation.git\}.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,notion},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/projeto-heiwa/Large Language Models for Data Annotation_2024_Tan et al.pdf;/home/baldoinov/Zotero/storage/RYFHD46W/2402.html}
}

@online{LayerNormalization_2016_BaEtAl,
  title = {Layer {{Normalization}}},
  author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
  date = {2016-07-21},
  eprint = {1607.06450},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1607.06450},
  url = {http://arxiv.org/abs/1607.06450},
  urldate = {2023-07-24},
  abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,notion,Statistics - Machine Learning},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Layer Normalization_2016_Ba et al.pdf;/home/baldoinov/Zotero/storage/77CITH8Q/1607.html}
}

@article{LeanStartupCanvas_2014_NardesMiranda,
  title = {Lean Startup e Canvas: uma proposta de metodologia para startups},
  shorttitle = {Lean Startup e Canvas},
  author = {Nardes, Felipe Bruno Souza and Miranda, Roberto Campos Da Rocha},
  date = {2014-11-15},
  journaltitle = {Revista Brasileira de Administração Científica},
  shortjournal = {RBADM},
  volume = {5},
  number = {3},
  pages = {252--272},
  issn = {2179-684X},
  doi = {10.6008/SPC2179-684X.2014.003.0015},
  url = {http://www.sustenere.co/index.php/rbadm/article/view/SPC2179-684X.2014.003.0015},
  urldate = {2025-03-21},
  abstract = {This paper introduces the use of the Lean Startup methodology and business modeling through the Canvas tool in nascent companies called startups. This is a new form of entrepreneurship, which confirms the prevalence of experimentation as opposed to detailed planning. Nowadays, some scholars and entrepreneurs defend the idea that you cannot tackle in the traditional way, with the aid of a business plan, creating startups. Supporting by literature on the subject, this study showed how the Lean Startup methodology can better cope with the dynamism and uncertainty present in the creation and development of a startup than the traditional approach, which supports the preparation of a plan business. However, for comparative purposes, it was also addressed in this study the main features of a business plan, examining the pros and cons of this tool. As a result, we sought to contribute to the success of entrepreneurs involved in startups.},
  langid = {portuguese},
  file = {/home/baldoinov/baldoinov/PDFs/Mackenzie/projetos-empreendedores/Lean Startup e Canvas_2014_Nardes et al.pdf}
}

@article{LearningAgentbasedModels_2023_MontiEtAl,
  title = {On Learning Agent-Based Models from Data},
  author = {Monti, Corrado and Pangallo, Marco and De Francisci Morales, Gianmarco and Bonchi, Francesco},
  date = {2023-06-07},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {13},
  number = {1},
  pages = {9268},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-023-35536-3},
  url = {https://www.nature.com/articles/s41598-023-35536-3},
  urldate = {2023-09-30},
  abstract = {Agent-Based Models (ABMs) are used in several fields to study the evolution of complex systems from micro-level assumptions. However, a significant drawback of ABMs is their inability to estimate agent-specific (or “micro”) variables, which hinders their ability to make accurate predictions using micro-level data. In this paper, we propose a protocol to learn the latent micro-variables of an ABM from data. We begin by translating an ABM into a probabilistic model characterized by a computationally tractable likelihood. Next, we use a gradient-based expectation maximization algorithm to maximize the likelihood of the latent variables. We showcase the efficacy of our protocol on an ABM of the housing market, where agents with different incomes bid higher prices to live in high-income neighborhoods. Our protocol produces accurate estimates of the latent variables while preserving the general behavior of the ABM. Moreover, our estimates substantially improve the out-of-sample forecasting capabilities of the ABM compared to simpler heuristics. Our protocol encourages modelers to articulate assumptions, consider the inferential process, and spot potential identification problems, thus making it a useful alternative to black-box data assimilation methods.},
  issue = {1},
  langid = {english},
  keywords = {Computational science,Computer science,notion},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/On learning agent-based models from data_2023_Monti et al.pdf}
}

@inproceedings{LearningOptimalPigovian_2023_HuaEtAl,
  title = {Learning {{Optimal}} "{{Pigovian Tax}}" in {{Sequential Social Dilemmas}}},
  booktitle = {Proceedings of the 2023 {{International Conference}} on {{Autonomous Agents}} and {{Multiagent Systems}}},
  author = {Hua, Yun and Gao, Shang and Li, Wenhao and Jin, Bo and Wang, Xiangfeng and Zha, Hongyuan},
  date = {2023-05-30},
  series = {{{AAMAS}} '23},
  pages = {2784--2786},
  publisher = {{International Foundation for Autonomous Agents and Multiagent Systems}},
  location = {Richland, SC},
  abstract = {In multi-agent reinforcement learning (MARL), each agent acts to maximize its individual accumulated rewards. Nevertheless, individual accumulated rewards could not fully reflect how others perceive them, resulting in selfish behaviors that undermine global performance, which brings the social dilemmas. This paper adapt the famous externality theory in economic area to analyze social dilemmas in MARL, and propose the method called Learning Optimal Pigovian Tax (LOPT) to internalize the externalities in MARL. Furthermore, a reward shaping mechanism based on the approximated optimal "Pigovian Tax'' is applied to reduce the social cost of each agent and tries to alleviate the social dilemmas. Compared with existing state-of-the-art methods, the proposed LOPT leads to higher collective social welfare in both the Escape Room and the Cleanup environments, which shows the superiority of our method in solving social dilemmas.},
  isbn = {978-1-4503-9432-1},
  keywords = {externality,multi-agent reinforcement learning,reward shaping,sequential social dilemmas},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Learning Optimal Pigovian Tax in Sequential Social Dilemmas_2023_Hua et al.pdf}
}

@book{LearningPython_2017_Lutz,
  title = {Learning {{Python}}},
  author = {Lutz, Mark},
  date = {2017},
  edition = {Fifth edition},
  publisher = {O'Reilly},
  location = {Beijing Boston Farnham Sebastopol Tokyo},
  isbn = {978-1-4493-5573-9},
  langid = {english},
  pagetotal = {1590}
}

@article{LearningRepresentationsBackpropagating_1986_RumelhartEtAl,
  title = {Learning Representations by Back-Propagating Errors},
  author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  date = {1986-10},
  journaltitle = {Nature},
  volume = {323},
  number = {6088},
  pages = {533--536},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/323533a0},
  url = {https://www.nature.com/articles/323533a0},
  urldate = {2024-06-25},
  abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
  langid = {english},
  keywords = {Humanities and Social Sciences,multidisciplinary,Science},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Learning representations by back-propagating errors_1986_Rumelhart et al.pdf}
}

@book{LearningSQLGenerate_2020_Beaulieu,
  title = {Learning {{SQL}}: {{Generate}}, {{Manipulate}}, and {{Retrieve Data}}},
  shorttitle = {Learning {{SQL}}},
  author = {Beaulieu, Alan},
  date = {2020},
  edition = {Third edition},
  publisher = {O´Reilly},
  location = {Beijing Boston Farnham Sebastopol Tokyo},
  isbn = {978-1-4920-5761-1},
  langid = {english},
  pagetotal = {360}
}

@book{LecturesPublicEconomics_2015_AtkinsonStiglitz,
  title = {Lectures on {{Public Economics}}},
  author = {Atkinson, Anthony B. and Stiglitz, Joseph E.},
  date = {2015},
  publisher = {Princeton University Press},
  location = {Princeton Oxford},
  abstract = {This classic introduction to public finance remains the best advanced-level textbook on the subject ever written. First published in 1980, Lectures on Public Economics still tops reading lists at many leading universities despite the fact that the book has been out of print for years. This new edition makes it readily available again to a new generation of students and practitioners in public economics.The lectures presented here examine the behavioral responses of households and firms to tax changes. Topics include the effects of taxation on labor supply, savings, risk-taking, the firm, debt, and economic growth. The book then delves into normative questions such as the design of tax systems, optimal taxation, public sector pricing, and public goods, including local public goods.Written by two of the world's preeminent economists, this edition of Lectures on Public Economics features a new introduction by Anthony Atkinson and Joseph Stiglitz that discusses the latest developments in the field and areas for future research. The definitive advanced-level textbook on public economics Examines the effects of taxation on households and firms Covers tax system design, optimal taxation, public sector pricing, and more Includes suggestions for further reading Additional resources available online},
  isbn = {978-0-691-16641-4},
  langid = {english},
  pagetotal = {532}
}

@online{LeontiefInsumoProdutoAntecedentes_2001_Guilhoto,
  type = {SSRN Scholarly Paper},
  title = {Leontief {{E Insumo-Produto}}: {{Antecedentes}}, {{Princípios E Evolução}} ({{Leotief}} and {{Input-Output}}: {{Background}}, {{Principles}} and {{Evolution}})},
  shorttitle = {Leontief {{E Insumo-Produto}}},
  author = {Guilhoto, Joaquim},
  date = {2001},
  number = {2414028},
  location = {Rochester, NY},
  doi = {10.2139/ssrn.2414028},
  url = {https://papers.ssrn.com/abstract=2414028},
  urldate = {2023-10-31},
  abstract = {Portuguese Abstract: Neste trabalho é feita uma revisão da literatura econômica, visando, de um lado apresentar os antecedentes da teoria de insumo-produto de Leontief, situando-a dentro da teoria do pensamento econômico, e de outro lado apresentar os princípios e a evolução desta teoria, tanto em termos teóricos como práticos. De modo a atingir os objetivo deste trabalho, primeiro é feito um levantamento da história do pensamento econômico da teoria de Leontief, desde as suas origens nos trabalhos de economistas pré-fisiocratas, como Petty e Cantillon, passando pelo trabalho do fisiocrata Quesnay, e dos trabalhos de outros economistas como Isnard, Smith, Ricardo, Torrens, Marx, Dmitriev, von Bortkiewicz e Walras. Passa-se então a apresentar os princípios da teoria de insumo-produto, primeiro através de uma breve apresentação da teoria dos modelos estáticos e dinâmicos de Leontief, depois, através da apresentação das principais áreas nas quais Leontief contribui com inovações. Em seguida é feita uma análise da evolução da teoria de insumo-produto através da apresentação das aplicações que vêm sendo desenvolvidas em estudos de insumo-produto de um modo geral e naqueles aplicados para a economia brasileira. Por último é feita uma apresentação das direções que a teoria de insumo-produto deverá tomar no futuro.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {economic theory,input-output,Leontief,notion},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Leontief E Insumo-Produto_2001_Guilhoto.pdf}
}

@book{LibreCADRealDummies__Heikell,
  title = {{{LibreCAD}} for {{Real Dummies}}},
  author = {Heikell, Johnny},
  langid = {english},
  file = {/home/baldoinov/baldoinov/PDFs/Livros/LibreCAD for Real Dummies_Heikell.pdf}
}

@book{LinkedNewScience_2002_BarabasiFrangos,
  title = {Linked: {{The New Science Of Networks}}},
  shorttitle = {Linked},
  author = {Barabasi, Albert-laszlo and Frangos, Jennifer},
  date = {2002-05-15},
  edition = {First Edition},
  publisher = {Perseus Books Group},
  location = {New York, NY},
  abstract = {In the 1980's, James Gleick's Chaos introduced the world to complexity. Now, Albert-László Barabási's Linked reveals the next major scientific leap: the study of networks. We've long suspected that we live in a small world, where everything is connected to everything else. Indeed, networks are pervasive--from the human brain to the Internet to the economy to our group of friends. These linkages, it turns out, aren't random. All networks, to the great surprise of scientists, have an underlying order and follow simple laws. Understanding the structure and behavior of these networks will help us do some amazing things, from designing the optimal organization of a firm to stopping a disease outbreak before it spreads catastrophically.In Linked, Barabási, a physicist whose work has revolutionized the study of networks, traces the development of this rapidly unfolding science and introduces us to the scientists carrying out this pioneering work. These "new cartographers" are mapping networks in a wide range of scientific disciplines, proving that social networks, corporations, and cells are more similar than they are different, and providing important new insights into the interconnected world around us. This knowledge, says Barabási, can shed light on the robustness of the Internet, the spread of fads and viruses, even the future of democracy. Engaging and authoritative, Linked provides an exciting preview of the next century in science, guaranteed to be transformed by these amazing discoveries.From Linked:This book has a simple message: think networks. It is about how networks emerge, what they look like, and how they evolve. It aims to develop a web-based view of nature, society, and technology, providing a unified framework to better understand issues ranging from the vulnerability of the Internet to the spread of diseases. Networks are present everywhere. All we need is an eye for them...We will see the challenges doctors face when they attempt to cure a disease by focusing on a single molecule or gene, disregarding the complex interconnected nature of the living matter. We will see that hackers are not alone in attacking networks: we all play Goliath, firing shots at a fragile ecological network that, without further support, could soon replicate our worst nightmares by turning us into an isolated group of species...Linked is meant to be an eye-opening trip that challenges you to walk across disciplines by stepping out of the box of reductionism. It is an invitation to explore link by link the next scientific revolution: the new science of networks.},
  isbn = {978-0-7382-0667-7},
  langid = {english},
  pagetotal = {288}
}

@online{LLMaAAMakingLarge_2023_ZhangEtAl,
  title = {{{LLMaAA}}: {{Making Large Language Models}} as {{Active Annotators}}},
  shorttitle = {{{LLMaAA}}},
  author = {Zhang, Ruoyu and Li, Yanzeng and Ma, Yongliang and Zhou, Ming and Zou, Lei},
  date = {2023-10-31},
  eprint = {2310.19596},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2310.19596},
  url = {http://arxiv.org/abs/2310.19596},
  urldate = {2024-03-01},
  abstract = {Prevalent supervised learning methods in natural language processing (NLP) are notoriously data-hungry, which demand large amounts of high-quality annotated data. In practice, acquiring such data is a costly endeavor. Recently, the superior few-shot performance of large language models (LLMs) has propelled the development of dataset generation, where the training data are solely synthesized from LLMs. However, such an approach usually suffers from low-quality issues, and requires orders of magnitude more labeled data to achieve satisfactory performance. To fully exploit the potential of LLMs and make use of massive unlabeled data, we propose LLMaAA, which takes LLMs as annotators and puts them into an active learning loop to determine what to annotate efficiently. To learn robustly with pseudo labels, we optimize both the annotation and training processes: (1) we draw k-NN examples from a small demonstration pool as in-context examples, and (2) we adopt the example reweighting technique to assign training samples with learnable weights. Compared with previous approaches, LLMaAA features both efficiency and reliability. We conduct experiments and analysis on two classic NLP tasks, named entity recognition and relation extraction. With LLMaAA, task-specific models trained from LLM-generated labels can outperform the teacher within only hundreds of annotated examples, which is much more cost-effective than other baselines.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,notion},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/projeto-heiwa/LLMaAA_2023_Zhang et al.pdf;/home/baldoinov/Zotero/storage/T9NVD7SI/2310.html}
}

@online{LLMFlashEfficient_2023_AlizadehEtAl,
  title = {{{LLM}} in a Flash: {{Efficient Large Language Model Inference}} with {{Limited Memory}}},
  shorttitle = {{{LLM}} in a Flash},
  author = {Alizadeh, Keivan and Mirzadeh, Iman and Belenko, Dmitry and Khatamifard, Karen and Cho, Minsik and Del Mundo, Carlo C. and Rastegari, Mohammad and Farajtabar, Mehrdad},
  date = {2023-12-12},
  eprint = {2312.11514},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2312.11514},
  url = {http://arxiv.org/abs/2312.11514},
  urldate = {2023-12-31},
  abstract = {Large language models (LLMs) are central to modern natural language processing, delivering exceptional performance in various tasks. However, their intensive computational and memory requirements present challenges, especially for devices with limited DRAM capacity. This paper tackles the challenge of efficiently running LLMs that exceed the available DRAM capacity by storing the model parameters on flash memory but bringing them on demand to DRAM. Our method involves constructing an inference cost model that harmonizes with the flash memory behavior, guiding us to optimize in two critical areas: reducing the volume of data transferred from flash and reading data in larger, more contiguous chunks. Within this flash memory-informed framework, we introduce two principal techniques. First, "windowing'" strategically reduces data transfer by reusing previously activated neurons, and second, "row-column bundling", tailored to the sequential data access strengths of flash memory, increases the size of data chunks read from flash memory. These methods collectively enable running models up to twice the size of the available DRAM, with a 4-5x and 20-25x increase in inference speed compared to naive loading approaches in CPU and GPU, respectively. Our integration of sparsity awareness, context-adaptive loading, and a hardware-oriented design paves the way for effective inference of LLMs on devices with limited memory.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,notion},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/LLM in a flash_2023_Alizadeh et al.pdf;/home/baldoinov/Zotero/storage/FBNP7TCJ/2312.html}
}

@online{LocalGlobalGraph_2024_EdgeEtAl,
  title = {From {{Local}} to {{Global}}: {{A Graph RAG Approach}} to {{Query-Focused Summarization}}},
  shorttitle = {From {{Local}} to {{Global}}},
  author = {Edge, Darren and Trinh, Ha and Cheng, Newman and Bradley, Joshua and Chao, Alex and Mody, Apurva and Truitt, Steven and Larson, Jonathan},
  date = {2024-04-24},
  eprint = {2404.16130},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2404.16130},
  url = {http://arxiv.org/abs/2404.16130},
  urldate = {2024-05-22},
  abstract = {The use of retrieval-augmented generation (RAG) to retrieve relevant information from an external knowledge source enables large language models (LLMs) to answer questions over private and/or previously unseen document collections. However, RAG fails on global questions directed at an entire text corpus, such as "What are the main themes in the dataset?", since this is inherently a query-focused summarization (QFS) task, rather than an explicit retrieval task. Prior QFS methods, meanwhile, fail to scale to the quantities of text indexed by typical RAG systems. To combine the strengths of these contrasting methods, we propose a Graph RAG approach to question answering over private text corpora that scales with both the generality of user questions and the quantity of source text to be indexed. Our approach uses an LLM to build a graph-based text index in two stages: first to derive an entity knowledge graph from the source documents, then to pregenerate community summaries for all groups of closely-related entities. Given a question, each community summary is used to generate a partial response, before all partial responses are again summarized in a final response to the user. For a class of global sensemaking questions over datasets in the 1 million token range, we show that Graph RAG leads to substantial improvements over a na\textbackslash "ive RAG baseline for both the comprehensiveness and diversity of generated answers. An open-source, Python-based implementation of both global and local Graph RAG approaches is forthcoming at https://aka.ms/graphrag.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Information Retrieval,H.3.3,I.2.7},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/From Local to Global_2024_Edge et al.pdf;/home/baldoinov/Zotero/storage/TDB6J8S3/2404.html}
}

@online{LoRALowRankAdaptation_2021_HuEtAl,
  title = {{{LoRA}}: {{Low-Rank Adaptation}} of {{Large Language Models}}},
  shorttitle = {{{LoRA}}},
  author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  date = {2021-06-17},
  url = {https://arxiv.org/abs/2106.09685v2},
  urldate = {2023-10-09},
  abstract = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/LoRA_2021_Hu et al.pdf}
}

@article{lucchesi_2019,
  title = {Economic Evaluation of Massive Restoration in {{Brazil}}: How to Achieve the {{iNDC-Brazil}} Target},
  author = {Lucchesi, Andrea and Pereda, Paula and Christofoletti, Maria Alice Moz and Ussami, Keyi Ando and Gusson, Eduardo and Da Cunha, Girlei Costa},
  date = {2019},
  journaltitle = {International Journal of Global Environmental Issues},
  volume = {18},
  number = {1},
  pages = {41--70}
}

@video{MachineLearningAlgorithms_2024_CrunchDAO,
  entrysubtype = {video},
  title = {Machine {{Learning Algorithms}} for {{Financial Markets}} with {{Dr}}. {{Edoardo Vittori}}},
  editor = {{CrunchDAO}},
  editortype = {director},
  date = {2024-05-06},
  url = {https://www.youtube.com/watch?app=desktop&v=6wK4q8QvsV4&ab_channel=CrunchDAO},
  urldate = {2025-08-17},
  abstract = {The talk "Machine Learning Algorithms for Financial Markets" begins with an overview of algorithms in financial markets and an introduction to essential machine learning tools.  We then explore how these technologies can be used to develop intraday trading strategies and conclude with a real-}
}

@article{MachineLearningApplied_2017_MullainathanSpiess,
  title = {Machine {{Learning}}: {{An Applied Econometric Approach}}},
  shorttitle = {Machine {{Learning}}},
  author = {Mullainathan, Sendhil and Spiess, Jann},
  date = {2017-05},
  journaltitle = {Journal of Economic Perspectives},
  volume = {31},
  number = {2},
  pages = {87--106},
  issn = {0895-3309},
  doi = {10.1257/jep.31.2.87},
  url = {https://www.aeaweb.org/articles?id=10.1257/jep.31.2.87},
  urldate = {2023-04-13},
  abstract = {. This similarity to econometrics raises questions: How do these new empirical tools fit with what we know? As empirical economists, how can we use them? We present a way of thinking about machine learning that gives it its own place in the econometric toolbox. Machine learning not only provides new tools, it solves a different problem. Specifically, machine learning revolves around the problem of prediction, while many economic applications revolve around parameter estimation. So applying machine learning to economics requires finding relevant tasks. Machine learning algorithms are now technically easy to use: you can download convenient packages in R or Python. This also raises the risk that the algorithms are applied naively or their output is misinterpreted. We hope to make them conceptually easier to use by providing a crisper understanding of how these algorithms work, where they excel, and where they can stumble—and thus where they can be most usefully applied.},
  langid = {english},
  keywords = {Econometric Modeling: General,Econometric Software,notion},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Machine Learning_2017_Mullainathan et al.pdf}
}

@online{MachineLearningApproaches_2022_Wang,
  title = {Machine {{Learning Approaches}} to {{Automated Mechanism Design}} for {{Public Project Problem}}},
  author = {Wang, Guanhua},
  date = {2022-04-14},
  eprint = {2204.07315},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2204.07315},
  url = {http://arxiv.org/abs/2204.07315},
  urldate = {2024-06-20},
  abstract = {Mechanism design is a central research branch in microeconomics. An effective mechanism can significantly improve performance and efficiency of social decisions under desired objectives, such as to maximize social welfare or to maximize revenue for agents. However, mechanism design is challenging for many common models including the public project problem model which we study in this thesis. A typical public project problem is a group of agents crowdfunding a public project (e.g., building a bridge). The mechanism will decide the payment and allocation for each agent (e.g., how much the agent pays, and whether the agent can use it) according to their valuations. The mechanism can be applied to various economic scenarios, including those related to cyber security. There are different constraints and optimized objectives for different public project scenarios (sub-problems), making it unrealistic to design a universal mechanism that fits all scenarios, and designing mechanisms for different settings manually is a taxing job. Therefore, we explore automated mechanism design (AMD) of public project problems under different constraints. In this thesis, we focus on the public project problem, which includes many sub-problems (excludable/non-excludable, divisible/indivisible, binary/non-binary). We study the classical public project model and extend this model to other related areas such as the zero-day exploit markets. For different sub-problems of the public project problem, we adopt different novel machine learning techniques to design optimal or near-optimal mechanisms via automated mechanism design. We evaluate our mechanisms by theoretical analysis or experimentally comparing our mechanisms against existing mechanisms. The experiments and theoretical results show that our mechanisms are better than state-of-the-art automated or manual mechanisms.},
  pubstate = {prepublished},
  keywords = {Computer Science - Multiagent Systems},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Machine Learning Approaches to Automated Mechanism Design for Public Project_2022_Wang.pdf;/home/baldoinov/Zotero/storage/RBQ6VARS/2204.html}
}

@book{MachineLearningDesign_2020_LakshmananEtAl,
  title = {Machine Learning Design Patterns: Solutions to Common Challenges in Data Preparation, Model Building, and MLOps},
  shorttitle = {Machine Learning Design Patterns},
  author = {Lakshmanan, Valliappa and Robinson, Sara and Munn, Michael},
  date = {2020-11-24},
  edition = {1ª edição},
  publisher = {O'Reilly Media},
  location = {Sebastopol, CA},
  abstract = {The design patterns in this book capture best practices and solutions to recurring problems in machine learning. The authors, three Google engineers, catalog proven methods to help data scientists tackle common problems throughout the ML process. These design patterns codify the experience of hundreds of experts into straightforward, approachable advice.  In this book, you will find detailed explanations of 30 patterns for data and problem representation, operationalization, repeatability, reproducibility, flexibility, explainability, and fairness. Each pattern includes a description of the problem, a variety of potential solutions, and recommendations for choosing the best technique for your situation.  You'll learn how to: Identify and mitigate common challenges when training, evaluating, and deploying ML models Represent data for different ML model types, including embeddings, feature crosses, and more Choose the right model type for specific problems Build a robust training loop that uses checkpoints, distribution strategy, and hyperparameter tuning Deploy scalable ML systems that you can retrain and update to reflect new data Interpret model predictions for stakeholders and ensure models are treating users fairly},
  isbn = {978-1-09-811578-4},
  langid = {Inglês}
}

@article{MachineLearningEnhancedPairs_2024_HadadEtAl,
  title = {Machine {{Learning-Enhanced Pairs Trading}}},
  author = {Hadad, Eli and Hodarkar, Sohail and Lemeneh, Beakal and Shasha, Dennis},
  date = {2024-06},
  journaltitle = {Forecasting},
  volume = {6},
  number = {2},
  pages = {434--455},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2571-9394},
  doi = {10.3390/forecast6020024},
  url = {https://www.mdpi.com/2571-9394/6/2/24},
  urldate = {2024-09-04},
  abstract = {Forecasting returns in financial markets is notoriously challenging due to the resemblance of price changes to white noise. In this paper, we propose novel methods to address this challenge. Employing high-frequency Brazilian stock market data at one-minute granularity over a full year, we apply various statistical and machine learning algorithms, including Bidirectional Long Short-Term Memory (BiLSTM) with attention, Transformers, N-BEATS, N-HiTS, Convolutional Neural Networks (CNNs), and Temporal Convolutional Networks (TCNs) to predict changes in the price ratio of closely related stock pairs. Our findings indicate that a combination of reversion and machine learning-based forecasting methods yields the highest profit-per-trade. Additionally, by allowing the model to abstain from trading when the predicted magnitude of change is small, profits per trade can be further increased. Our proposed forecasting approach, utilizing a blend of methods, demonstrates superior accuracy compared to individual methods for high-frequency data.},
  issue = {2},
  langid = {english},
  keywords = {ARIMA,BiLSTM,forecasting,high-frequency data,N-BEATS,N-HiTS,pairs trading,transformers},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Machine Learning-Enhanced Pairs Trading_2024_Hadad et al.pdf}
}

@article{MachineLearningMethods_2015_BajariEtAl,
  title = {Machine {{Learning Methods}} for {{Demand Estimation}}},
  author = {Bajari, Patrick and Nekipelov, Denis and Ryan, Stephen P. and Yang, Miaoyu},
  date = {2015},
  journaltitle = {The American Economic Review},
  volume = {105},
  number = {5},
  eprint = {43821932},
  eprinttype = {jstor},
  pages = {481--485},
  publisher = {American Economic Association},
  issn = {0002-8282},
  url = {https://www.jstor.org/stable/43821932},
  urldate = {2023-05-30},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Machine Learning Methods for Demand Estimation_2015_Bajari et al.pdf}
}

@article{MachineLearningMethods_2019_AtheyImbens,
  title = {Machine {{Learning Methods That Economists Should Know About}}},
  author = {Athey, Susan and Imbens, Guido W.},
  date = {2019},
  journaltitle = {Annual Review of Economics},
  volume = {11},
  number = {1},
  pages = {685--725},
  doi = {10.1146/annurev-economics-080217-053433},
  url = {https://doi.org/10.1146/annurev-economics-080217-053433},
  urldate = {2023-04-13},
  abstract = {We discuss the relevance of the recent machine learning (ML) literature for economics and econometrics. First we discuss the differences in goals, methods, and settings between the ML literature and the traditional econometrics and statistics literatures. Then we discuss some specific methods from the ML literature that we view as important for empirical researchers in economics. These include supervised learning methods for regression and classification, unsupervised learning methods, and matrix completion methods. Finally, we highlight newly developed methods at the intersection of ML and econometrics that typically perform better than either off-the-shelf ML or more traditional econometric methods when applied to particular classes of problems, including causal inference for average treatment effects, optimal policy estimation, and estimation of the counterfactual effect of price changes in consumer choice models.},
  keywords = {causal inference,econometrics,JEL C30,machine learning,notion},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Machine Learning Methods That Economists Should Know About_2019_Athey et al.pdf}
}

@article{MachineLearningMethods_2023_AraujoGaglianone,
  title = {Machine Learning Methods for Inflation Forecasting in {{Brazil}}: {{New}} Contenders versus Classical Models},
  shorttitle = {Machine Learning Methods for Inflation Forecasting in {{Brazil}}},
  author = {Araujo, Gustavo Silva and Gaglianone, Wagner Piazza},
  date = {2023-06},
  journaltitle = {Latin American Journal of Central Banking},
  shortjournal = {Latin American Journal of Central Banking},
  volume = {4},
  number = {2},
  pages = {100087},
  issn = {26661438},
  doi = {10.1016/j.latcb.2023.100087},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S2666143823000042},
  urldate = {2024-04-08},
  langid = {english},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Machine learning methods for inflation forecasting in Brazil_2023_Araujo et al.pdf}
}

@book{MachineLearningProbabilistic_2012_Murphy,
  title = {Machine {{Learning}}: A Probabilistic Perspective},
  shorttitle = {Machine Learning},
  author = {Murphy, Kevin P.},
  date = {2012},
  series = {Adaptive Computation and Machine Learning Series},
  publisher = {MIT Press},
  location = {Cambridge, MA},
  isbn = {978-0-262-01802-9},
  langid = {english},
  pagetotal = {1067},
  keywords = {Machine learning,Probabilities}
}

@book{MachineLearningPyTorch_2022_RaschkaEtAl,
  title = {Machine {{Learning}} with {{PyTorch}} and {{Scikit-Learn}}: {{Develop}} Machine Learning and Deep Learning Models with {{Python}}},
  shorttitle = {Machine {{Learning}} with {{PyTorch}} and {{Scikit-Learn}}},
  author = {Raschka, Sebastian and Liu, Yuxi and Mirjalili, Vahid and Dzhulgakov, Dmytro},
  date = {2022-02-25},
  publisher = {Packt Publishing},
  location = {Birmingham Mumbai},
  abstract = {This book from the bestselling and widely acclaimed Python Machine Learning series is a comprehensive guide to machine and deep learning using PyTorch's simple-to-code framework.Purchase of the print or Kindle book includes a free eBook in PDF format.Key FeaturesLearn applied machine learning with a solid foundation in theoryClear, intuitive explanations take you deep into the theory and practice of Python machine learningFully updated and expanded to cover PyTorch, transformers, XGBoost, graph neural networks, and best practicesBook DescriptionMachine Learning with PyTorch and Scikit-Learn is a comprehensive guide to machine learning and deep learning with PyTorch. It acts as both a step-by-step tutorial and a reference you'll keep coming back to as you build your machine learning systems.Packed with clear explanations, visualizations, and examples, the book covers all the essential machine learning techniques in depth. While some books teach you only to follow instructions, with this machine learning book, we teach the principles allowing you to build models and applications for yourself.Why PyTorch?PyTorch is the Pythonic way to learn machine learning, making it easier to learn and simpler to code with. This book explains the essential parts of PyTorch and how to create models using popular libraries, such as PyTorch Lightning and PyTorch Geometric.You will also learn about generative adversarial networks (GANs) for generating new data and training intelligent agents with reinforcement learning. Finally, this new edition is expanded to cover the latest trends in deep learning, including graph neural networks and large-scale transformers used for natural language processing (NLP).This PyTorch book is your companion to machine learning with Python, whether you're a Python developer new to machine learning or want to deepen your knowledge of the latest developments.What you will learnExplore frameworks, models, and techniques for machines to 'learn' from dataUse scikit-learn for machine learning and PyTorch for deep learningTrain machine learning classifiers on images, text, and moreBuild and train neural networks, transformers, and boosting algorithmsDiscover best practices for evaluating and tuning modelsPredict continuous target outcomes using regression analysisDig deeper into textual and social media data using sentiment analysisWho this book is forIf you have a good grasp of Python basics and want to start learning about machine learning and deep learning, then this is the book for you. This is an essential resource written for developers and data scientists who want to create practical machine learning and deep learning applications using scikit-learn and PyTorch.Before you get started with this book, you'll need a good understanding of calculus, as well as linear algebra.Table of ContentsGiving Computers the Ability to Learn from DataTraining Simple Machine Learning Algorithms for ClassificationA Tour of Machine Learning Classifiers Using Scikit-LearnBuilding Good Training Datasets - Data PreprocessingCompressing Data via Dimensionality ReductionLearning Best Practices for Model Evaluation and Hyperparameter TuningCombining Different Models for Ensemble LearningApplying Machine Learning to Sentiment AnalysisPredicting Continuous Target Variables with Regression AnalysisWorking with Unlabeled Data - Clustering Analysis(N.B. Please use the Look Inside option to see further chapters)},
  isbn = {978-1-80181-931-2},
  langid = {english},
  pagetotal = {774}
}

@book{Macroeconomia_2017_Blanchard,
  title = {Macroeconomia},
  author = {Blanchard, Olivier},
  date = {2017},
  edition = {7ª edição},
  publisher = {Pearson Universidades},
  abstract = {Obra de referência no assunto, este livro apresenta uma visão global e unificada da macroeconomia, auxiliando os estudantes a entenderem as conexões entre os mercados financeiros, de bens de consumo e de trabalho em todo o mundo. Quadros detalhados integrados a esta sétima edição foram atualizados para exemplificar a macroeconomia de hoje, reforçar as lições dos modelos e ensinar os estudantes a empregar suas habilidades analíticas. Organizado em duas partes, o livro possui uma seção central, que foca em mercados de curto, médio e longo prazos, e três extensões principais, que oferecem uma cobertura aprofundada dos assuntos mais relevantes. Desse modo, o livro ajuda o aluno a compreender eventos macroeconômicos atuais ― desde a crise econômica e a política monetária dos Estados Unidos até os problemas relacionados à zona do Euro e o crescimento da China ―, mas também eventos que poderão se desdobrar no futuro, tornando-se ideal para os estudantes de economia, administração e ciências contábeis.},
  isbn = {978-85-430-2054-9},
  langid = {portuguese}
}

@dataset{MacroEconomicIndicators__,
  title = {Macro-{{Economic Indicators}}},
  author = {given=FAO, given-i=FAO},
  url = {https://www.fao.org/faostat/en/#data/MK},
  urldate = {2023-12-01},
  file = {/home/baldoinov/Zotero/storage/HVLGC7WC/en.html}
}

@book{MacroeconomicPatternsStories_2009_Leamer,
  title = {Macroeconomic {{Patterns}} and {{Stories}}},
  author = {Leamer, Edward E.},
  date = {2009},
  publisher = {Springer},
  location = {Berlin},
  isbn = {978-3-540-46388-7 978-3-540-46389-4},
  langid = {english},
  pagetotal = {359},
  keywords = {Case studies,Economic conditions,Macroeconomics,Recessions,United States}
}

@online{MacroF1Macro_2021_OpitzBurst,
  title = {Macro {{F1}} and {{Macro F1}}},
  author = {Opitz, Juri and Burst, Sebastian},
  date = {2021-02-08},
  eprint = {1911.03347},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1911.03347},
  url = {http://arxiv.org/abs/1911.03347},
  urldate = {2023-10-03},
  abstract = {The 'macro F1' metric is frequently used to evaluate binary, multi-class and multi-label classification problems. Yet, we find that there exist two different formulas to calculate this quantity. In this note, we show that only under rare circumstances the two computations can be considered equivalent. More specifically, one formula well 'rewards' classifiers which produce a skewed error type distribution. In fact, the difference in outcome of the two computations can be as high as 0.5. The two computations may not only diverge in their scalar result but can also lead to different classifier rankings.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,notion,Statistics - Machine Learning},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/projeto-heiwa/Macro F1 and Macro F1_2021_Opitz et al.pdf;/home/baldoinov/Zotero/storage/4LY3DDZB/1911.html}
}

@book{MatematicaFinanceira_2010_Samanez,
  title = {Matemática Financeira},
  author = {Samanez, Carlos Patrício},
  date = {2010-08-03},
  publisher = {Prentice Hall/Sp},
  isbn = {978-85-7605-799-4},
  langid = {portuguese},
  keywords = {Assunto não informado}
}

@book{MathematicsMachineLearning__DeisenrothEtAl,
  title = {Mathematics for {{Machine Learning}}},
  author = {Deisenroth, Marc Peter and Faisal, A Aldo and Ong, Cheng Soon},
  langid = {english},
  file = {/home/baldoinov/baldoinov/PDFs/Livros/Mathematics for Machine Learning_Deisenroth et al.pdf}
}

@article{MatrixFactorizationTechniques_2009_KorenEtAl,
  title = {Matrix {{Factorization Techniques}} for {{Recommender Systems}}},
  author = {Koren, Yehuda and Bell, Robert and Volinsky, Chris},
  date = {2009-08},
  journaltitle = {Computer},
  volume = {42},
  number = {8},
  pages = {30--37},
  issn = {1558-0814},
  doi = {10.1109/MC.2009.263},
  url = {https://ieeexplore.ieee.org/document/5197422/?arnumber=5197422},
  urldate = {2024-08-16},
  abstract = {As the Netflix Prize competition has demonstrated, matrix factorization models are superior to classic nearest neighbor techniques for producing product recommendations, allowing the incorporation of additional information such as implicit feedback, temporal effects, and confidence levels.},
  eventtitle = {Computer},
  keywords = {Bioinformatics,Collaboration,Computational intelligence,Filtering,Genomics,Matrix factorization,Motion pictures,Nearest neighbor searches,Netflix Prize,Predictive models,Recommender systems,Sea measurements},
  file = {/home/baldoinov/baldoinov/PDFs/Mackenzie/projeto-aplicado-iii/Matrix Factorization Techniques for Recommender Systems_2009_Koren et al.pdf;/home/baldoinov/Zotero/storage/Y96LWS5H/5197422.html}
}

@book{MatrizAfricanaNo_2021_Nascimento,
  title = {A Matriz Africana No Mundo},
  editor = {Nascimento, Elisa Larkin},
  date = {2021-09-16},
  publisher = {Selo Negro Edições},
  isbn = {978-85-87478-32-0},
  langid = {brazilian}
}

@online{MeasureIntelligence_2019_Chollet,
  title = {On the {{Measure}} of {{Intelligence}}},
  author = {Chollet, François},
  date = {2019-11-25},
  eprint = {1911.01547},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1911.01547},
  url = {http://arxiv.org/abs/1911.01547},
  urldate = {2023-09-28},
  abstract = {To make deliberate progress towards more intelligent and more human-like artificial systems, we need to be following an appropriate feedback signal: we need to be able to define and evaluate intelligence in a way that enables comparisons between two systems, as well as comparisons with humans. Over the past hundred years, there has been an abundance of attempts to define and measure intelligence, across both the fields of psychology and AI. We summarize and critically assess these definitions and evaluation approaches, while making apparent the two historical conceptions of intelligence that have implicitly guided them. We note that in practice, the contemporary AI community still gravitates towards benchmarking intelligence by comparing the skill exhibited by AIs and humans at specific tasks such as board games and video games. We argue that solely measuring skill at any given task falls short of measuring intelligence, because skill is heavily modulated by prior knowledge and experience: unlimited priors or unlimited training data allow experimenters to "buy" arbitrary levels of skills for a system, in a way that masks the system's own generalization power. We then articulate a new formal definition of intelligence based on Algorithmic Information Theory, describing intelligence as skill-acquisition efficiency and highlighting the concepts of scope, generalization difficulty, priors, and experience. Using this definition, we propose a set of guidelines for what a general AI benchmark should look like. Finally, we present a benchmark closely following these guidelines, the Abstraction and Reasoning Corpus (ARC), built upon an explicit set of priors designed to be as close as possible to innate human priors. We argue that ARC can be used to measure a human-like form of general fluid intelligence and that it enables fair general intelligence comparisons between AI systems and humans.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,notion},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/On the Measure of Intelligence_2019_Chollet.pdf;/home/baldoinov/Zotero/storage/W9KIEJXK/1911.html}
}

@article{MeasurementDeadweightLoss_1981_Diewert,
  title = {The {{Measurement}} of {{Deadweight Loss Revisited}}},
  author = {Diewert, W. E.},
  date = {1981},
  journaltitle = {Econometrica},
  volume = {49},
  number = {5},
  eprint = {1912752},
  eprinttype = {jstor},
  pages = {1225--1244},
  publisher = {[Wiley, Econometric Society]},
  issn = {0012-9682},
  doi = {10.2307/1912752},
  url = {https://www.jstor.org/stable/1912752},
  urldate = {2023-12-07},
  abstract = {The paper studies an economy with H households, N + 1 commodities, and M fixed factors with commodity taxes and government expenditures on goods and services. The first and second order directional derivatives of a certain weighted sum of utility functions with respect to any direction of tax change are calculated. The resulting measure of deadweight loss, due essentially to Boiteux, is contrasted with a measure based on Debreu's coefficient of resource utilization as well as with the more familiar measure of loss due to Hotelling and Harberger. The basic analytical technique used is the usual comparative statistics analysis, except that duality theory is used to simplify the computations.},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/The Measurement of Deadweight Loss Revisited_1981_Diewert.pdf}
}

@article{MeasurementDeadweightLoss_1984_Diewert,
  title = {The {{Measurement}} of {{Deadweight Loss}} in an {{Open Economy}}},
  author = {Diewert, W. E.},
  date = {1984},
  journaltitle = {Economica},
  volume = {51},
  number = {201},
  eprint = {2553932},
  eprinttype = {jstor},
  pages = {23--42},
  publisher = {{[London School of Economics, Wiley, London School of Economics and Political Science, Suntory and Toyota International Centres for Economics and Related Disciplines]}},
  issn = {0013-0427},
  doi = {10.2307/2553932},
  url = {https://www.jstor.org/stable/2553932},
  urldate = {2023-12-07},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/The Measurement of Deadweight Loss in an Open Economy_1984_Diewert.pdf}
}

@article{MeasurementWaste_1964_Harberger,
  title = {The {{Measurement}} of {{Waste}}},
  author = {Harberger, Arnold C.},
  date = {1964},
  journaltitle = {The American Economic Review},
  volume = {54},
  number = {3},
  eprint = {1818490},
  eprinttype = {jstor},
  pages = {58--76},
  publisher = {American Economic Association},
  issn = {0002-8282},
  url = {https://www.jstor.org/stable/1818490},
  urldate = {2023-09-28},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/The Measurement of Waste_1964_Harberger.pdf}
}

@online{MetricsMultiClassClassification_2020_GrandiniEtAl,
  title = {Metrics for {{Multi-Class Classification}}: An {{Overview}}},
  shorttitle = {Metrics for {{Multi-Class Classification}}},
  author = {Grandini, Margherita and Bagli, Enrico and Visani, Giorgio},
  date = {2020-08-13},
  eprint = {2008.05756},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2008.05756},
  urldate = {2024-03-31},
  abstract = {Classification tasks in machine learning involving more than two classes are known by the name of "multi-class classification". Performance indicators are very useful when the aim is to evaluate and compare different classification models or machine learning techniques. Many metrics come in handy to test the ability of a multi-class classifier. Those metrics turn out to be useful at different stage of the development process, e.g. comparing the performance of two different models or analysing the behaviour of the same model by tuning different parameters. In this white paper we review a list of the most promising multi-class metrics, we highlight their advantages and disadvantages and show their possible usages during the development of a classification model.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Metrics for Multi-Class Classification_2020_Grandini et al.pdf;/home/baldoinov/Zotero/storage/JKGVT3NE/2008.html}
}

@book{MicroeconometricsMethodsApplications_2005_CameronTrivedi,
  title = {Microeconometrics: {{Methods}} and {{Applications}}},
  shorttitle = {Microeconometrics},
  author = {Cameron, Adrian Colin and Trivedi, Pravin K.},
  date = {2005},
  edition = {13th printing},
  publisher = {Cambridge University Press},
  location = {Cambridge},
  isbn = {978-0-521-84805-3},
  langid = {english},
  pagetotal = {1034}
}

@book{Microeconomia_2013_PindyckRubinfeld,
  title = {Microeconomia},
  author = {Pindyck, Robert S. and Rubinfeld, Daniel L.},
  date = {2013},
  edition = {8ª edição},
  publisher = {Pearson Universidades},
  abstract = {O contexto dos principais acontecimentos do cenário econômico mundial é a base utilizada por Microeconomia, de Robert Pindyck e Daniel Rubinfeld, para abordar assuntos como análise de demanda, custo, formulação de estratégias de preços e análise da política pública, a fim de aproximar ainda mais o estudante à realidade do tema. Com conteúdo revisado e atualizado, a oitava edição mantém sua tradição didática, que permite o estudo e a aprendizagem progressiva sobre microeconomia, com destaque para a aplicação prática do conteúdo na forma de exercícios e de um maior número de exemplos. Dessa maneira, a obra pode ser adotada em diferentes cursos, tornando-o flexível e compatível com as necessidades de futuros gestores e economistas.},
  isbn = {978-85-430-0028-2},
  langid = {portuguese}
}

@book{Microeconomia_2015_Varian,
  title = {Microeconomia},
  author = {Varian, Hal R.},
  date = {2015-03-30},
  publisher = {Elsevier},
  isbn = {978-85-352-5143-2},
  langid = {portuguese},
  keywords = {Administração e serviços auxiliares}
}

@article{MicroeconomiaReducionistaMicroeconomia_2006_Prado,
  title = {Microeconomia reducionista e microeconomia sistêmica},
  author = {Prado, Eleutério},
  date = {2006},
  journaltitle = {Nova Economia},
  volume = {16},
  number = {2},
  issn = {1980-5381},
  url = {https://revistas.face.ufmg.br/index.php/novaeconomia/article/view/470},
  urldate = {2024-06-12},
  abstract = {This paper argues that another microeconomics, methodologically different from neoclassical~ reductionism, is rapidly developing: systemic andevolutionary microeconomics. The former is based on clear-sighted agents that have access to information but do not really learn, and always proceed with substantive rationality. The latter is based on partially blind agents who learn and act rationally in an adaptive way. The study demonstrates that the crux of the difference between these two alternatives is the way theyconnect the parts among themselves and the parts to the whole. In the former, the agents are independent of each other and global properties are obtained by aggregation. In the latter, the agents are organized by social structures and~ they form compositions that have emergent properties. Therefore, the parts and the whole belong to each other; that is, they are inseparable. In the former, the agents are defined only by their intrinsic properties, but in the latter, they are also defined by their relations.},
  issue = {2},
  langid = {portuguese},
  keywords = {microeconomia evolucionária,microeconomia sistêmica,racionalidade adaptativa},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Microeconomia reducionista e microeconomia sistemica_2006_Prado.pdf}
}

@book{MicroeconomicAnalysis_1992_Varian,
  title = {Microeconomic Analysis},
  author = {Varian, Hal R.},
  date = {1992-03-17},
  edition = {3rd ed. edição},
  publisher = {W. W. Norton \& Company},
  location = {New York},
  abstract = {Microeconomic Analysis has been a fixture of graduate programs in economics for fifteen years, providing unique authority, clarity, and breadth of coverage. The Third Edition continues to supply the building blocks of microeconomic analysis: a thorough treatment of optimization and equilibrium methods, coupled with numerous examples of their application. The Third Edition expands on the earlier editions in two ways. First, the coverage has been rewritten and rearranged. Second, chapters have been added on game theory, oligopoly, asset markets, and information economics. The new chapters fully update the text, highlighting significant developments of the last decade at a level that is accessible for first-year graduate students.},
  isbn = {978-0-393-95735-8},
  langid = {Inglês}
}

@book{MicroeconomicTheoryBasic_2016_NicholsonSnyder,
  title = {Microeconomic Theory: Basic Principles and Extensions},
  shorttitle = {Microeconomic Theory},
  author = {Nicholson, Walter and Snyder, Christopher M.},
  date = {2016-09-13},
  edition = {11},
  publisher = {Cengage Learning},
  location = {Australia ; Boston, MA},
  abstract = {Now you can truly understand and apply the latest economic models as you work directly with theoretical tools, real-world applications, and the popular new behavioral economics in this reader-friendly, market-leading book. MICROECONOMIC THEORY: BASIC PRINCIPLES AND EXTENSIONS, 12E takes a calculus-based approach to provide the ideal level of mathematical rigor, whether you are an upper-level undergraduate or beginning graduate student. Insightful graphic presentations help you visually grasp the connections between the calculus and the algebraic and geometric approach to the same material. End-of-chapter problems present simple numerical/mathematical exercises, which strengthen your microeconomic intuition and are followed by more analytical, theoretical, behavioral, and complex problems. Unlike other more theoretical texts, MICROECONOMIC THEORY, 12E closely connects all theory to real applications in the world today.},
  isbn = {978-1-305-50579-7},
  langid = {Inglês},
  keywords = {notion}
}

@online{MicroVsMacro_2022_,
  title = {Micro vs {{Macro F1}} Score, What’s the Difference?},
  date = {2022-07-20T19:56:10},
  url = {https://stephenallwright.com/micro-vs-macro-f1-score/},
  urldate = {2023-10-06},
  abstract = {Micro average and macro average are aggregation methods for F1 score, a metric which is used to measure the performance of classification machine learning models. They are often shown together, which can make it confusing to know what the difference is.},
  langid = {english},
  organization = {Stephen Allwright},
  keywords = {notion},
  file = {/home/baldoinov/Zotero/storage/3YB5U9I5/micro-vs-macro-f1-score.html}
}

@inproceedings{MineracaoArgumentosEm_2024_SantosEtAl,
  title = {Mineração de Argumentos em Textos de Redes Sociais no Idioma Português},
  booktitle = {Simpósio Brasileiro de Tecnologia da Informação e da Linguagem Humana (STIL)},
  author = {family=Santos, given=Vitor Domingos Baldoino, prefix=do, useprefix=false and family=Santos, given=Livia Alabarse, prefix=dos, useprefix=false and Coelho, Orlando B. and family=Araujo, given=Renata Mendes, prefix=de, useprefix=false and family=Oliveira, given=Ivan Carlos Alcântara, prefix=de, useprefix=false},
  date = {2024-11-17},
  pages = {306--316},
  publisher = {SBC},
  issn = {0000-0000},
  doi = {10.5753/stil.2024.245433},
  url = {https://sol.sbc.org.br/index.php/stil/article/view/31143},
  urldate = {2024-11-21},
  abstract = {Este artigo apresenta os desafios e os avanços de pesquisa voltada à construção de soluções computacionais capazes de apoiar o entendimento do debate em redes sociais no idioma português. Uma das bases fundamentais dessas soluções é a aplicação de técnicas de Mineração de Argumentos. Apresentamos as estratégias utilizadas para o endereçamento de desafios da mineração de argumentos em redes sociais, em particular, o uso de deep learning. Os resultados obtidos demonstram boa eficácia dos modelos selecionados para as tarefas consideradas, tendo atingido um F1-Score de 0,85 para a análise de sentimento, 0,97 na detecção de posição e 0,76 na detecção de ironia.},
  eventtitle = {Simpósio Brasileiro de Tecnologia da Informação e da Linguagem Humana (STIL)},
  langid = {portuguese},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/projeto-heiwa/Mineracao de Argumentos em Textos de Redes Sociais no Idioma Portugues_2024_Santos et al.pdf}
}

@legislation{MinisterioSaude_1997_Brasil,
  title = {Ministério Da {{Saúde}}},
  namea = {{Brasil}},
  nameatype = {collaborator},
  date = {1997-01-06},
  journaltitle = {Diário Oficial da União},
  number = {Lei nº 9.431, de 6 de janeiro de 1997.},
  pages = {265},
  url = {http://www.planalto.gov.br/ccivil_03/leis/l9431.htm},
  urldate = {2020-04-23},
  abstract = {Dispõe sobre a obrigatoriedade da manutenção de programa de controle de infecções hospitalares pelos hospitais do País.},
  annotation = {Brasília, DF}
}

@legislation{MinisterioSaude_2004_Brasil,
  title = {Ministério Da {{Saúde}}},
  namea = {{Brasil}},
  nameatype = {collaborator},
  date = {2004-09-16},
  journaltitle = {Diário Oficial da União},
  number = {Resolução RDC nº 216, de 15 de setembro de 2004},
  pages = {25},
  url = {http://portal.anvisa.gov.br/documents/33916/388704/RESOLU%25C3%2587%25C3%2583O-RDC%2BN%2B216%2BDE%2B15%2BDE%2BSETEMBRO%2BDE%2B2004.pdf/23701496-925d-4d4d-99aa-9d479b316c4b},
  urldate = {2020-04-23},
  abstract = {Dispõe sobre Regulamento Técnico de Boas Práticas para Serviços de Alimentação.},
  annotation = {Brasília, DF}
}

@legislation{MinisterioSaude_2007_Brasil,
  title = {Ministério Da {{Saúde}}},
  namea = {{Brasil}},
  nameatype = {collaborator},
  date = {2007-02-14},
  journaltitle = {Diário Oficial da União},
  number = {Resolução Normativa nº 147, de 14 de fevereiro de 2007.},
  pages = {131},
  abstract = {Altera a Resolução Normativa - RN nº 136, de 01º de novembro de 2006.},
  annotation = {Brasília, DF}
}

@legislation{MinisterioSaude_2011_Brasil,
  title = {Ministério Da {{Saúde}}},
  namea = {{Brasil}},
  nameatype = {collaborator},
  date = {2011-12-12},
  journaltitle = {Diário Oficial da União},
  number = {Portaria nº 2.914, de 12 de dezembro de 2011.},
  pages = {39},
  url = {https://bvsms.saude.gov.br/bvs/saudelegis/gm/2011/prt2914_12_12_2011.html},
  urldate = {2020-04-23},
  abstract = {Dispõe sobre os procedimentos de controle e de vigilância da qualidade da água para consumo humano e seu padrão de potabilidade.},
  annotation = {Brasília, DF}
}

@legislation{MinisterioSaude_2017_Brasil,
  title = {Ministério Da {{Saúde}}},
  namea = {{Brasil}},
  nameatype = {collaborator},
  date = {2017-09-21},
  journaltitle = {Diário Oficial da União},
  number = {Portaria nº 2.436, de 21 de setembro de 2017.},
  pages = {68},
  url = {https://bvsms.saude.gov.br/bvs/saudelegis/gm/2017/prt2436_22_09_2017.html},
  urldate = {2020-04-23},
  abstract = {Aprova a Política Nacional de Atenção Básica, estabelecendo a revisão de diretrizes para a organização da Atenção Básica, no âmbito do Sistema Único de Saúde (SUS).},
  annotation = {Brasília, DF}
}

@online{MitigatingColdstartForecasting_2023_FatemiEtAl,
  title = {Mitigating {{Cold-start Forecasting}} Using {{Cold Causal Demand Forecasting Model}}},
  author = {Fatemi, Zahra and Huynh, Minh and Zheleva, Elena and Syed, Zamir and Di, Xiaojun},
  date = {2023-06-15},
  eprint = {2306.09261},
  eprinttype = {arXiv},
  url = {http://arxiv.org/abs/2306.09261},
  urldate = {2024-10-11},
  abstract = {Forecasting multivariate time series data, which involves predicting future values of variables over time using historical data, has significant practical applications. Although deep learning-based models have shown promise in this field, they often fail to capture the causal relationship between dependent variables, leading to less accurate forecasts. Additionally, these models cannot handle the cold-start problem in time series data, where certain variables lack historical data, posing challenges in identifying dependencies among variables. To address these limitations, we introduce the Cold Causal Demand Forecasting (CDF-cold) framework that integrates causal inference with deep learning-based models to enhance the forecasting accuracy of multivariate time series data affected by the cold-start problem. To validate the effectiveness of the proposed approach, we collect 15 multivariate time-series datasets containing the network traffic of different Google data centers. Our experiments demonstrate that the CDF-cold framework outperforms state-of-the-art forecasting models in predicting future values of multivariate time series data.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Recovery/Modelo de Incidencia de Acao Contra/Mitigating Cold-start Forecasting using Cold Causal Demand Forecasting Model_2023_Fatemi et al.pdf;/home/baldoinov/Zotero/storage/5872HAD9/2306.html}
}

@online{MLGymNewFramework_2025_NathaniEtAl,
  title = {{{MLGym}}: {{A New Framework}} and {{Benchmark}} for {{Advancing AI Research Agents}}},
  shorttitle = {{{MLGym}}},
  author = {Nathani, Deepak and Madaan, Lovish and Roberts, Nicholas and Bashlykov, Nikolay and Menon, Ajay and Moens, Vincent and Budhiraja, Amar and Magka, Despoina and Vorotilov, Vladislav and Chaurasia, Gaurav and Hupkes, Dieuwke and Cabral, Ricardo Silveira and Shavrina, Tatiana and Foerster, Jakob and Bachrach, Yoram and Wang, William Yang and Raileanu, Roberta},
  date = {2025-02-20},
  eprint = {2502.14499},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2502.14499},
  url = {http://arxiv.org/abs/2502.14499},
  urldate = {2025-06-29},
  abstract = {We introduce Meta MLGym and MLGym-Bench, a new framework and benchmark for evaluating and developing LLM agents on AI research tasks. This is the first Gym environment for machine learning (ML) tasks, enabling research on reinforcement learning (RL) algorithms for training such agents. MLGym-bench consists of 13 diverse and open-ended AI research tasks from diverse domains such as computer vision, natural language processing, reinforcement learning, and game theory. Solving these tasks requires real-world AI research skills such as generating new ideas and hypotheses, creating and processing data, implementing ML methods, training models, running experiments, analyzing the results, and iterating through this process to improve on a given task. We evaluate a number of frontier large language models (LLMs) on our benchmarks such as Claude-3.5-Sonnet, Llama-3.1 405B, GPT-4o, o1-preview, and Gemini-1.5 Pro. Our MLGym framework makes it easy to add new tasks, integrate and evaluate models or agents, generate synthetic data at scale, as well as develop new learning algorithms for training agents on AI research tasks. We find that current frontier models can improve on the given baselines, usually by finding better hyperparameters, but do not generate novel hypotheses, algorithms, architectures, or substantial improvements. We open-source our framework and benchmark to facilitate future research in advancing the AI research capabilities of LLM agents.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/baldoinov/baldoinov/PDFs/Mestrado/MLGym_2025_Nathani et al.pdf;/home/baldoinov/Zotero/storage/8WZQCINV/2502.html}
}

@book{MobiliarioParaDesign_2015_Booth,
  title = {Mobiliário para o design de interiores},
  author = {Booth, Sam},
  namea = {Plunkett, Drew},
  nameatype = {collaborator},
  date = {2015-11-30},
  publisher = {Gustavo Gili},
  isbn = {978-85-8452-056-5},
  langid = {portuguese},
  keywords = {Arquitetura}
}

@online{ModelbasedReinforcementLearning_2022_MoerlandEtAl,
  title = {Model-Based {{Reinforcement Learning}}: {{A Survey}}},
  shorttitle = {Model-Based {{Reinforcement Learning}}},
  author = {Moerland, Thomas M. and Broekens, Joost and Plaat, Aske and Jonker, Catholijn M.},
  date = {2022-03-31},
  eprint = {2006.16712},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2006.16712},
  url = {http://arxiv.org/abs/2006.16712},
  urldate = {2024-03-14},
  abstract = {Sequential decision making, commonly formalized as Markov Decision Process (MDP) optimization, is a important challenge in artificial intelligence. Two key approaches to this problem are reinforcement learning (RL) and planning. This paper presents a survey of the integration of both fields, better known as model-based reinforcement learning. Model-based RL has two main steps. First, we systematically cover approaches to dynamics model learning, including challenges like dealing with stochasticity, uncertainty, partial observability, and temporal abstraction. Second, we present a systematic categorization of planning-learning integration, including aspects like: where to start planning, what budgets to allocate to planning and real data collection, how to plan, and how to integrate planning in the learning and acting loop. After these two sections, we also discuss implicit model-based RL as an end-to-end alternative for model learning and planning, and we cover the potential benefits of model-based RL. Along the way, the survey also draws connections to several related RL fields, like hierarchical RL and transfer learning. Altogether, the survey presents a broad conceptual overview of the combination of planning and learning for MDP optimization.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Model-based Reinforcement Learning_2022_Moerland et al.pdf;/home/baldoinov/Zotero/storage/L2Z4J7MV/2006.html}
}

@article{ModelbasedReinforcementLearning_2023_MoerlandEtAl,
  title = {Model-Based {{Reinforcement Learning}}: {{A Survey}}},
  shorttitle = {Model-Based {{Reinforcement Learning}}},
  author = {Moerland, Thomas M. and Broekens, Joost and Plaat, Aske and Jonker, Catholijn M.},
  date = {2023-01-03},
  journaltitle = {Foundations and Trends® in Machine Learning},
  shortjournal = {MAL},
  volume = {16},
  number = {1},
  pages = {1--118},
  publisher = {Now Publishers, Inc.},
  issn = {1935-8237, 1935-8245},
  doi = {10.1561/2200000086},
  url = {https://www.nowpublishers.com/article/Details/MAL-086},
  urldate = {2023-12-05},
  abstract = {Model-based Reinforcement Learning: A Survey},
  langid = {english},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Model-based Reinforcement Learning_2023_Moerland et al.pdf}
}

@online{ModelEvaluationModel_2020_Raschka,
  title = {Model {{Evaluation}}, {{Model Selection}}, and {{Algorithm Selection}} in {{Machine Learning}}},
  author = {Raschka, Sebastian},
  date = {2020-11-10},
  eprint = {1811.12808},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1811.12808},
  urldate = {2024-02-26},
  abstract = {The correct use of model evaluation, model selection, and algorithm selection techniques is vital in academic machine learning research as well as in many industrial settings. This article reviews different techniques that can be used for each of these three subtasks and discusses the main advantages and disadvantages of each technique with references to theoretical and empirical studies. Further, recommendations are given to encourage best yet feasible practices in research and applications of machine learning. Common methods such as the holdout method for model evaluation and selection are covered, which are not recommended when working with small datasets. Different flavors of the bootstrap technique are introduced for estimating the uncertainty of performance estimates, as an alternative to confidence intervals via normal approximation if bootstrapping is computationally feasible. Common cross-validation techniques such as leave-one-out cross-validation and k-fold cross-validation are reviewed, the bias-variance trade-off for choosing k is discussed, and practical tips for the optimal choice of k are given based on empirical evidence. Different statistical tests for algorithm comparisons are presented, and strategies for dealing with multiple comparisons such as omnibus tests and multiple-comparison corrections are discussed. Finally, alternative methods for algorithm selection, such as the combined F-test 5x2 cross-validation and nested cross-validation, are recommended for comparing machine learning algorithms when datasets are small.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,notion,Statistics - Machine Learning},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/machine-learning-w-pytorch-n-sklearn/Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning_2020_Raschka.pdf;/home/baldoinov/Zotero/storage/X49AKS5J/1811.html}
}

@inproceedings{ModelingAgentBehaviors_2020_OsobaEtAl,
  title = {Modeling {{Agent Behaviors}} for {{Policy Analysis}} via {{Reinforcement Learning}}},
  booktitle = {2020 19th {{IEEE International Conference}} on {{Machine Learning}} and {{Applications}} ({{ICMLA}})},
  author = {Osoba, Osonde A. and Vardavas, Raffaele and Grana, Justin and Zutshi, Rushil and Jaycocks, Amber},
  date = {2020-12},
  pages = {213--219},
  doi = {10.1109/ICMLA51294.2020.00043},
  abstract = {Agent-based Models (ABMs) are valuable tools for policy analysis. ABMs help analysts explore the emergent consequences of regulatory and policy interventions in multi-agent decision-making settings. But the validity of inferences drawn from ABM explorations depends on the quality of the ABM agents' behavioral models. Prior approaches for specifying behaviors have limitations. This paper examines the value of reinforcement learning (RL) models as adaptive, high-performing, and behaviorally-valid models of agent decision-making in ABMs. We discuss the value of RL for modeling agents' utility-maximizing behaviors in policy-relevant ABMs. We address the problem of adapting RL algorithms to handle multi-agency in games by adapting and extending methods from recent literature. We evaluate examples of such RL-based ABM agents via experiments on two policy-relevant ABMs: a Minority Game ABM, and an ABM of Influenza Transmission. The RL behavioral models can outperform the default adaptive behavioral models. We also run analytic experiments on our RL-equipped ABMs: explorations of the effects of dynamic behavioral heterogeneity in a population, the impact of social network factors on adaptability, and the emergence of synchronization in a community. Our results suggest that the RL formalism can be an efficient abstraction for behavioral models in ABMs.},
  eventtitle = {2020 19th {{IEEE International Conference}} on {{Machine Learning}} and {{Applications}} ({{ICMLA}})},
  keywords = {actor-critic models,Adaptation models,agent-based models,Analytical models,Decision making,Machine learning algorithms,Multi-agent learning,notion,policy optimization,reinforcement learning,Reinforcement learning,Synchronization,Tools},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Modeling Agent Behaviors for Policy Analysis via Reinforcement Learning_2020_Osoba et al.pdf;/home/baldoinov/Zotero/storage/MMBESXWK/9356182.html}
}

@article{ModelingAgentDecision_2023_AnEtAl,
  title = {Modeling Agent Decision and Behavior in the Light of Data Science and Artificial Intelligence},
  author = {An, Li and Grimm, Volker and Bai, Yu and Sullivan, Abigail and Turner, B. L. and Malleson, Nicolas and Heppenstall, Alison and Vincenot, Christian and Robinson, Derek and Ye, Xinyue and Liu, Jianguo and Lindkvist, Emilie and Tang, Wenwu},
  date = {2023-08-01},
  journaltitle = {Environmental Modelling \& Software},
  shortjournal = {Environmental Modelling \& Software},
  volume = {166},
  pages = {105713},
  issn = {1364-8152},
  doi = {10.1016/j.envsoft.2023.105713},
  url = {https://www.sciencedirect.com/science/article/pii/S1364815223000993},
  urldate = {2023-08-19},
  abstract = {Agent-based modeling (ABM) has been widely used in numerous disciplines and practice domains, subject to many eulogies and criticisms. This article presents key advances and challenges in agent-based modeling over the last two decades and shows that understanding agents’ behaviors is a major priority for various research fields. We demonstrate that artificial intelligence and data science will likely generate revolutionary impacts for science and technology towards understanding agent decisions and behaviors in complex systems. We propose an innovative approach that leverages reinforcement learning and convolutional neural networks to equip agents with the intelligence of self-learning their behavior rules directly from data. We call for further developments of ABM, especially modeling agent behaviors, in the light of data science and artificial intelligence.},
  keywords = {Agent-based modeling,Artificial intelligence,Data science,Machine learning,Modeling agent decisions and actions,notion},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Modeling agent decision and behavior in the light of data science and_2023_An et al.pdf;/home/baldoinov/Zotero/storage/RSMUX2YL/S1364815223000993.html}
}

@article{ModelingEconomicSystems_2017_Tesfatsion,
  title = {Modeling {{Economic Systems}} as {{Locally-Constructive Sequential Games}}},
  author = {Tesfatsion, Leigh},
  date = {2017-07-11},
  url = {https://dr.lib.iastate.edu/handle/20.500.12876/22642},
  urldate = {2024-05-29},
  abstract = {{$<$}p{$>$}Real-world economies are open-ended dynamic systems consisting of heterogeneous interacting participants. Human participants are decision-makers who strategically take into account the past actions and potential future actions of other participants. All participants are forced to be locally constructive, meaning their actions at any given time must be based on their local states; and participant actions at any given time affect future local states. Taken together, these properties imply real-world economies are locally-constructive sequential games. This study discusses a modeling approach, agent-based computational economics (ACE), that permits researchers to study economic systems from this point of view. ACE modeling principles and objectives are first concisely presented. The remainder of the study then highlights challenging issues and edgier explorations that ACE researchers are currently pursuing.{$<$}/p{$>$}},
  langid = {english},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Modeling Economic Systems as Locally-Constructive Sequential Games_2017_Tesfatsion.pdf}
}

@inproceedings{ModelingInterAttack_2022_HsiaoEtAl,
  title = {Modeling {{Inter Round Attack}} of {{Online Debaters}} for {{Winner Prediction}}},
  booktitle = {Proceedings of the {{ACM Web Conference}} 2022},
  author = {Hsiao, Fa-Hsuan and Yen, An-Zi and Huang, Hen-Hsen and Chen, Hsin-Hsi},
  date = {2022-04-25},
  series = {{{WWW}} '22},
  pages = {2860--2869},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3485447.3512006},
  url = {https://doi.org/10.1145/3485447.3512006},
  urldate = {2023-12-14},
  abstract = {In a debate, two debaters with opposite stances put forward arguments to fight for their viewpoints. Debaters organize their arguments to support their proposition and attack opponents’ points. The common purpose of debating is to persuade the opponents and the audiences to agree with the mentioned propositions. Previous works have investigated the issue of identifying which debater is more persuasive. However, modeling the interaction of arguments between rounds is rarely discussed. In this paper, we focus on assessing the overall performance of debaters in a multi-round debate on online forums. To predict the winner in a multi-round debate, we propose a novel neural model that is aimed at capturing the interaction of arguments by exploiting raw text, structure information, argumentative discourse units (ADUs), and the relations among ADUs. Experimental results show that our model achieves competitive performance compared with the existing models, and is capable of extracting essential argument relations during a multi-round debate by leveraging argumentative structure and attention mechanism.},
  isbn = {978-1-4503-9096-5},
  keywords = {Argument Mining,Debate Winner Prediction,Inter Round Attack Attention,notion}
}

@article{ModellingArgumentationShort_2022_LytosEtAl,
  title = {Modelling Argumentation in Short Text: {{A}} Case of Social Media Debate},
  shorttitle = {Modelling Argumentation in Short Text},
  author = {Lytos, Anastasios and Lagkas, Thomas and Sarigiannidis, Panagiotis and Argyriou, Vasileios and Eleftherakis, George},
  date = {2022-02-01},
  journaltitle = {Simulation Modelling Practice and Theory},
  shortjournal = {Simulation Modelling Practice and Theory},
  volume = {115},
  pages = {102446},
  issn = {1569-190X},
  doi = {10.1016/j.simpat.2021.102446},
  url = {https://www.sciencedirect.com/science/article/pii/S1569190X21001398},
  urldate = {2023-12-14},
  abstract = {The technological leaps of artificial intelligence (AI) and the rise of machine learning have triggered significant progress in a plethora of natural language processing (NLP) and natural language understanding tasks. One of these tasks is argumentation mining which has received significant interest in recent years and is regarded as a key domain for future decision-making systems, behaviour modelling, and natural language understanding problems. Until recently, natural language modelling tasks, such as computational argumentation schemes, were often tested in controlled environments, such as persuasive essays, reducing unexpected behaviours that could occur in real-life settings, like a public debate on social media. Additionally, the growing demand for enhancing the trust and the explainability of the AI services has dictated the design and adoption of modelling schemes to increase the confidence in the outcomes of the AI solutions. This paper attempts to explore modelling argumentation in short text and proposes a novel framework for argumentation detection under the name Abstract Framework for Argumentation Detection (AFAD). Moreover, different proof-of-concept implementations are provided to examine the applicability of the proposed framework to very short text developing a rule-based mechanism and compare the results with data-driven solutions. Eventually, a combination of the deployed methods is applied increasing the correct predictions in the minority class on an imbalanced dataset. The findings suggest that the modelling process provides solid grounds for technical research while the hybrid solutions have the potential to be applied to a wide range of NLP-related tasks offering a deeper understanding of human language and reasoning.},
  keywords = {Argumentation detection,Argumentation mining,Argumentation modelling,Computational linguistics,Natural language processing (NLP),notion,Semantic similarity,Social media,Symbolic artificial intelligence},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/projeto-heiwa-revisao-de-literatura/Modelling argumentation in short text_2022_Lytos et al.pdf}
}

@book{ModernArchitectureMediterranean_2010_Lejeune,
  title = {Modern Architecture and the {{Mediterranean}}: Vernacular Dialogues and Contested Identities},
  shorttitle = {Modern Architecture and the {{Mediterranean}}},
  editor = {Lejeune, Jean-François},
  date = {2010},
  edition = {1. publ},
  publisher = {Routledge},
  location = {London New York},
  isbn = {978-0-203-87190-4 978-0-415-77633-2 978-0-415-77634-9},
  langid = {english},
  pagetotal = {268}
}

@book{ModernIndustrialOrganization_2005_CarltonPerloff,
  title = {Modern {{Industrial Organization}}},
  author = {Carlton, Dennis W. and Perloff, Jeffrey M.},
  date = {2005},
  edition = {4. ed},
  publisher = {Pearson Addison Wesley},
  location = {Boston},
  isbn = {978-0-321-18023-0},
  langid = {english},
  pagetotal = {822}
}

@video{MontagemPregacaoExpositiva_2022_LopesGoncalves,
  entrysubtype = {video},
  title = {Montagem de Pregação Expositiva},
  editor = {Lopes, Hernandes Dias and Gonçalves, Douglas},
  editortype = {director},
  namea = {{Cortes JesusCopy}},
  nameatype = {collaborator},
  date = {2022-08-18},
  url = {https://www.youtube.com/watch?v=FCua90-7CU4},
  urldate = {2024-09-13},
  abstract = {EPISÓDIO COMPLETO: ~~~•~HERNANDES~DIAS~LOPES~-~Podcast~JesusC...~~ Nosso convidado no podcast de hoje é Hernandes Dias Lopes. Ele contou pra gente sua história de vida desde a infância até a juventude, nos falou sobre quais eram seus sonhos e planos, como começou sua jornada no ministério que hoje chegam há 40 anos e também compartilhou os milagres de Deus na sua vida.  Cortes em sua maioria com o propósito de edificação. São partes dos assuntos que rolaram no Podcast Jesuscopy, postados neste canal individualmente. Se gostarem, curtem e compartilhem. Direitos reservados ao Canal Jesuscopy:  @JesusCopy   \#hernandesdiaslopes\hspace{0pt} \#pregação \#cortesPodcast\hspace{0pt}}
}

@online{MoshiSpeechtextFoundation_2024_DefossezEtAl,
  title = {Moshi: A Speech-Text Foundation Model for Real-Time Dialogue},
  shorttitle = {Moshi},
  author = {Défossez, Alexandre and Mazaré, Laurent and Orsini, Manu and Royer, Amélie and Pérez, Patrick and Jégou, Hervé and Grave, Edouard and Zeghidour, Neil},
  date = {2024-10-02},
  eprint = {2410.00037},
  eprinttype = {arXiv},
  eprintclass = {eess},
  doi = {10.48550/arXiv.2410.00037},
  url = {http://arxiv.org/abs/2410.00037},
  urldate = {2025-08-23},
  abstract = {We introduce Moshi, a speech-text foundation model and full-duplex spoken dialogue framework. Current systems for spoken dialogue rely on pipelines of independent components, namely voice activity detection, speech recognition, textual dialogue and text-to-speech. Such frameworks cannot emulate the experience of real conversations. First, their complexity induces a latency of several seconds between interactions. Second, text being the intermediate modality for dialogue, non-linguistic information that modifies meaning -- such as emotion or non-speech sounds -- is lost in the interaction. Finally, they rely on a segmentation into speaker turns, which does not take into account overlapping speech, interruptions and interjections. Moshi solves these independent issues altogether by casting spoken dialogue as speech-to-speech generation. Starting from a text language model backbone, Moshi generates speech as tokens from the residual quantizer of a neural audio codec, while modeling separately its own speech and that of the user into parallel streams. This allows for the removal of explicit speaker turns, and the modeling of arbitrary conversational dynamics. We moreover extend the hierarchical semantic-to-acoustic token generation of previous work to first predict time-aligned text tokens as a prefix to audio tokens. Not only this "Inner Monologue" method significantly improves the linguistic quality of generated speech, but we also illustrate how it can provide streaming speech recognition and text-to-speech. Our resulting model is the first real-time full-duplex spoken large language model, with a theoretical latency of 160ms, 200ms in practice, and is available at https://github.com/kyutai-labs/moshi.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Moshi_2024_Defossez et al.pdf;/home/baldoinov/Zotero/storage/WZJPUDEJ/2410.html}
}

@book{MostlyHarmlessEconometrics_2009_AngristPischke,
  title = {Mostly Harmless Econometrics: An Empiricist's Companion},
  shorttitle = {Mostly Harmless Econometrics},
  author = {Angrist, Joshua David and Pischke, Jorn-Steffen},
  date = {2009-01-04},
  edition = {Illustrated edição},
  publisher = {Princeton University Press},
  location = {Princeton},
  abstract = {The core methods in today's econometric toolkit are linear regression for statistical control, instrumental variables methods for the analysis of natural experiments, and differences-in-differences methods that exploit policy changes. In the modern experimentalist paradigm, these techniques address clear causal questions such as: Do smaller classes increase learning? Should wife batterers be arrested? How much does education raise wages? Mostly Harmless Econometrics shows how the basic tools of applied econometrics allow the data to speak. In addition to econometric essentials, Mostly Harmless Econometrics covers important new extensions--regression-discontinuity designs and quantile regression--as well as how to get standard errors right. Joshua Angrist and Jörn-Steffen Pischke explain why fancier econometric techniques are typically unnecessary and even dangerous. The applied econometric methods emphasized in this book are easy to use and relevant for many areas of contemporary social science.An irreverent review of econometric essentials A focus on tools that applied researchers use most Chapters on regression-discontinuity designs, quantile regression, and standard errors Many empirical examples A clear and concise resource with wide applications},
  isbn = {978-0-691-12035-5},
  langid = {Inglês}
}

@article{MultiagentDeepReinforcement_2022_GronauerDiepold,
  title = {Multi-Agent Deep Reinforcement Learning: A Survey},
  shorttitle = {Multi-Agent Deep Reinforcement Learning},
  author = {Gronauer, Sven and Diepold, Klaus},
  date = {2022-02-01},
  journaltitle = {Artificial Intelligence Review},
  shortjournal = {Artif Intell Rev},
  volume = {55},
  number = {2},
  pages = {895--943},
  issn = {1573-7462},
  doi = {10.1007/s10462-021-09996-w},
  url = {https://doi.org/10.1007/s10462-021-09996-w},
  urldate = {2023-12-05},
  abstract = {The advances in reinforcement learning have recorded sublime success in various domains. Although the multi-agent domain has been overshadowed by its single-agent counterpart during this progress, multi-agent reinforcement learning gains rapid traction, and the latest accomplishments address problems with real-world complexity. This article provides an overview of the current developments in the field of multi-agent deep reinforcement learning. We focus primarily on literature from recent years that combines deep reinforcement learning methods with a multi-agent scenario. To survey the works that constitute the contemporary landscape, the main~contents are divided into three parts. First, we analyze the structure of training schemes that are applied to train multiple agents. Second, we consider the emergent patterns of agent behavior in cooperative, competitive and mixed scenarios. Third, we systematically enumerate challenges that exclusively arise in the multi-agent domain and review methods that are leveraged to cope with these challenges. To conclude this survey, we discuss advances, identify trends, and outline possible directions for future work in this research area.},
  langid = {english},
  keywords = {Deep learning,Machine learning,Multi-agent learning,Multi-agent systems,notion,Reinforcement learning,Survey},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Multi-agent deep reinforcement learning_2022_Gronauer et al.pdf}
}

@article{MultiAgentReinforcementLearning_2021_CaneseEtAl,
  title = {Multi-{{Agent Reinforcement Learning}}: {{A Review}} of {{Challenges}} and {{Applications}}},
  shorttitle = {Multi-{{Agent Reinforcement Learning}}},
  author = {Canese, Lorenzo and Cardarilli, Gian Carlo and Di Nunzio, Luca and Fazzolari, Rocco and Giardino, Daniele and Re, Marco and Spanò, Sergio},
  date = {2021-01},
  journaltitle = {Applied Sciences},
  volume = {11},
  number = {11},
  pages = {4948},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2076-3417},
  doi = {10.3390/app11114948},
  url = {https://www.mdpi.com/2076-3417/11/11/4948},
  urldate = {2024-01-15},
  abstract = {In this review, we present an analysis of the most used multi-agent reinforcement learning algorithms. Starting with the single-agent reinforcement learning algorithms, we focus on the most critical issues that must be taken into account in their extension to multi-agent scenarios. The analyzed algorithms were grouped according to their features. We present a detailed taxonomy of the main multi-agent approaches proposed in the literature, focusing on their related mathematical models. For each algorithm, we describe the possible application fields, while pointing out its pros and cons. The described multi-agent algorithms are compared in terms of the most important characteristics for multi-agent reinforcement learning applications—namely, nonstationarity, scalability, and observability. We also describe the most common benchmark environments used to evaluate the performances of the considered methods.},
  issue = {11},
  langid = {english},
  keywords = {machine learning,multi-agent,notion,reinforcement learning,swarm},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Multi-Agent Reinforcement Learning_2021_Canese et al.pdf}
}

@online{MultiAgentReinforcementLearning_2021_ZhangEtAl,
  title = {Multi-{{Agent Reinforcement Learning}}: {{A Selective Overview}} of {{Theories}} and {{Algorithms}}},
  shorttitle = {Multi-{{Agent Reinforcement Learning}}},
  author = {Zhang, Kaiqing and Yang, Zhuoran and Başar, Tamer},
  date = {2021-04-28},
  eprint = {1911.10635},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1911.10635},
  url = {http://arxiv.org/abs/1911.10635},
  urldate = {2024-01-15},
  abstract = {Recent years have witnessed significant advances in reinforcement learning (RL), which has registered great success in solving various sequential decision-making problems in machine learning. Most of the successful RL applications, e.g., the games of Go and Poker, robotics, and autonomous driving, involve the participation of more than one single agent, which naturally fall into the realm of multi-agent RL (MARL), a domain with a relatively long history, and has recently re-emerged due to advances in single-agent RL techniques. Though empirically successful, theoretical foundations for MARL are relatively lacking in the literature. In this chapter, we provide a selective overview of MARL, with focus on algorithms backed by theoretical analysis. More specifically, we review the theoretical results of MARL algorithms mainly within two representative frameworks, Markov/stochastic games and extensive-form games, in accordance with the types of tasks they address, i.e., fully cooperative, fully competitive, and a mix of the two. We also introduce several significant but challenging applications of these algorithms. Orthogonal to the existing reviews on MARL, we highlight several new angles and taxonomies of MARL theory, including learning in extensive-form games, decentralized MARL with networked agents, MARL in the mean-field regime, (non-)convergence of policy-based methods for learning in games, etc. Some of the new angles extrapolate from our own research endeavors and interests. Our overall goal with this chapter is, beyond providing an assessment of the current state of the field on the mark, to identify fruitful future research directions on theoretical studies of MARL. We expect this chapter to serve as continuing stimulus for researchers interested in working on this exciting while challenging topic.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems,notion,Statistics - Machine Learning},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Multi-Agent Reinforcement Learning_2021_Zhang et al.pdf;/home/baldoinov/Zotero/storage/EJLJFY9U/1911.html}
}

@online{MultiagentReinforcementLearning_2023_HuhMohapatra,
  title = {Multi-Agent {{Reinforcement Learning}}: {{A Comprehensive Survey}}},
  shorttitle = {Multi-Agent {{Reinforcement Learning}}},
  author = {Huh, Dom and Mohapatra, Prasant},
  date = {2023-12-15},
  eprint = {2312.10256},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2312.10256},
  url = {http://arxiv.org/abs/2312.10256},
  urldate = {2024-01-31},
  abstract = {The prevalence of multi-agent applications pervades various interconnected systems in our everyday lives. Despite their ubiquity, the integration and development of intelligent decision-making agents in a shared environment pose challenges to their effective implementation. This survey delves into the domain of multi-agent systems (MAS), placing a specific emphasis on unraveling the intricacies of learning optimal control within the MAS framework, commonly known as multi-agent reinforcement learning (MARL). The objective of this survey is to provide comprehensive insights into various dimensions of MAS, shedding light on myriad opportunities while highlighting the inherent challenges that accompany multi-agent applications. We hope not only to contribute to a deeper understanding of the MAS landscape but also to provide valuable perspectives for both researchers and practitioners. By doing so, we aim to facilitate informed exploration and foster development within the dynamic realm of MAS, recognizing the need for adaptive strategies and continuous evolution in addressing emerging complexities in MARL.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems,notion},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Multi-agent Reinforcement Learning_2023_Huh et al.pdf;/home/baldoinov/Zotero/storage/EK3XD78R/2312.html}
}

@article{MultigraphRepresentationEvent_2024_HuangEtAl,
  title = {A Multi-Graph Representation for Event Extraction},
  author = {Huang, Hui and Chen, Yanping and Lin, Chuan and Huang, Ruizhang and Zheng, Qinghua and Qin, Yongbin},
  date = {2024-07-01},
  journaltitle = {Artificial Intelligence},
  shortjournal = {Artificial Intelligence},
  volume = {332},
  pages = {104144},
  issn = {0004-3702},
  doi = {10.1016/j.artint.2024.104144},
  url = {https://www.sciencedirect.com/science/article/pii/S0004370224000808},
  urldate = {2024-05-17},
  abstract = {Event extraction has a trend in identifying event triggers and arguments in a unified framework, which has the advantage of avoiding the cascading failure in pipeline methods. The main problem is that joint models usually assume a one-to-one relationship between event triggers and arguments. It leads to the argument multiplexing problem, in which an argument mention can serve different roles in an event or shared by different events. To address this problem, we propose a multigraph-based event extraction framework. It allows parallel edges between any nodes, which is effective to represent semantic structures of an event. The framework enables the neural network to map a sentence(s) into a structurized semantic representation, which encodes multi-overlapped events. After evaluated on four public datasets, our method achieves the state-of-the-art performance, outperforming all compared models. Analytical experiments show that the multigraph representation is effective to address the argument multiplexing problem and helpful to advance the discriminability of the neural network for event extraction.},
  keywords = {Argument multiplexing,Event extraction,Event representation,Multigraph},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/A multi-graph representation for event extraction_2024_Huang et al.pdf;/home/baldoinov/Zotero/storage/XASYV6A6/S0004370224000808.html}
}

@online{MultilabelTextClassification_2021_VenelinValkov,
  title = {Multi-Label {{Text Classification}} with {{BERT}} and {{PyTorch Lightning}}},
  author = {{Venelin Valkov}},
  date = {2021-04-26},
  url = {https://curiousily.com/posts/multi-label-text-classification-with-bert-and-pytorch-lightning/},
  urldate = {2024-08-08},
  abstract = {Fine-tune BERT for multi-label text classification on toxic comments},
  langid = {english},
  organization = {Curiousily},
  file = {/home/baldoinov/Zotero/storage/UD7H3ZYD/multi-label-text-classification-with-bert-and-pytorch-lightning.html}
}

@online{MultiValueAlignmentNormative_2023_RiadEtAl,
  title = {Multi-{{Value Alignment}} in {{Normative Multi-Agent System}}: {{An Evolutionary Optimisation Approach}}},
  shorttitle = {Multi-{{Value Alignment}} in {{Normative Multi-Agent System}}},
  author = {Riad, Maha and family=Carvalho, given=Vinicius, prefix=de, useprefix=true and Golpayegani, Fatemeh},
  date = {2023-10-12},
  eprint = {2310.08362},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2310.08362},
  url = {http://arxiv.org/abs/2310.08362},
  urldate = {2024-01-31},
  abstract = {Value-alignment in normative multi-agent systems is used to promote a certain value and to ensure the consistent behaviour of agents in autonomous intelligent systems with human values. However, the current literature is limited to the incorporation of effective norms for single-value alignment with no consideration of agents' heterogeneity and the requirement of simultaneous promotion and alignment of multiple values. This research proposes a multi-value promotion model that uses multi-objective evolutionary algorithms and decentralised reasoning to produce the optimum parametric set of norms that is aligned with multiple simultaneous values of heterogeneous agents and the system. To understand various aspects of this complex problem, several evolutionary algorithms were used to find a set of optimised norm parameters considering two toy tax scenarios with two and five values are considered. The results are analysed from different perspectives to show the impact of a selected evolutionary algorithm on the solution, and the importance of understanding the relation between values when prioritising them.},
  pubstate = {prepublished},
  keywords = {Computer Science - Multiagent Systems,notion},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Multi-Value Alignment in Normative Multi-Agent System_2023_Riad et al.pdf;/home/baldoinov/Zotero/storage/57NBI9PQ/2310.html}
}

@book{MusicoProfissaoOu_2007_JoaoAlexandreLucianoGarrutiFilho,
  title = {Musico - Profissao Ou Ministerio?},
  author = {{João Alexandre} and {Luciano Garruti Filho}},
  date = {2007-01-01},
  edition = {1ª edição},
  publisher = {Vpc Produçoes},
  isbn = {978-85-900406-2-0},
  langid = {portuguese},
  file = {/home/baldoinov/baldoinov/PDFs/Livros/Musico - Profissao Ou Ministerio_2007_Joao Alexandre et al.pdf}
}

@inproceedings{NaoSePerca_2021_SousaEtAl,
  title = {Não se perca no debate! Mineração de Argumentação em Redes Sociais},
  booktitle = {Anais do Brazilian Workshop on Social Network Analysis and Mining (BraSNAM)},
  author = {Sousa, João Pedro da Silva and family=Nascimento, given=Rodrigo Costa Uchoa, prefix=do, useprefix=false and family=Araujo, given=Renata Mendes, prefix=de, useprefix=false and Coelho, Orlando Bisacchi},
  date = {2021-07-18},
  pages = {139--150},
  publisher = {SBC},
  issn = {2595-6094},
  doi = {10.5753/brasnam.2021.16132},
  url = {https://sol.sbc.org.br/index.php/brasnam/article/view/16132},
  urldate = {2023-04-12},
  abstract = {Argumentation Mining is a subject at the intersection of Computational Linguistics and Data Science that is quite relevant for decision making and includes tasks such as identification and retrieval of the structure of an argument and each participant’s utterance polarities. This work is a first step towards establishing an Argumentation Mining process focused on arguments in social networks aimed at recognizing the structure of the debates that occur in the networks. The results of this process are then input to a debate visualization tool, Visu, as a way to foster the understanding of debates between users of social media.},
  eventtitle = {Anais do X Brazilian Workshop on Social Network Analysis and Mining},
  langid = {portuguese},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/projeto-heiwa/Nao se perca no debate_2021_Sousa et al.pdf}
}

@article{NaoTemosTempo_2017_LinsEtAl,
  title = {Não temos tempo a perder: uma introdução à análise de sobrevivência},
  shorttitle = {Não temos tempo a perder},
  author = {Lins, Rodrigo and Figueiredo, Dalson and Rocha, Enivaldo},
  date = {2017-04-17},
  journaltitle = {Revista Política Hoje},
  volume = {26},
  number = {1},
  pages = {279--298},
  issn = {0104-7094},
  doi = {10.51359/1808-8708.2017.12091},
  url = {https://periodicos.ufpe.br/revistas/index.php/politicahoje/article/view/12091},
  urldate = {2025-02-04},
  abstract = {O que fazer quando a variável dependente é o tempo até a ocorrência de um determinado evento? Este artigo apresenta uma introdução intuitiva à análise de sobrevivência com foco no estimador de Kaplan-Meier. Nosso público alvo são estudantes de graduação e pós-graduação em fases iniciais de treinamento. Metodologicamente, o desenho de pesquisa utiliza simulação básica para apresentar a implementação computacional e a interpretação substantiva dos coeficientes. Todas as rotinas computacionais foram reportadas com o objetivo de aumentar a transparência e garantir a replicabilidade dos resultados. Ainda adotamos o protocolo TIER. Com este artigo esperamos difundir a utilização da análise de sobrevivência em Ciência Política e contribuir com o avanço da literatura sobre metodologia política.},
  issue = {1},
  langid = {portuguese},
  keywords = {análise de sobrevivência,ciência política,estimador de Kaplan-Meier,métodos quantitativos,SPSS},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Nao temos tempo a perder_2017_Lins et al.pdf}
}

@online{NaturalLanguageProcessing__Jablonski,
  title = {Natural {{Language Processing With Python}}'s {{NLTK Package}}},
  author = {Jablonski, Joanna},
  url = {https://realpython.com/nltk-nlp-python/},
  urldate = {2023-09-21},
  abstract = {In this beginner-friendly tutorial, you'll take your first steps with Natural Language Processing (NLP) and Python's Natural Language Toolkit (NLTK). You'll learn how to process unstructured data in order to be able to analyze it and draw conclusions from it.},
  langid = {english},
  keywords = {notion},
  file = {/home/baldoinov/Zotero/storage/ECCEYPQC/nltk-nlp-python.html}
}

@online{NaturalLanguageProcessing_2022_Alok,
  title = {Natural {{Language Processing}} in {{Python Using spaCy}}},
  author = {Alok, Udisha},
  date = {2022-09-08T10:19:10},
  url = {https://blog.quantinsti.com/spacy-python/},
  urldate = {2023-09-21},
  abstract = {spaCy is a powerful Python library for natural language processing. In this guide, we look at tokenisation, named entity recognition, pos tagging, and more using spaCy and Python.},
  langid = {english},
  organization = {Quantitative Finance \& Algo Trading Blog by QuantInsti},
  keywords = {notion},
  file = {/home/baldoinov/Zotero/storage/5W4KTL63/spacy-python.html}
}

@article{NaturalSelectionBad_2016_SmaldinoMcElreath,
  title = {The Natural Selection of Bad Science},
  author = {Smaldino, Paul E. and McElreath, Richard},
  date = {2016-09},
  journaltitle = {Royal Society Open Science},
  volume = {3},
  number = {9},
  pages = {160384},
  publisher = {Royal Society},
  doi = {10.1098/rsos.160384},
  url = {https://royalsocietypublishing.org/doi/full/10.1098/rsos.160384},
  urldate = {2024-06-10},
  abstract = {Poor research design and data analysis encourage false-positive findings. Such poor methods persist despite perennial calls for improvement, suggesting that they result from something more than just misunderstanding. The persistence of poor methods results partly from incentives that favour them, leading to the natural selection of bad science. This dynamic requires no conscious strategizing—no deliberate cheating nor loafing—by scientists, only that publication is a principal factor for career advancement. Some normative methods of analysis have almost certainly been selected to further publication instead of discovery. In order to improve the culture of science, a shift must be made away from correcting misunderstandings and towards rewarding understanding. We support this argument with empirical evidence and computational modelling. We first present a 60-year meta-analysis of statistical power in the behavioural sciences and show that power has not improved despite repeated demonstrations of the necessity of increasing power. To demonstrate the logical consequences of structural incentives, we then present a dynamic model of scientific communities in which competing laboratories investigate novel or previously published hypotheses using culturally transmitted research methods. As in the real world, successful labs produce more ‘progeny,’ such that their methods are more often copied and their students are more likely to start labs of their own. Selection for high output leads to poorer methods and increasingly high false discovery rates. We additionally show that replication slows but does not stop the process of methodological deterioration. Improving the quality of research requires change at the institutional level.},
  keywords = {Campbell’s Law,cultural evolution,incentives,metascience,replication,statistical power},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/The natural selection of bad science_2016_Smaldino et al.pdf}
}

@online{NBEATSNeuralBasis_2020_OreshkinEtAl,
  title = {N-{{BEATS}}: {{Neural}} Basis Expansion Analysis for Interpretable Time Series Forecasting},
  shorttitle = {N-{{BEATS}}},
  author = {Oreshkin, Boris N. and Carpov, Dmitri and Chapados, Nicolas and Bengio, Yoshua},
  date = {2020-02-20},
  eprint = {1905.10437},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.1905.10437},
  url = {http://arxiv.org/abs/1905.10437},
  urldate = {2024-11-04},
  abstract = {We focus on solving the univariate times series point forecasting problem using deep learning. We propose a deep neural architecture based on backward and forward residual links and a very deep stack of fully-connected layers. The architecture has a number of desirable properties, being interpretable, applicable without modification to a wide array of target domains, and fast to train. We test the proposed architecture on several well-known datasets, including M3, M4 and TOURISM competition datasets containing time series from diverse domains. We demonstrate state-of-the-art performance for two configurations of N-BEATS for all the datasets, improving forecast accuracy by 11\% over a statistical benchmark and by 3\% over last year's winner of the M4 competition, a domain-adjusted hand-crafted hybrid between neural network and statistical time series models. The first configuration of our model does not employ any time-series-specific components and its performance on heterogeneous datasets strongly suggests that, contrarily to received wisdom, deep learning primitives such as residual blocks are by themselves sufficient to solve a wide range of forecasting problems. Finally, we demonstrate how the proposed architecture can be augmented to provide outputs that are interpretable without considerable loss in accuracy.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Recovery/Modelo de Incidencia de Acao Contra/N-BEATS_2020_Oreshkin et al.pdf;/home/baldoinov/Zotero/storage/Q9SI87MK/1905.html}
}

@online{NearestNeighborMatching_2022_sharad,
  type = {Forum post},
  title = {Nearest {{Neighbor Matching}}},
  author = {{sharad}},
  date = {2022-03-07},
  url = {https://stats.stackexchange.com/q/566949},
  urldate = {2025-07-29},
  organization = {Cross Validated},
  file = {/home/baldoinov/Zotero/storage/LEFPN8SX/nearest-neighbor-matching.html}
}

@article{Necropolitica_2016_Mbembe,
  title = {Necropolítica},
  author = {Mbembe, Achille},
  date = {2016},
  journaltitle = {Arte \& Ensaios},
  number = {32},
  pages = {122--151},
  issn = {2448-3338},
  doi = {10.60001/ae.n32.p122},
  url = {https://revistas.ufrj.br/index.php/ae/article/view/8993},
  urldate = {2024-04-01},
  abstract = {Este ensaio pressupõe que a expressão máxima da soberania reside, em grande~medida, no poder e na capacidade de ditar quem pode viver e quem deve morrer.~Por isso, matar ou deixar viver constituem os limites da soberania, seus atributos~fundamentais. Exercitar a soberania é exercer controle sobre a mortalidade e definir a vida como a implantação e manifestação de poder.},
  issue = {32},
  langid = {portuguese},
  file = {/home/baldoinov/baldoinov/PDFs/Livros/Necropolitica_2016_Mbembe.pdf}
}

@article{nepstad_2014,
  title = {Slowing {{Amazon}} Deforestation through Public Policy and Interventions in Beef and Soy Supply Chains},
  author = {Nepstad, D. and McGrath, D. and Stickler, C. and Alencar, A. and Azevedo, A. and Swette, B. and Bezerra, T. and DiGiano, M. and Shimada, J. and Seroa da Motta, R. and Armijo, E.},
  date = {2014},
  journaltitle = {Science (New York, N.Y.)},
  shortjournal = {Science},
  volume = {344},
  number = {6188},
  pages = {1118--1123}
}

@online{NetflixUseCases__M,
  title = {Netflix {{Use Cases}} [Classic] | {{Creately}}},
  author = {M, Nick},
  url = {https://creately.com/diagram/example/igrfmy322/netflix-use-cases-classic},
  urldate = {2024-03-26},
  file = {/home/baldoinov/Zotero/storage/A9STA3EY/netflix-use-cases-classic.html}
}

@book{Networks_2018_Newman,
  title = {Networks},
  author = {Newman, Mark},
  date = {2018-09-19},
  edition = {2nd edition},
  publisher = {Oxford University Press},
  location = {Oxford, United Kingdom ; New York, NY, United States of America},
  abstract = {The study of networks, including computer networks, social networks, and biological networks, has attracted enormous interest in the last few years. The rise of the Internet and the wide availability of inexpensive computers have made it possible to gather and analyze network data on an unprecedented scale, and the development of new theoretical tools has allowed us to extract knowledge from networks of many different kinds. The study of networks is broadly interdisciplinary and central developments have occurred in many fields, including mathematics, physics, computer and information sciences, biology, and the social sciences. This book brings together the most important breakthroughs in each of these fields and presents them in a coherent fashion, highlighting the strong interconnections between work in different areas.Topics covered include the measurement of networks; methods for analyzing network data, including methods developed in physics, statistics, and sociology; fundamentals of graph theory; computer algorithms; mathematical models of networks, including random graph models and generative models; and theories of dynamical processes taking place on networks.},
  isbn = {978-0-19-880509-0},
  langid = {english},
  pagetotal = {800},
  keywords = {notion}
}

@article{NetworkTakeover_2012_Barabasi,
  title = {The Network Takeover},
  author = {Barabási, Albert-László},
  date = {2012-01},
  journaltitle = {Nature Physics},
  shortjournal = {Nature Phys},
  volume = {8},
  number = {1},
  pages = {14--16},
  publisher = {Nature Publishing Group},
  issn = {1745-2481},
  doi = {10.1038/nphys2188},
  url = {https://www.nature.com/articles/nphys2188},
  urldate = {2024-02-14},
  abstract = {Reductionism, as a paradigm, is expired, and complexity, as a field, is tired. Data-based mathematical models of complex systems are offering a fresh perspective, rapidly developing into a new discipline: network science.},
  issue = {1},
  langid = {english},
  keywords = {Atomic,Classical and Continuum Physics,Complex Systems,Condensed Matter Physics,general,Mathematical and Computational Physics,Molecular,notion,Optical and Plasma Physics,Physics,Theoretical},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/The network takeover_2012_Barabasi.pdf}
}

@article{NewPerspectivesInstitutionalist_2015_GrabnerKapeller,
  title = {New {{Perspectives}} on {{Institutionalist Pattern Modeling}}: {{Systemism}}, {{Complexity}}, and {{Agent-Based Modeling}}},
  shorttitle = {New {{Perspectives}} on {{Institutionalist Pattern Modeling}}},
  author = {Gräbner, Claudius and Kapeller, Jakob},
  date = {2015-04-03},
  journaltitle = {Journal of Economic Issues},
  volume = {49},
  number = {2},
  pages = {433--440},
  publisher = {Routledge},
  issn = {0021-3624},
  doi = {10.1080/00213624.2015.1042765},
  url = {https://doi.org/10.1080/00213624.2015.1042765},
  urldate = {2024-06-19},
  abstract = {We focus on the complementarity between original institutional economics, Mario Bunge’s framework of systemism, and the formal tools developed by complexity economists, especially in the context of agent-based modeling. We assert that original institutional economics might profit from exploiting this complementarity.},
  keywords = {agent-based computational economics,aggregation,B41,B52,C63,complexity,original institutionalism,systemism},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/New Perspectives on Institutionalist Pattern Modeling_2015_Grabner et al.pdf}
}

@online{NewSpotifyUse__Len,
  title = {New Spotify Use Case [Classic] | {{Creately}}},
  author = {Len, Éanna},
  url = {https://creately.com/diagram/example/ijlamayg1/new-spotify-use-case-classic},
  urldate = {2024-03-26},
  file = {/home/baldoinov/Zotero/storage/64BDT2VD/new-spotify-use-case-classic.html}
}

@report{NISTBigData_2019_NISTBigDataPublicWorkingGroupDefinitionsandTaxonomiesSubgroup.,
  title = {{{NIST Big Data Interoperability Framework}}: Volume 6, Reference Architecture Version 3},
  shorttitle = {{{NIST Big Data Interoperability Framework}}},
  author = {{NIST Big Data Public Working Group, Definitions and Taxonomies Subgroup.}},
  date = {2019-10},
  number = {NIST SP 1500-6r2},
  pages = {NIST SP 1500-6r2},
  institution = {{National Institute of Standards and Technology}},
  location = {Gaithersburg, MD},
  doi = {10.6028/NIST.SP.1500-6r2},
  url = {https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.1500-6r2.pdf},
  urldate = {2025-03-09},
  abstract = {Big Data is a term used to describe the large amount of data in the networked, digitized, sensor-laden, information-driven world. While opportunities exist with Big Data, the data can overwhelm traditional technical approaches, and the growth of data is outpacing scientific and technological advances in data analytics. To advance progress in Big Data, the NIST Big Data Public Working Group (NBD-PWG) is working to develop consensus on important fundamental concepts related to Big Data. The results are reported in the NIST Big Data Interoperability Framework (NBDIF) series of volumes. This volume, Volume 6, summarizes the work performed by the NBD-PWG to characterize Big Data from an architecture perspective, presents the NIST Big Data Reference Architecture (NBDRA) conceptual model, discusses the roles and fabrics of the NBDRA, presents an activities view of the NBDRA to describe the activities performed by the roles, and presents a functional component view of the NBDRA containing the classes of functional components that carry out the activities.},
  langid = {english},
  file = {/home/baldoinov/baldoinov/PDFs/Mackenzie/ecossistemas-de-big-data-ii/NIST Big Data Interoperability Framework_2019_NIST Big Data Public Working Group, Definitions and Taxonomies Subgroup..pdf}
}

@article{NivelInstrucionalPessoas_2015_GoncalvesEtAl,
  title = {Nível instrucional de pessoas com deficiência no Brasil},
  author = {Gonçalves, Taísa Grasiela Gomes Liduenha and Meletti, Silvia Márcia Ferreira and family=Santos, given=Natália Gomes, prefix=dos, useprefix=false},
  date = {2015-12-16},
  journaltitle = {Crítica Educativa},
  volume = {1},
  number = {2},
  pages = {24--39},
  issn = {2447-4223},
  doi = {10.22476/revcted.v1i2.37},
  url = {https://criticaeducativa.ufscar.br/index.php/criticaeducativa/article/view/37},
  urldate = {2024-06-23},
  abstract = {O objetivo deste estudo foi analisar o nível instrucional da população brasileira, especificamente, das pessoas com deficiência. Para a análise utilizou-se os dados do censo demográfico de 2010 do Instituto Brasileiro de Geografia e Estatística (IBGE). Os resultados indicaram: alto percentual de pessoas sem instrução ou com o ensino fundamental incompleto (44, 9\%) com 15 anos ou mais.~ Ao tratar das pessoas com deficiência com 10 anos ou mais, os dados mostraram que, em média, são analfabetas: 13,5\% das pessoas com deficiência visual, 21,2\% das pessoas com deficiência auditiva, 30,2\% das pessoas com deficiência física e 45,6\% das pessoas com deficiência mental. Ainda com relação à população com deficiência mental, a concentração (52,5\%) do analfabetismo ocorre na faixa etária dos 10 aos 14 anos. Espera-se que esse estudo possa subsidiar o debate sobre o direito à educação para todos os alunos e fomentar a produção do conhecimento sobre o analfabetismo.},
  issue = {2},
  langid = {portuguese},
  keywords = {indicadores sociais.},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Nivel instrucional de pessoas com deficiencia no Brasil_2015_Goncalves et al.pdf}
}

@article{NonparametricEstimationExact_1995_HausmanNewey,
  title = {Nonparametric {{Estimation}} of {{Exact Consumers Surplus}} and {{Deadweight Loss}}},
  author = {Hausman, Jerry A. and Newey, Whitney K.},
  date = {1995},
  journaltitle = {Econometrica},
  volume = {63},
  number = {6},
  eprint = {2171777},
  eprinttype = {jstor},
  pages = {1445--1476},
  publisher = {[Wiley, Econometric Society]},
  issn = {0012-9682},
  doi = {10.2307/2171777},
  url = {https://www.jstor.org/stable/2171777},
  urldate = {2024-06-19},
  abstract = {We apply nonparametric regression models to estimation of demand curves of the type most often used in applied research. From the demand curve estimators we derive estimates of exact consumers surplus and deadweight loss, which are the most widely used welfare and economic efficiency measures in areas of economics such as public finance. We also develop tests of the symmetry and downward sloping properties of compensated demand. We work out asymptotic normal sampling theory for kernel and series nonparametric estimators, as well as for the parametric case. The paper includes an application to gasoline demand. Empirical questions of interest here are the shape of the demand curve and the average magnitude of welfare loss from a tax on gasoline. In this application we compare parametric and nonparametric estimates of the demand curve, calculate exact and approximate measures of consumers surplus and deadweight loss, and give standard error estimates. We also analyze the sensitivity of the welfare measures to components of nonparametric regression estimators such as the number of terms in a series approximation.},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Nonparametric Estimation of Exact Consumers Surplus and Deadweight Loss_1995_Hausman et al.pdf}
}

@online{NoPropTrainingNeural_2025_LiEtAl,
  title = {{{NoProp}}: {{Training Neural Networks}} without {{Back-propagation}} or {{Forward-propagation}}},
  shorttitle = {{{NoProp}}},
  author = {Li, Qinyu and Teh, Yee Whye and Pascanu, Razvan},
  date = {2025-03-31},
  eprint = {2503.24322},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2503.24322},
  url = {http://arxiv.org/abs/2503.24322},
  urldate = {2025-05-09},
  abstract = {The canonical deep learning approach for learning requires computing a gradient term at each layer by back-propagating the error signal from the output towards each learnable parameter. Given the stacked structure of neural networks, where each layer builds on the representation of the layer below, this approach leads to hierarchical representations. More abstract features live on the top layers of the model, while features on lower layers are expected to be less abstract. In contrast to this, we introduce a new learning method named NoProp, which does not rely on either forward or backwards propagation. Instead, NoProp takes inspiration from diffusion and flow matching methods, where each layer independently learns to denoise a noisy target. We believe this work takes a first step towards introducing a new family of gradient-free learning methods, that does not learn hierarchical representations -- at least not in the usual sense. NoProp needs to fix the representation at each layer beforehand to a noised version of the target, learning a local denoising process that can then be exploited at inference. We demonstrate the effectiveness of our method on MNIST, CIFAR-10, and CIFAR-100 image classification benchmarks. Our results show that NoProp is a viable learning algorithm which achieves superior accuracy, is easier to use and computationally more efficient compared to other existing back-propagation-free methods. By departing from the traditional gradient based learning paradigm, NoProp alters how credit assignment is done within the network, enabling more efficient distributed learning as well as potentially impacting other characteristics of the learning process.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/NoProp_2025_Li et al.pdf;/home/baldoinov/Zotero/storage/XGUD3TSI/2503.html}
}

@book{NovaContabilidadeSocial_2021_PaulaniBraga,
  title = {A Nova Contabilidade Social},
  author = {Paulani, Leda Maria and Braga, Márcio Bobik},
  date = {2021-10-01},
  publisher = {Editora Saraiva},
  isbn = {978-85-02-06430-0},
  langid = {brazilian}
}

@book{NovoBrasilConquistas_2011_FishlowBacha,
  title = {O Novo Brasil: as Conquistas Políticas, Econômicas, Sociais e nas Relações Internacionais},
  shorttitle = {O Novo Brasil},
  author = {Fishlow, Albert and Bacha, Edmar},
  date = {2011},
  edition = {1ª edição},
  publisher = {Saint Paul},
  abstract = {Albert Fishlow condensa seu conhecimento nesta obra de valioso teor histórico, que reflete a complexidade dos acontecimentos econômicos e políticos dos últimos 25 anos no país.},
  isbn = {978-85-8004-012-8},
  langid = {portuguese}
}

@article{ODDProtocolDescribing_2020_GrimmEtAl,
  title = {The {{ODD Protocol}} for {{Describing Agent-Based}} and {{Other Simulation Models}}: {{A Second Update}} to {{Improve Clarity}}, {{Replication}}, and {{Structural Realism}}},
  shorttitle = {The {{ODD Protocol}} for {{Describing Agent-Based}} and {{Other Simulation Models}}},
  author = {Grimm, Volker and Railsback, Steven F. and Vincenot, Christian E. and Berger, Uta and Gallagher, Cara and DeAngelis, Donald L. and Edmonds, Bruce and Ge, Jiaqi and Giske, Jarl and Groeneveld, Jürgen and Johnston, Alice S. A. and Milles, Alexander and Nabe-Nielsen, Jacob and Polhill, J. Gareth and Radchuk, Viktoriia and Rohwäder, Marie-Sophie and Stillman, Richard A. and Thiele, Jan C. and Ayllón, Daniel},
  date = {2020},
  journaltitle = {Journal of Artificial Societies and Social Simulation},
  shortjournal = {JASSS},
  volume = {23},
  number = {2},
  pages = {7},
  issn = {1460-7425},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/The ODD Protocol for Describing Agent-Based and Other Simulation Models_2020_Grimm et al.pdf;/home/baldoinov/Zotero/storage/UT5XGWEH/7.html}
}

@article{ODDProtocolReview_2010_GrimmEtAl,
  title = {The {{ODD}} Protocol: {{A}} Review and First Update},
  shorttitle = {The {{ODD}} Protocol},
  author = {Grimm, Volker and Berger, Uta and DeAngelis, Donald L. and Polhill, J. Gary and Giske, Jarl and Railsback, Steven F.},
  date = {2010-11-24},
  journaltitle = {Ecological Modelling},
  shortjournal = {Ecological Modelling},
  volume = {221},
  number = {23},
  pages = {2760--2768},
  issn = {0304-3800},
  doi = {10.1016/j.ecolmodel.2010.08.019},
  url = {https://www.sciencedirect.com/science/article/pii/S030438001000414X},
  urldate = {2024-06-06},
  abstract = {The ‘ODD’ (Overview, Design concepts, and Details) protocol was published in 2006 to standardize the published descriptions of individual-based and agent-based models (ABMs). The primary objectives of ODD are to make model descriptions more understandable and complete, thereby making ABMs less subject to criticism for being irreproducible. We have systematically evaluated existing uses of the ODD protocol and identified, as expected, parts of ODD needing improvement and clarification. Accordingly, we revise the definition of ODD to clarify aspects of the original version and thereby facilitate future standardization of ABM descriptions. We discuss frequently raised critiques in ODD but also two emerging, and unanticipated, benefits: ODD improves the rigorous formulation of models and helps make the theoretical foundations of large models more visible. Although the protocol was designed for ABMs, it can help with documenting any large, complex model, alleviating some general objections against such models.},
  keywords = {Model description,Model formulation,Model replication,Scientific communication,Standardization},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/The ODD protocol_2010_Grimm et al.pdf;/home/baldoinov/Zotero/storage/LQQNIWB6/S030438001000414X.html}
}

@article{OnlineLearningComprehensive_2021_HoiEtAl,
  title = {Online Learning: {{A}} Comprehensive Survey},
  author = {Hoi, Steven C. H. and Sahoo, Doyen and Lu, Jing and Zhao, Peilin},
  date = {2021},
  journaltitle = {Neurocomputing},
  volume = {459},
  pages = {249--289},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2021.04.112},
  url = {https://www.sciencedirect.com/science/article/pii/S0925231221006706},
  abstract = {Online learning represents a family of machine learning methods, where a learner attempts to tackle some predictive (or any type of decision-making) task by learning from a sequence of data instances one by one at each time. The goal of online learning is to maximize the accuracy/correctness for the sequence of predictions/decisions made by the online learner given the knowledge of correct answers to previous prediction/learning tasks and possibly additional information. This is in contrast to traditional batch or offline machine learning methods that are often designed to learn a model from the entire training data set at once. Online learning has become a promising technique for learning from continuous streams of data in many real-world applications. This survey aims to provide a comprehensive survey of the online machine learning literature through a systematic review of basic ideas and key principles and a proper categorization of different algorithms and techniques. Generally speaking, according to the types of learning tasks and the forms of feedback information, the existing online learning works can be classified into three major categories: (i) online supervised learning where full feedback information is always available, (ii) online learning with limited feedback, and (iii) online unsupervised learning where no feedback is available. Due to space limitation, the survey will be mainly focused on the first category, but also briefly cover some basics of the other two categories. Finally, we also discuss some open issues and attempt to shed light on potential future research directions in this field.},
  keywords = {notion,Online convex optimization,Online learning,Sequential decision making},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Online learning_2021_Hoi et al.pdf}
}

@online{OpenLoopNatura_2020_Samothrakis,
  title = {Open {{Loop In Natura Economic Planning}}},
  author = {Samothrakis, Spyridon},
  date = {2020-05-14},
  eprint = {2005.01539},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2005.01539},
  url = {http://arxiv.org/abs/2005.01539},
  urldate = {2023-10-22},
  abstract = {The debate between the optimal way of allocating societal surplus (i.e. products and services) has been raging, in one form or another, practically forever; following the collapse of the Soviet Union in 1991, the market became the only legitimate form of organisation -- there was no other alternative. Working within the tradition of Marx, Leontief, Kantorovich, Beer and Cockshott, we propose what we deem an automated planning system that aims to operate on unit level (e.g., factories and citizens), rather than on aggregate demand and sectors. We explain why it is both a viable and desirable alternative to current market conditions and position our solution within current societal structures. Our experiments show that it would be trivial to plan for up to 50K industrial goods and 5K final goods in commodity hardware.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,notion},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Open Loop In Natura Economic Planning_2020_Samothrakis.pdf;/home/baldoinov/Zotero/storage/VQRF29HT/2005.html}
}

@article{OptimalAuctionsDeep_2021_DuttingEtAl,
  title = {Optimal Auctions through Deep Learning},
  author = {Dütting, Paul and Feng, Zhe and Narasimhan, Harikrishna and Parkes, David C. and Ravindranath, Sai S.},
  date = {2021-08},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {64},
  number = {8},
  pages = {109--116},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/3470442},
  url = {https://dl.acm.org/doi/10.1145/3470442},
  urldate = {2024-06-11},
  abstract = {Designing an incentive compatible auction that maximizes expected revenue is an intricate task. The single-item case was resolved in a seminal piece of work by Myerson in 1981. Even after 30--40 years of intense research, the problem remains unsolved for settings with two or more items. We overview recent research results that show how tools from deep learning are shaping up to become a powerful tool for the automated design of near-optimal auctions auctions. In this approach, an auction is modeled as a multilayer neural network, with optimal auction design framed as a constrained learning problem that can be addressed with standard machine learning pipelines. Through this approach, it is possible to recover to a high degree of accuracy essentially all known analytically derived solutions for multi-item settings and obtain novel mechanisms for settings in which the optimal mechanism is unknown.},
  langid = {english},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Optimal auctions through deep learning_2021_Dutting et al.pdf}
}

@online{OptimalDynamicTaxes_2011_GolosovEtAl,
  type = {SSRN Scholarly Paper},
  title = {Optimal {{Dynamic Taxes}}},
  author = {Golosov, Mikhail and Troshkin, Maxim and Tsyvinski, Aleh},
  date = {2011-12-01},
  number = {1967376},
  location = {Rochester, NY},
  url = {https://papers.ssrn.com/abstract=1967376},
  urldate = {2024-06-11},
  abstract = {We study optimal labor and savings distortions in a lifecycle model with idiosyncratic shocks. We show a tight connection between its recursive formulation and a static Mirrlees model with two goods, which allows us to derive elasticity-based expressions for the dynamic optimal distortions. We derive a generalization of a savings distortion for non-separable preferences and show that, under certain conditions, the labor wedge tends to zero for sufficiently high skills. We estimate skill distributions using individual data on the U.S. taxes and labor incomes. Computed optimal distortions decrease for sufficiently high incomes and increase with age.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Aleh Tsyvinski,Maxim Troshkin,Mikhail Golosov,Optimal Dynamic Taxes,SSRN},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Optimal Dynamic Taxes_2011_Golosov et al.pdf}
}

@article{OptimalTaxationPublic_1971_DiamondMirrlees,
  title = {Optimal {{Taxation}} and {{Public Production I}}: {{Production Efficiency}}},
  author = {Diamond, Peter A. and Mirrlees, James A.},
  date = {1971},
  journaltitle = {The American Economic Review},
  volume = {61},
  number = {1},
  eprint = {1910538},
  eprinttype = {jstor},
  pages = {8--27},
  url = {http://www.jstor.org/stable/1910538},
  langid = {english},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Optimal Taxation and Public Production I_1971_Diamond et al.pdf}
}

@article{OptimalTaxationTheory_2009_MankiwEtAl,
  title = {Optimal {{Taxation}} in {{Theory}} and {{Practice}}},
  author = {Mankiw, N. Gregory and Weinzierl, Matthew and Yagan, Danny},
  date = {2009-12},
  journaltitle = {Journal of Economic Perspectives},
  volume = {23},
  number = {4},
  pages = {147--174},
  issn = {0895-3309},
  doi = {10.1257/jep.23.4.147},
  url = {https://www.aeaweb.org/articles?id=10.1257/jep.23.4.147},
  urldate = {2024-06-11},
  abstract = {The optimal design of a tax system is a topic that has long fascinated economic theorists and flummoxed economic policymakers. This paper explores the interplay between tax theory and tax policy. It identifies key lessons policymakers might take from the academic literature on how taxes ought to be designed, and it discusses the extent to which these lessons are reflected in actual tax policy.},
  langid = {english},
  keywords = {Optimal Taxation,Taxation and Subsidies: Efficiency},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Optimal Taxation in Theory and Practice_2009_Mankiw et al.pdf}
}

@article{OptimalTaxTheory_1976_Mirrlees,
  title = {Optimal Tax Theory: {{A}} Synthesis},
  shorttitle = {Optimal Tax Theory},
  author = {Mirrlees, J. A.},
  date = {1976-11-01},
  journaltitle = {Journal of Public Economics},
  shortjournal = {Journal of Public Economics},
  volume = {6},
  number = {4},
  pages = {327--358},
  issn = {0047-2727},
  doi = {10.1016/0047-2727(76)90047-5},
  url = {https://www.sciencedirect.com/science/article/pii/0047272776900475},
  urldate = {2024-06-11},
  abstract = {Necessary conditions for optimal taxation are derived (i) when taxes are constrained to be linear, (ii) when the form of taxation is unconstrained, (iii) when some commodities are subject to nonlinear taxation, the remainder to proportional taxation. Among the results obtained are several that help to determine upon which commodities the tax system ought to bear most heavily. In particular, a criterion for the effect of commodity taxes in the presence of an optimal income tax is found. The paper concludes with a general principle of simple form for optimal economic policies of all kinds.},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Optimal tax theory_1976_Mirrlees.pdf}
}

@article{OriginsStateCapacity_2009_BesleyPersson,
  title = {The {{Origins}} of {{State Capacity}}: {{Property Rights}}, {{Taxation}}, and {{Politics}}},
  shorttitle = {The {{Origins}} of {{State Capacity}}},
  author = {Besley, Timothy and Persson, Torsten},
  date = {2009-09},
  journaltitle = {American Economic Review},
  volume = {99},
  number = {4},
  pages = {1218--1244},
  issn = {0002-8282},
  doi = {10.1257/aer.99.4.1218},
  url = {https://www.aeaweb.org/articles?id=10.1257/aer.99.4.1218},
  urldate = {2023-04-20},
  abstract = {Economists generally assume that the state has sufficient institutional capacity to support markets and levy taxes. This paper develops a framework where "policy choices" in market regulation and taxation are constrained by past investments in legal and fiscal capacity. It studies the economic and political determinants of such investments, demonstrating that legal and fiscal capacity are typically complements. The results show that, among other things, common interest public goods, such as fighting external wars, as well as political stability and inclusive political institutions, are conducive to building state capacity. Some correlations in cross-country data are consistent with the theory. (JEL D72, E62, H11, H20, P14)},
  langid = {english},
  keywords = {and Performance of Government,and Revenue: General,and Voting Behavior,Capitalist Systems: Property Rights,Elections,Fiscal Policy,Legislatures,Models of Political Processes: Rent-seeking,notion,Scope,Structure,Subsidies,Taxation},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/The Origins of State Capacity_2009_Besley et al.pdf}
}

@book{OsFundamentosFisica_2021_RamalhoEtAl,
  title = {Os Fundamentos da Física: Volume 3},
  shorttitle = {Moderna Plus},
  author = {Ramalho, Francisco and Ferraro, Nicolau Gilberto and Soares, Paulo Antônio de Toledo},
  date = {2021-05-20},
  publisher = {Moderna},
  isbn = {978-85-16-07415-9},
  langid = {portuguese}
}

@book{OsFundamentosFisica_2021_RamalhoEtAla,
  title = {Os Fundamentos da Física: Volume 2},
  shorttitle = {Moderna Plus},
  author = {Ramalho, Francisco and Ferraro, Nicolau Gilberto and Soares, Paulo Antônio de Toledo},
  date = {2021-05-20},
  publisher = {Moderna},
  isbn = {978-85-16-07414-2},
  langid = {portuguese}
}

@book{OsFundamentosFisica_2021_RamalhoEtAlb,
  title = {Os Fundamentos da Física: Volume 1},
  shorttitle = {Moderna Plus},
  author = {Ramalho, Francisco and Ferraro, Nicolau Gilberto and Soares, Paulo Antônio de Toledo},
  date = {2021-05-20},
  publisher = {Moderna},
  isbn = {978-85-16-07413-5},
  langid = {portuguese}
}

@incollection{OutofEquilibriumEconomicsAgentBased_2006_Arthur,
  title = {Out-of-{{Equilibrium Economics}} and {{Agent-Based Modeling}}},
  booktitle = {Handbook of {{Computational Economics}}},
  author = {Arthur, W. Brian},
  editor = {Tesfatsion, L. and Judd, K. L.},
  date = {2006-01-01},
  volume = {2},
  pages = {1551--1564},
  publisher = {Elsevier},
  doi = {10.1016/S1574-0021(05)02032-0},
  url = {https://www.sciencedirect.com/science/article/pii/S1574002105020320},
  urldate = {2024-05-29},
  abstract = {Standard neoclassical economics asks what agents' actions, strategies, or expectations are in equilibrium with (consistent with) the outcome or pattern these behaviors aggregatively create. Agent-based computational economics enables us to ask a wider question: how agents' actions, strategies, or expectations might react to—might endogenously change with—the patterns they create. In other words, it enables us to examine how the economy behaves out of equilibrium, when it is not at a steady state. This out-of-equilibrium approach is not a minor adjunct to standard economic theory; it is economics done in a more general way. When examined out of equilibrium, economic patterns sometimes simplify into a simple, homogeneous equilibrium of standard economics; but just as often they show perpetually novel and complex behavior. The static equilibrium approach suffers two characteristic indeterminacies: it cannot easily resolve among multiple equilibria; nor can it easily model individuals' choices of expectations. Both problems are ones of formation (of an equilibrium and of an “ecology” of expectations, respectively), and when analyzed in formation—that is, out of equilibrium—these anomalies disappear.},
  keywords = {agent-based,complexity,evolutionary economics,indeterminacy,out-of-equilibrium economics},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Out-of-Equilibrium Economics and Agent-Based Modeling_2006_Arthur.pdf;/home/baldoinov/Zotero/storage/TQR95VJA/S1574002105020320.html}
}

@article{OverviewIDPTTask_2021_CorreaEtAl,
  title = {Overview of the IDPT Task on Irony Detection in Portuguese at IberLEF 2021},
  author = {Corrêa, Ulisses B. and Coelho, Leonardo and Santos, Leonardo and family=Freitas, given=Larissa A., prefix=de, useprefix=false},
  date = {2021-09-06},
  journaltitle = {Procesamiento del Lenguaje Natural},
  volume = {67},
  number = {0},
  pages = {269--276},
  issn = {1989-7553},
  url = {http://journal.sepln.org/sepln/ojs/ojs/index.php/pln/article/view/6395},
  urldate = {2023-11-14},
  abstract = {This paper presents the Task on Irony Detection in Portuguese (IDPT), held within Iberian Languages Evaluation Forum (IberLEF 2021). We asked the participants to develop systems capable of identifying irony in texts. We created two corpora containing tweets and news articles. Twelve teams registered to the task, among which six submitted both predictions and technical reports. The best performing system achieved a Balanced Accuracy (Bacc) value of 0.52 for tweets (Team PiLN) and 0.92 for news (Team BERT4EVER).},
  issue = {0},
  langid = {spanish},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/projeto-heiwa/Overview of the IDPT Task on Irony Detection in Portuguese at IberLEF 2021_2021_Correa et al.pdf}
}

@incollection{PanelDataForecasting_2013_Baltagi,
  title = {Panel {{Data Forecasting}}},
  booktitle = {Handbook of {{Economic Forecasting}}},
  author = {Baltagi, Badi H.},
  editor = {Elliott, Graham and Timmermann, Allan},
  date = {2013-01-01},
  series = {Handbook of {{Economic Forecasting}}},
  volume = {2},
  pages = {995--1024},
  publisher = {Elsevier},
  doi = {10.1016/B978-0-444-62731-5.00018-X},
  url = {https://www.sciencedirect.com/science/article/pii/B978044462731500018X},
  urldate = {2024-10-11},
  abstract = {This chapter reviews the panel data forecasting literature. Starting with simple forecasts based on fixed and random effects panel data models. Next, these forecasts are extended to allow for various ARMA type structure on the disturbances, as well as spatial autoregressive and moving average type disturbances. These forecasting methods are then studied in the context of seemingly unrelated regressions. We highlight several forecasting empirical applications using panel data, as well as several Monte Carlo studies that compare various forecasting methods using panel data. The chapter concludes with suggestions for further work in this area.},
  keywords = {Fixed effects,Heterogeneous panels,Random effects,Seemingly unrelated regressions,Serial correlation,Spatial dependence},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Recovery/Modelo de Incidencia de Acao Contra/Panel Data Forecasting_2013_Baltagi.pdf}
}

@software{PanelSplit_2024_FreySeimon,
  title = {{{PanelSplit}}},
  author = {Frey, Eric and Seimon, Ben},
  date = {2024-06},
  doi = {10.5281/zenodo.10777259},
  url = {https://github.com/4Freye/panelsplit},
  urldate = {2024-10-22},
  abstract = {A tool for performing cross-validation with panel data},
  version = {0.4.2}
}

@online{Paper2CodeAutomatingCode_2025_SeoEtAl,
  title = {{{Paper2Code}}: {{Automating Code Generation}} from {{Scientific Papers}} in {{Machine Learning}}},
  shorttitle = {{{Paper2Code}}},
  author = {Seo, Minju and Baek, Jinheon and Lee, Seongyun and Hwang, Sung Ju},
  date = {2025-04-26},
  eprint = {2504.17192},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2504.17192},
  url = {http://arxiv.org/abs/2504.17192},
  urldate = {2025-04-29},
  abstract = {Despite the rapid growth of machine learning research, corresponding code implementations are often unavailable, making it slow and labor-intensive for researchers to reproduce results and build upon prior work. In the meantime, recent Large Language Models (LLMs) excel at understanding scientific documents and generating high-quality code. Inspired by this, we introduce PaperCoder, a multi-agent LLM framework that transforms machine learning papers into functional code repositories. PaperCoder operates in three stages: planning, where it constructs a high-level roadmap, designs the system architecture with diagrams, identifies file dependencies, and generates configuration files; analysis, which focuses on interpreting implementation-specific details; and generation, where modular, dependency-aware code is produced. Moreover, each phase is instantiated through a set of specialized agents designed to collaborate effectively across the pipeline. We then evaluate PaperCoder on generating code implementations from machine learning papers based on both model-based and human evaluations, specifically from the original paper authors, with author-released repositories as ground truth if available. Our results demonstrate the effectiveness of PaperCoder in creating high-quality, faithful implementations. Furthermore, it consistently shows strengths in the recently released PaperBench benchmark, surpassing strong baselines by substantial margins. Code is available at: https://github.com/going-doer/Paper2Code.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Paper2Code_2025_Seo et al.pdf;/home/baldoinov/Zotero/storage/AS8LTZ4K/2504.html}
}

@inproceedings{ParadoxoIAPara_2024_BrederEtAl,
  title = {O Paradoxo da IA para Sustentabilidade e a Sustentabilidade da IA},
  booktitle = {Workshop sobre as Implicações da Computação na Sociedade (WICS)},
  author = {Breder, Gabriel B. and Brum, Douglas F. and Dirk, Lucas and Ferro, Mariza},
  date = {2024-07-21},
  pages = {105--116},
  publisher = {SBC},
  issn = {2763-8707},
  doi = {10.5753/wics.2024.2363},
  url = {https://sol.sbc.org.br/index.php/wics/article/view/29514},
  urldate = {2024-11-21},
  abstract = {In recent years, the popularization of Artificial Intelligence (AI) has generated an increasing impact on several sectors, making it necessary to analyze the consequences of its use in relation to ethical and environmental issues. In the environmental area, research is being carried out to measure the impact of AI algorithms in terms of energy consumption and equivalent Carbon Dioxide emissions (CO2e). In this paper, the paradox involving AI and sustainability will be addressed, with an emphasis on the environmental issue and the importance of reporting energy consumption in research involving Machine Learning and the feasibility of using online tools to measure the amount of CO2e.},
  eventtitle = {Workshop sobre as Implicações da Computação na Sociedade (WICS)},
  langid = {portuguese},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/O Paradoxo da IA para Sustentabilidade e a Sustentabilidade da IA_2024_Breder et al.pdf}
}

@online{ParameterEfficientFineTuningMethods_2023_XuEtAl,
  title = {Parameter-{{Efficient Fine-Tuning Methods}} for {{Pretrained Language Models}}: {{A Critical Review}} and {{Assessment}}},
  shorttitle = {Parameter-{{Efficient Fine-Tuning Methods}} for {{Pretrained Language Models}}},
  author = {Xu, Lingling and Xie, Haoran and Qin, Si-Zhao Joe and Tao, Xiaohui and Wang, Fu Lee},
  date = {2023-12-19},
  eprint = {2312.12148},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2312.12148},
  url = {http://arxiv.org/abs/2312.12148},
  urldate = {2024-03-02},
  abstract = {With the continuous growth in the number of parameters of transformer-based pretrained language models (PLMs), particularly the emergence of large language models (LLMs) with billions of parameters, many natural language processing (NLP) tasks have demonstrated remarkable success. However, the enormous size and computational demands of these models pose significant challenges for adapting them to specific downstream tasks, especially in environments with limited computational resources. Parameter Efficient Fine-Tuning (PEFT) offers an effective solution by reducing the number of fine-tuning parameters and memory usage while achieving comparable performance to full fine-tuning. The demands for fine-tuning PLMs, especially LLMs, have led to a surge in the development of PEFT methods, as depicted in Fig. 1. In this paper, we present a comprehensive and systematic review of PEFT methods for PLMs. We summarize these PEFT methods, discuss their applications, and outline future directions. Furthermore, we conduct experiments using several representative PEFT methods to better understand their effectiveness in parameter efficiency and memory efficiency. By offering insights into the latest advancements and practical applications, this survey serves as an invaluable resource for researchers and practitioners seeking to navigate the challenges and opportunities presented by PEFT in the context of PLMs.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,notion},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models_2023_Xu et al.pdf;/home/baldoinov/Zotero/storage/CGBH22ZE/2312.html}
}

@article{PartiallyObservableMarkov_2018_YahyaouiTkiouat,
  title = {Partially Observable {{Markov}} Methods in an Agent-Based Simulation: A Tax Evasion Case Study},
  shorttitle = {Partially Observable {{Markov}} Methods in an Agent-Based Simulation},
  author = {Yahyaoui, Fayçal and Tkiouat, Mohamed},
  date = {2018-01-01},
  journaltitle = {Procedia Computer Science},
  shortjournal = {Procedia Computer Science},
  series = {{{PROCEEDINGS OF THE FIRST INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTING IN DATA SCIENCES}}, {{ICDS2017}}},
  volume = {127},
  pages = {256--263},
  issn = {1877-0509},
  doi = {10.1016/j.procs.2018.01.121},
  url = {https://www.sciencedirect.com/science/article/pii/S1877050918301339},
  urldate = {2024-06-07},
  abstract = {Defining and testing a policy on a socioeconomic system is one of the main problems addressed by agent-based modelling. While research continues to be conducted to come up with hybrid frameworks that tackle the complexity of different problems, no model explicitly integrates computational replications of multi-agent systems, particularly in dealing with partially observable situations. We show in our work how a Markov based reinforced learning and partially observable computations in the behaviour of a taxpayer agent can contribute to refining the analysis of an audit policy.},
  keywords = {bottom-up simulation,decision support,multi-agent,POMDP,tax evasion},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Partially observable Markov methods in an agent-based simulation_2018_Yahyaoui et al.pdf}
}

@book{PatternRecognitionMachine_2006_Bishop,
  title = {Pattern {{Recognition}} and {{Machine Learning}}},
  author = {Bishop, Christopher M.},
  date = {2006},
  series = {Information Science and Statistics},
  publisher = {Springer},
  location = {New York},
  isbn = {978-0-387-31073-2},
  langid = {english},
  file = {/home/baldoinov/baldoinov/PDFs/Livros/Pattern Recognition and Machine Learning_2006_Bishop.pdf}
}

@article{pendrill_2019,
  title = {Deforestation Displaced: Trade in Forest-Risk Commodities and the Prospects for a Global Forest Transition},
  author = {Pendrill, F. and Persson, U.M. and Godar, J. and Kastner, T.},
  date = {2019},
  journaltitle = {Environmental Research Letters},
  volume = {14},
  number = {5}
}

@online{PenPaperExercises_2022_Gutmann,
  title = {Pen and {{Paper Exercises}} in {{Machine Learning}}},
  author = {Gutmann, Michael U.},
  date = {2022-06-27},
  eprint = {2206.13446},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2206.13446},
  url = {http://arxiv.org/abs/2206.13446},
  urldate = {2023-10-05},
  abstract = {This is a collection of (mostly) pen-and-paper exercises in machine learning. The exercises are on the following topics: linear algebra, optimisation, directed graphical models, undirected graphical models, expressive power of graphical models, factor graphs and message passing, inference for hidden Markov models, model-based learning (including ICA and unnormalised models), sampling and Monte-Carlo integration, and variational inference.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,notion,Statistics - Machine Learning},
  file = {/home/baldoinov/baldoinov/PDFs/Livros/Pen and Paper Exercises in Machine Learning_2022_Gutmann.pdf;/home/baldoinov/Zotero/storage/9HVYZZQS/2206.html}
}

@online{PersonalityTraitsLarge_2023_Serapio-GarciaEtAl,
  title = {Personality {{Traits}} in {{Large Language Models}}},
  author = {Serapio-García, Greg and Safdari, Mustafa and Crepy, Clément and Sun, Luning and Fitz, Stephen and Romero, Peter and Abdulhai, Marwa and Faust, Aleksandra and Matarić, Maja},
  date = {2023-09-21},
  eprint = {2307.00184},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2307.00184},
  url = {http://arxiv.org/abs/2307.00184},
  urldate = {2024-07-03},
  abstract = {The advent of large language models (LLMs) has revolutionized natural language processing, enabling the generation of coherent and contextually relevant human-like text. As LLMs increasingly power conversational agents used by the general public world-wide, the synthetic personality embedded in these models, by virtue of training on large amounts of human data, is becoming increasingly important. Since personality is a key factor determining the effectiveness of communication, we present a comprehensive method for administering and validating personality tests on widely-used LLMs, as well as for shaping personality in the generated text of such LLMs. Applying this method, we found: 1) personality measurements in the outputs of some LLMs under specific prompting configurations are reliable and valid; 2) evidence of reliability and validity of synthetic LLM personality is stronger for larger and instruction fine-tuned models; and 3) personality in LLM outputs can be shaped along desired dimensions to mimic specific human personality profiles. We discuss application and ethical implications of the measurement and shaping method, in particular regarding responsible AI.},
  pubstate = {prepublished},
  keywords = {68T35,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Human-Computer Interaction,I.2.7},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Personality Traits in Large Language Models_2023_Serapio-Garcia et al.pdf;/home/baldoinov/Zotero/storage/KVLSMGUJ/2307.html}
}

@video{PessoasQueForam_2021_DesenhandoaBiblia,
  entrysubtype = {video},
  title = {5 Pessoas Que Foram Transformadas Por {{Deus}}},
  editor = {{Desenhando a Bíblia}},
  editortype = {director},
  editora = {Jacob, Patricia},
  editoratype = {scriptwriter},
  date = {2021-06-20},
  volume = {105},
  url = {https://www.youtube.com/watch?v=vcWQRhJOWeA},
  urldate = {2024-09-27},
  abstract = {📖 Como ler a Bíblia todo dia e restituir a sua intimidade com Deus: https://bit.ly/restituindosuavidacomDeus Pelas páginas da Bíblia encontramos inúmeras pessoas que tiveram um encontro com Jesus e foram transformadas por Deus. Só depois desse verdadeiro encontro com o Senhor é que vamos fluir no nosso ministério e até mesmo na vida pessoal ou profissional!  No estudo Bíblico de hoje, veremos 5 pessoas que foram transformadas por Deus e como você e eu também podemos ser! Deus abençoe a sua vida, Patricia Jacob ✔ Seja membro deste canal e nos abençoe ainda mais: https://bit.ly/membrosdesenhandoabiblia ====================================== O canal Desenhando a Bíblia é para você que quer aprender mais sobre a Palavra de Deus de forma dinâmica e didática! ====================================== ✔ REDES SOCIAIS: Instagram: ~~/~desenhando.a.biblia~~ Facebook: ~~/~desenhandoabibliabr~~ ✔  VOZ do canal:  ~~/~patriciajacob\_locutora~~ Contato comercial: ✉ desenhandoabibliabr@gmail.com ====================================== ▶ Aprenda a FAZER VÍDEOS como esse: https://bit.ly/animacao-do-zero ▶ Conheça a vida dos PERSONAGENS BÍBLICOS: https://hotm.art/nyYApc3F ====================================== ▶ NÃO CLIQUE AQUI: https://bit.ly/2knSnrB  ====================================== Áudio: www.bensound.com Imagens: videoscribe e www.pixabay.com "Mas todos nós, com rosto descoberto, refletindo como um espelho a glória do Senhor, somos transformados de glória em glória na mesma imagem, como pelo Espírito do Senhor." (2 Coríntios 3:18) \#presençatransfoormadoradedeus \#presençadedeus \#transformadospelapresençadedeus}
}

@inproceedings{PettingZooGymMultiAgent_2020_TerryEtAl,
  title = {{{PettingZoo}}: {{Gym}} for {{Multi-Agent Reinforcement Learning}}},
  shorttitle = {{{PettingZoo}}},
  author = {Terry, J. K. and Black, Benjamin and Grammel, Nathaniel and Jayakumar, Mario and Hari, Ananth and Sullivan, Ryan and Santos, L. and Perez, Rodrigo and Horsch, Caroline and Dieffendahl, Clemens and Williams, Niall L. and Lokesh, Yashas and Ravi, Praveen},
  date = {2020-09-30},
  url = {https://www.semanticscholar.org/paper/PettingZoo%3A-Gym-for-Multi-Agent-Reinforcement-Terry-Black/3a70562df004e08d91b125e6d15255e31e445efa},
  urldate = {2024-06-07},
  abstract = {This paper introduces the PettingZoo library and the accompanying Agent Environment Cycle ("AEC") games model. PettingZoo is a library of diverse sets of multi-agent environments with a universal, elegant Python API. PettingZoo was developed with the goal of accelerating research in Multi-Agent Reinforcement Learning ("MARL"), by making work more interchangeable, accessible and reproducible akin to what OpenAI's Gym library did for single-agent reinforcement learning. PettingZoo's API, while inheriting many features of Gym, is unique amongst MARL APIs in that it's based around the novel AEC games model. We argue, in part through case studies on major problems in popular MARL environments, that the popular game models are poor conceptual models of games commonly used in MARL and accordingly can promote confusing bugs that are hard to detect, and that the AEC games model addresses these problems.},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/PettingZoo_2020_Terry et al.pdf}
}

@article{PhysicsinformedMachineLearning_2021_KarniadakisEtAl,
  title = {Physics-Informed Machine Learning},
  author = {Karniadakis, George Em and Kevrekidis, Ioannis G. and Lu, Lu and Perdikaris, Paris and Wang, Sifan and Yang, Liu},
  date = {2021-06},
  journaltitle = {Nature Reviews Physics},
  shortjournal = {Nat Rev Phys},
  volume = {3},
  number = {6},
  pages = {422--440},
  publisher = {Nature Publishing Group},
  issn = {2522-5820},
  doi = {10.1038/s42254-021-00314-5},
  url = {https://www.nature.com/articles/s42254-021-00314-5},
  urldate = {2023-12-31},
  abstract = {Despite great progress in simulating multiphysics problems using the numerical discretization of partial differential equations (PDEs), one still cannot seamlessly incorporate noisy data into existing algorithms, mesh generation remains complex, and high-dimensional problems governed by parameterized PDEs cannot be tackled. Moreover, solving inverse problems with hidden physics is often prohibitively expensive and requires different formulations and elaborate computer codes. Machine learning has emerged as a promising alternative, but training deep neural networks requires big data, not always available for scientific problems. Instead, such networks can be trained from additional information obtained by enforcing the physical laws (for example, at random points in the continuous space-time domain). Such physics-informed learning integrates (noisy) data and mathematical models, and implements them through neural networks or other kernel-based regression networks. Moreover, it may be possible to design specialized network architectures that automatically satisfy some of the physical invariants for better accuracy, faster training and improved generalization. Here, we review some of the prevailing trends in embedding physics into machine learning, present some of the current capabilities and limitations and discuss diverse applications of physics-informed learning both for forward and inverse problems, including discovering hidden physics and tackling high-dimensional problems.},
  issue = {6},
  langid = {english},
  keywords = {Applied mathematics,Computational science,notion},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Physics-informed machine learning_2021_Karniadakis et al.pdf}
}

@article{pimm_2014,
  title = {The Biodiversity of Species and Their Rates of Extinction, Distribution, and Protection},
  author = {Pimm, S.L. and Jenkins, C.N. and Abell, R. and Brooks, T.M. and Gittleman, J.L. and Joppa, L.N. and Raven, P.H. and Roberts, C.M. and Sexton, J.O.},
  date = {2014},
  journaltitle = {Science (New York, N.Y.)},
  shortjournal = {Science},
  volume = {344},
  number = {6187}
}

@online{PJotinhaValeMesmo__,
  title = {{{PJotinha}}, Vale Mesmo a Pena?},
  url = {https://speakerdeck.com/bellesamways/pjotinha-vale-mesmo-a-pena?slide=18},
  urldate = {2023-06-17},
  abstract = {“Sou júnior e recebi uma proposta PJ, o que devo fazer?” Nesses slides você vai entender um pouco dos dois lados, o CLT e o PJ. Afinal, qual vale mais a pena? Vou te dar algumas informações importantes para que você tire a sua conclusão dentro da sua realidade.},
  langid = {english},
  organization = {Speaker Deck},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Diversos/PJotinha, vale mesmo a pena_.pdf;/home/baldoinov/Zotero/storage/RWP638DZ/pjotinha-vale-mesmo-a-pena.html}
}

@book{PlanejarEspacosPara_2014_Higgins,
  title = {Planejar espaços para o design de interiores},
  author = {Higgins, Ian},
  date = {2014-10-27},
  publisher = {Gustavo Gili},
  isbn = {978-85-8452-009-1},
  langid = {portuguese},
  keywords = {Arquitetura}
}

@online{PolicyfocusedAgentbasedModeling_2020_OsobaEtAl,
  title = {Policy-Focused {{Agent-based Modeling}} Using {{RL Behavioral Models}}},
  author = {Osoba, Osonde A. and Vardavas, Raffaele and Grana, Justin and Zutshi, Rushil and Jaycocks, Amber},
  date = {2020-11-05},
  eprint = {2006.05048},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2006.05048},
  url = {http://arxiv.org/abs/2006.05048},
  urldate = {2023-06-12},
  abstract = {Agent-based Models (ABMs) are valuable tools for policy analysis. ABMs help analysts explore the emergent consequences of policy interventions in multi-agent decision-making settings. But the validity of inferences drawn from ABM explorations depends on the quality of the ABM agents' behavioral models. Standard specifications of agent behavioral models rely either on heuristic decision-making rules or on regressions trained on past data. Both prior specification modes have limitations. This paper examines the value of reinforcement learning (RL) models as adaptive, high-performing, and behaviorally-valid models of agent decision-making in ABMs. We test the hypothesis that RL agents are effective as utility-maximizing agents in policy ABMs. We also address the problem of adapting RL algorithms to handle multi-agency in games by adapting and extending methods from recent literature. We evaluate the performance of such RL-based ABM agents via experiments on two policy-relevant ABMs: a minority game ABM, and an ABM of Influenza Transmission. We run some analytic experiments on our AI-equipped ABMs e.g. explorations of the effects of behavioral heterogeneity in a population and the emergence of synchronization in a population. The experiments show that RL behavioral models are effective at producing reward-seeking or reward-maximizing behaviors in ABM agents. Furthermore, RL behavioral models can learn to outperform the default adaptive behavioral models in the two ABMs examined.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems,notion},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Policy-focused Agent-based Modeling using RL Behavioral Models_2020_Osoba et al.pdf;/home/baldoinov/Zotero/storage/HF6D5B4C/2006.html}
}

@online{PoliticasPublicasPara__Negri,
  title = {Políticas Públicas para Ciência e Tecnologia no Brasil: cenário e evolução recente},
  author = {Negri, Fernanda De},
  langid = {portuguese},
  pubstate = {prepublished},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Politicas Publicas para Ciencia e Tecnologia no Brasil_Negri.pdf}
}

@article{PoorReadersFeel_2012_MorganEtAl,
  title = {Do {{Poor Readers Feel Angry}}, {{Sad}}, and {{Unpopular}}?},
  author = {Morgan, Paul L. and Farkas, George and Wu, Qiong},
  date = {2012},
  journaltitle = {Scientific studies of reading : the official journal of the Society for the Scientific Study of Reading},
  shortjournal = {Sci Stud Read},
  volume = {16},
  number = {4},
  eprint = {26180489},
  eprinttype = {pmid},
  pages = {360--381},
  issn = {1088-8438},
  doi = {10.1080/10888438.2011.570397},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4500191/},
  urldate = {2024-09-11},
  abstract = {We investigated whether being poorly skilled in reading contributes to children’s self-reported feelings of anger, distractibility, anxiety, sadness, loneliness, and social isolation. Data were analyzed from a longitudinal sub-sample of children (N=2,751) participating in the Early Childhood Longitudinal Study—Kindergarten Cohort. Multi-level logistic regression analyses indicated that poor readers in 3rd grade were more likely to consider themselves as angry, distractible, sad, lonely, and unpopular in 5th grade than those who had not been poor readers in 3rd grade. About 20\% of 3rd grade poor readers reported feeling angry and unpopular in 5th grade. Being poorly skilled in mathematics increased children’s risk of feeling sad or lonely, but not of feeling angry, distractible, or unpopular. The results provide additional empirical evidence that reading failure contributes to generalized socio-emotional maladjustment in young children.},
  pmcid = {PMC4500191},
  file = {/home/baldoinov/baldoinov/PDFs/Mackenzie/projeto-aplicado-iii/Do Poor Readers Feel Angry, Sad, and Unpopular_2012_Morgan et al.pdf}
}

@dataset{PopulationEmployment__FAO,
  title = {Population and {{Employment}}},
  author = {FAO},
  url = {https://www.fao.org/faostat/en/#data/OA},
  urldate = {2023-12-01},
  file = {/home/baldoinov/Zotero/storage/3HYJSRCU/en.html}
}

@online{PortugueseNamedEntity_2020_SouzaEtAl,
  title = {Portuguese {{Named Entity Recognition}} Using {{BERT-CRF}}},
  author = {Souza, Fábio and Nogueira, Rodrigo and Lotufo, Roberto},
  date = {2020-02-27},
  eprint = {1909.10649},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1909.10649},
  url = {http://arxiv.org/abs/1909.10649},
  urldate = {2023-11-16},
  abstract = {Recent advances in language representation using neural networks have made it viable to transfer the learned internal states of a trained model to downstream natural language processing tasks, such as named entity recognition (NER) and question answering. It has been shown that the leverage of pre-trained language models improves the overall performance on many tasks and is highly beneficial when labeled data is scarce. In this work, we train Portuguese BERT models and employ a BERT-CRF architecture to the NER task on the Portuguese language, combining the transfer capabilities of BERT with the structured predictions of CRF. We explore feature-based and fine-tuning training strategies for the BERT model. Our fine-tuning approach obtains new state-of-the-art results on the HAREM I dataset, improving the F1-score by 1 point on the selective scenario (5 NE classes) and by 4 points on the total scenario (10 NE classes).},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval,Computer Science - Machine Learning,notion},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/projeto-heiwa/Portuguese Named Entity Recognition using BERT-CRF_2020_Souza et al.pdf;/home/baldoinov/baldoinov/PDFs/Projetos/projeto-heiwa/Portuguese Named Entity Recognition using BERT-CRF_2020_Souza et al2.pdf;/home/baldoinov/Zotero/storage/ZVB7R2AL/1909.html}
}

@online{PortugueseTweetsSentiment__,
  title = {Portuguese {{Tweets}} for {{Sentiment Analysis}}},
  url = {https://www.kaggle.com/datasets/augustop/portuguese-tweets-for-sentiment-analysis},
  urldate = {2024-02-27},
  abstract = {800k portuguese tweets separated in positive, negative and neutral classes},
  langid = {english},
  file = {/home/baldoinov/Zotero/storage/CMDPNGCH/portuguese-tweets-for-sentiment-analysis.html}
}

@book{PracticalNaturalLanguage_2020_VajjalaEtAl,
  title = {Practical {{Natural Language Processing}}: {{A Comprehensive Guide}} to {{Building Real-World NLP Systems}}},
  shorttitle = {Practical {{Natural Language Processing}}},
  author = {Vajjala, Sowmya and Majumder, Bodhisattwa and Gupta, Anuj and Surana, Harshit},
  date = {2020-06-30},
  edition = {1st edition},
  publisher = {O'Reilly Media},
  location = {Sebastopol, CA},
  abstract = {Many books and courses tackle natural language processing (NLP) problems with toy use cases and well-defined datasets. But if you want to build, iterate, and scale NLP systems in a business setting and tailor them for particular industry verticals, this is your guide. Software engineers and data scientists will learn how to navigate the maze of options available at each step of the journey.Through the course of the book, authors Sowmya Vajjala, Bodhisattwa Majumder, Anuj Gupta, and Harshit Surana will guide you through the process of building real-world NLP solutions embedded in larger product setups. You\&;ll learn how to adapt your solutions for different industry verticals such as healthcare, social media, and retail.With this book, you\&;ll:Understand the wide spectrum of problem statements, tasks, and solution approaches within NLPImplement and evaluate different NLP applications using machine learning and deep learning methodsFine-tune your NLP solution based on your business problem and industry verticalEvaluate various algorithms and approaches for NLP product tasks, datasets, and stagesProduce software solutions following best practices around release, deployment, and DevOps for NLP systemsUnderstand best practices, opportunities, and the roadmap for NLP from a business and product leader\&;s perspective},
  isbn = {978-1-4920-5405-4},
  langid = {english},
  pagetotal = {424}
}

@article{PracticalSurveyFaster_2023_FournierEtAl,
  title = {A {{Practical Survey}} on {{Faster}} and {{Lighter Transformers}}},
  author = {Fournier, Quentin and Caron, Gaétan Marceau and Aloise, Daniel},
  date = {2023-07-17},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {55},
  pages = {304:1--304:40},
  issn = {0360-0300},
  doi = {10.1145/3586074},
  url = {https://doi.org/10.1145/3586074},
  urldate = {2023-07-26},
  abstract = {Recurrent neural networks are effective models to process sequences. However, they are unable to learn long-term dependencies because of their inherent sequential nature. As a solution, Vaswani et al. introduced the Transformer, a model solely based on the attention mechanism that is able to relate any two positions of the input sequence, hence modelling arbitrary long dependencies. The Transformer has improved the state-of-the-art across numerous sequence modelling tasks. However, its effectiveness comes at the expense of a quadratic computational and memory complexity with respect to the sequence length, hindering its adoption. Fortunately, the deep learning community has always been interested in improving the models’ efficiency, leading to a plethora of solutions such as parameter sharing, pruning, mixed-precision, and knowledge distillation. Recently, researchers have directly addressed the Transformer’s limitation by designing lower-complexity alternatives such as the Longformer, Reformer, Linformer, and Performer. However, due to the wide range of solutions, it has become challenging for researchers and practitioners to determine which methods to apply in practice to meet the desired tradeoff between capacity, computation, and memory. This survey addresses this issue by investigating popular approaches to make Transformers faster and lighter and by providing a comprehensive explanation of the methods’ strengths, limitations, and underlying assumptions.},
  issue = {14s},
  keywords = {Deep learning,efficient transformer,notion,self-attention,survey},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/A Practical Survey on Faster and Lighter Transformers_2023_Fournier et al.pdf}
}

@incollection{Preface_2021_Kissell,
  title = {Preface},
  booktitle = {Algorithmic {{Trading Methods}} ({{Second Edition}})},
  editor = {Kissell, Robert L.},
  date = {2021-01-01},
  pages = {xix-xx},
  publisher = {Academic Press},
  doi = {10.1016/B978-0-12-815630-8.05001-8},
  url = {https://www.sciencedirect.com/science/article/pii/B9780128156308050018},
  urldate = {2025-04-21},
  isbn = {978-0-12-815630-8}
}

@article{PreprocessingSchemeHighcardinality_2001_Micci-Barreca,
  title = {A Preprocessing Scheme for High-Cardinality Categorical Attributes in Classification and Prediction Problems},
  author = {Micci-Barreca, Daniele},
  date = {2001-07-01},
  journaltitle = {ACM SIGKDD Explorations Newsletter},
  shortjournal = {SIGKDD Explor. Newsl.},
  volume = {3},
  number = {1},
  pages = {27--32},
  issn = {1931-0145},
  doi = {10.1145/507533.507538},
  url = {https://doi.org/10.1145/507533.507538},
  urldate = {2023-10-25},
  abstract = {Categorical data fields characterized by a large number of distinct values represent a serious challenge for many classification and regression algorithms that require numerical inputs. On the other hand, these types of data fields are quite common in real-world data mining applications and often contain potentially relevant information that is difficult to represent for modeling purposes.This paper presents a simple preprocessing scheme for high-cardinality categorical data that allows this class of attributes to be used in predictive models such as neural networks, linear and logistic regression. The proposed method is based on a well-established statistical method (empirical Bayes) that is straightforward to implement as an in-database procedure. Furthermore, for categorical attributes with an inherent hierarchical structure, like ZIP codes, the preprocessing scheme can directly leverage the hierarchy by blending statistics at the various levels of aggregation.While the statistical methods discussed in this paper were first introduced in the mid 1950's, the use of these methods as a preprocessing step for complex models, like neural networks, has not been previously discussed in any literature.},
  keywords = {categorical attributes,empirical bayes,hierarchical attributes,neural networks,notion,predictive models}
}

@video{PresencaJesusProvoca_2021_Goncalves,
  entrysubtype = {video},
  title = {A {{Presença}} de {{Jesus Provoca Mudanças}}},
  editor = {Gonçalves, Josué},
  editortype = {director},
  namea = {{Mais de Cristo TV}},
  nameatype = {collaborator},
  date = {2021-05-19},
  publisher = {Mais de Cristo TV},
  url = {https://www.youtube.com/watch?v=DKuWd8QhTuQ},
  urldate = {2024-09-30},
  abstract = {Palavra do pastor Josué Gonçalves na Quarta Fé, dia 19 de maio de 2021.   ———————————— Nossa agenda semanal: - Domingo:   - Escola Bíblica 9h   - Celebração da Vida 10h, 18h e 20h - Quarta-feira:    - Quarta Fé 18h30 e 20h - Sexta-feira:   - Alive Teens 20h   - Alive 22h ———————————— Confira também: ———————————— Conheça mais da nossa igreja: Site: https://maisdecristo.com.br/ Facebook: ~~/~maisdecristooficial~~ Instagram: ~~/~maisdecristooficial~~ ————————————— Conheça a Mais de Cristo WORSHIP, nossas músicas: YouTube: ~~~/~@maisdecristomusic~~ Facebook: ~~/~maisdecristoworship~~ Instagram: ~~/~maisdecristoworship~~ ———————————— Esperamos você na nossa igreja em Florianópolis, SC na Rua Professor Egídio Ferreira, 200 Capoeiras}
}

@legislation{PresidenciaRepublica_1990_Brasil,
  title = {Presidência Da {{República}}},
  namea = {{Brasil}},
  nameatype = {collaborator},
  date = {1990-12-11},
  journaltitle = {Diário Oficial da União},
  number = {Lei nº 8.112, de  11 de dezembro de  1990.},
  pages = {1},
  url = {http://www.planalto.gov.br/ccivil_03/leis/l8112cons.htm},
  urldate = {2020-04-23},
  abstract = {Dispõe sobre o regime jurídico dos servidores públicos civis da União, das autarquias e das fundações públicas federais.},
  annotation = {Brasília, DF}
}

@legislation{PresidenciaRepublica_1996_Brasil,
  title = {Presidência Da {{República}}},
  namea = {{Brasil}},
  nameatype = {collaborator},
  date = {1996-12-23},
  journaltitle = {Diário Oficial da União},
  volume = {134, n. 248},
  number = {Lei nº 9.394, de 20 de dezembro de 1996.},
  pages = {27834--27841},
  url = {http://www.planalto.gov.br/ccivil_03/leis/l9394.htm},
  urldate = {2020-04-23},
  abstract = {Estabelece as diretrizes e bases da educação nacional.},
  annotation = {Brasília, DF}
}

@legislation{PresidenciaRepublica_2001_Brasil,
  title = {Presidência Da {{República}}},
  namea = {{Brasil}},
  nameatype = {collaborator},
  date = {2001-04-09},
  journaltitle = {Diário Oficial da União},
  number = {Lei nº 10.216, de 6 de abril de 2001.},
  pages = {2},
  url = {http://www.planalto.gov.br/ccivil_03/LEIS/LEIS_2001/L10216.htm},
  urldate = {2020-04-23},
  abstract = {Dispõe sobre a proteção e os direitos das pessoas portadoras de transtornos mentais e redireciona o modelo assistencial em saúde mental.},
  annotation = {Brasília, DF}
}

@legislation{PresidenciaRepublica_2009_Brasil,
  title = {Presidência Da {{República}}},
  namea = {{Brasil}},
  nameatype = {collaborator},
  date = {2009-06-17},
  journaltitle = {Diário Oficial da União},
  number = {Lei nº 11.947, de 16 de junho de 2009.},
  pages = {2},
  url = {http://www.planalto.gov.br/ccivil_03/_ato2007-2010/2009/lei/l11947.htm},
  urldate = {2020-04-23},
  abstract = {Dispõe sobre o atendimento da alimentação escolar e do Programa Dinheiro Direto na Escola aos alunos da educação básica; altera as Leis nos 10.880, de 9 de junho de 2004, 11.273, de 6 de fevereiro de 2006, 11.507, de 20 de julho de 2007; revoga dispositivos da Medida Provisória no 2.178-36, de 24 de agosto de 2001, e a Lei no 8.913, de 12 de julho de 1994; e dá outras providências.},
  annotation = {Brasília, DF}
}

@article{PretrainPromptPredict_2023_LiuEtAl,
  title = {Pre-Train, {{Prompt}}, and {{Predict}}: {{A Systematic Survey}} of {{Prompting Methods}} in {{Natural Language Processing}}},
  shorttitle = {Pre-Train, {{Prompt}}, and {{Predict}}},
  author = {Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
  date = {2023-01-16},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {55},
  number = {9},
  pages = {195:1--195:35},
  issn = {0360-0300},
  doi = {10.1145/3560815},
  url = {https://dl.acm.org/doi/10.1145/3560815},
  urldate = {2024-03-11},
  abstract = {This article surveys and organizes research works in a new paradigm in natural language processing, which we dub “prompt-based learning.” Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P(y|x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x′ that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string x̂, from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: It allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this article, we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g., the choice of pre-trained language models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts but also release other resources, e.g., a website NLPedia–Pretrain including constantly updated survey and paperlist.},
  keywords = {Pre-trained language models,prompting},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/projeto-heiwa/Pre-train, Prompt, and Predict_2023_Liu et al.pdf}
}

@article{PriceindependentWelfarePrescriptions_1980_Roberts,
  title = {Price-Independent Welfare Prescriptions},
  author = {Roberts, Kevin},
  date = {1980-06-01},
  journaltitle = {Journal of Public Economics},
  shortjournal = {Journal of Public Economics},
  volume = {13},
  number = {3},
  pages = {277--297},
  issn = {0047-2727},
  doi = {10.1016/0047-2727(86)90007-1},
  url = {https://www.sciencedirect.com/science/article/pii/0047272786900071},
  urldate = {2023-10-27},
  abstract = {In most welfare analyses it is a commonplace to assume that welfare prescriptions concerning the distribution of income can be made without knowing the price configuration in the economy under consideration. This paper investigates when such price-independent prescriptions are theoritically justified. Unlike most fields of study dealing with the characterization of admissible functional forms, e.g. the study of aggregation in demand theory and social choice theory, the present problem requires an investigation of the interplay between individuals' and society's preferences: demand functions and social welfare functions. Several joint characterization theorems of price-independence are provided.},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Price-independent welfare prescriptions_1980_Roberts.pdf}
}

@book{ProbabilidadeUmCurso_2022_Ross,
  title = {Probabilidade: um curso moderno com aplicações},
  shorttitle = {Probabilidade},
  author = {Ross, Sheldon},
  date = {2022-11-09},
  publisher = {Bookman},
  isbn = {978-85-7780-621-8},
  langid = {portuguese}
}

@book{ProcessamentoLinguagemNatural_2024_CaseliNunes,
  title = {Processamento de Linguagem Natural: Conceitos, Técnicas e Aplicações em Português},
  shorttitle = {Processamento de Linguagem Natural},
  editor = {Caseli, Helena de Medeiros and Nunes, Maria das Graças Volpe},
  date = {2024-10-29},
  edition = {3},
  publisher = {Graça Nunes},
  location = {São Carlos, SP},
  abstract = {A Parte 1 (Introdução) apresenta a área de PLN no Brasil, no Prefácio, e introduz os principais conceitos na Introdução. A Parte 2 (Fala), ao contrário do restante do livro, que trata de processamento de texto, apresenta a área de processamento de fala: seus principais conceitos,  técnicas, recursos e aplicações. A Parte 3 (Palavras) dedica-se a desvendar a morfologia e a atribuição das categorias das palavras. Também aborda o processamento de expressões multipalavras.  A Parte 4 (Estrutura) fornece toda a conceitualização de sintaxe, os principais tipos de análise, suas diferenças, vantagens e desvantagens. A Parte 5 (Significado) trata dos conceitos, modelos e técnicas relativos à apreensão do sentido implicado pela língua escrita. Isso pode ocorrer pelo uso de teorias e modelos simbólicos ou não simbólicos. Questões discursivas e retóricas implicadas pelo texto são tratadas na Parte 6 (Discurso) deste livro. A Parte 7 (Dados e Avaliação) trata de duas questões que norteiam todas as tarefas do PLN: a escolha e a preparação dos dados que alimentam os algoritmos e os métodos e critérios de avaliação dos sistemas criados. Esta parte foi acrescida de um novo capítulo nesta edição. A nova Parte 8 (Geração e Modelos) foi expandida para incluir o capítulo sobre geração de linguagem, dedicado aos principais métodos de geração de língua natural. Os métodos estado-da-arte, por outro lado, são detalhados no capítulo Modelos de Linguagem. A Parte 9 (Interação) cobre dois tipos de sistemas clássicos de PLN que se tornaram muito populares com o comércio eletrônico e, mais recentemente, com os agentes conversacionais: são os sistemas de perguntas e respostas, mais conhecidos na sua denominação em inglês – Question-Answering – e os sistemas de diálogos.  A Parte 10 (Aplicações) ilustra o PLN em várias aplicações, desde as mais clássicas, como recuperação  e extração  de informações, tradução  ou sumarização automática , avaliação de complexidade textual  e correção automática de redações , até as mais atuais, como a detecção de fake news  Na Parte 11 (Domínios) os capítulos ilustram o PLN aplicado a três diferentes domínios – saúde e mais especificamente saúde mental ; direito ou jurídico , particularizando o tratamento das entidades nomeados neste domínio  e a geração automática de ementas jurídicas;  e humanidades digitais– e a um gênero específico de texto, muito desafiador, aquele das redes sociais. A parte 12 (Sociedade) discute algumas questões éticas que a IA, em geral, e o PLN, em particular, têm provocado, pela forma como novas tecnologias têm sido criadas e usadas recentemente. Finalmente, o último capítulo discorre sobre algumas perspectivas para o PLN do português.},
  isbn = {9786501205816},
  langid = {portuguese},
  keywords = {Ciência da Computação,PLN},
  file = {/home/baldoinov/baldoinov/PDFs/Livros/Processamento de Linguagem Natural_2024_Caseli et al.pdf}
}

@article{ProcessoETLEm_2010_FerreiraEtAl,
  title = {O {{Processo ETL}} Em {{Sistemas Data Warehouse}}},
  author = {Ferreira, João and Miranda, Miguel and Abelha, António and Machado, José},
  date = {2010-01-01},
  abstract = {Resumo. Extração, Transformação e Carga (Extract Transform Load -ETL) são procedimentos de uma técnica de Data Warehouse (DW), que é responsável pela extracção de dados de várias fontes, a sua limpeza, optimização e inserção desses dados num DW. Este artigo tem como objectivo demonstrar o funcionamento genérico do processo ETL em sistemas DW. O processo ETL é uma das fases mais críticas na construção de um sistema DW, pois é nesta fase que grandes volumes de dados são processados. Será abordado de forma sucinta, o modo como este processamento ocorre, e ainda, as ferramentas de ETL disponíveis no mercado. Por fim, serão abordados quais os critérios a ter em consideração na escolha de uma destas ferramentas.},
  file = {/home/baldoinov/baldoinov/PDFs/Mackenzie/topicos-de-banco-de-dados/O Processo ETL em Sistemas Data Warehouse_2010_Ferreira et al.pdf}
}

@article{programme_our_2013,
  title = {Our Nutrient World: {{The}} Challenge to Produce More Food and Energy with Less Pollution},
  shorttitle = {Our Nutrient World},
  author = {Programme, United Nations Environment and Management, Global Partnership on Nutrient and Initiative, International Nitrogen},
  date = {2013},
  url = {https://wedocs.unep.org/xmlui/handle/20.500.11822/10747},
  urldate = {2023-12-11},
  abstract = {Without swift and collective action, the next generation will inherit a world where millions may suffer from food insecurity caused by too few nutrients, where the nutrient pollution threats from too much will become more extreme, and where unsustainable use of nutrients will aggravate biodiversity loss and climate change.}
}

@online{PromptDesignEngineering_2024_Amatriain,
  title = {Prompt {{Design}} and {{Engineering}}: {{Introduction}} and {{Advanced Methods}}},
  shorttitle = {Prompt {{Design}} and {{Engineering}}},
  author = {Amatriain, Xavier},
  date = {2024-01-30},
  eprint = {2401.14423},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2401.14423},
  url = {http://arxiv.org/abs/2401.14423},
  urldate = {2024-02-07},
  abstract = {Prompt design and engineering has become an important discipline in just the past few months. In this paper, we provide an introduction to the main concepts and design approaches. We also provide more advanced techniques all the way to those needed to design LLM-based agents. We finish by providing a list of existing tools for prompt engineering.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Software Engineering,notion},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Prompt Design and Engineering_2024_Amatriain.pdf;/home/baldoinov/Zotero/storage/FEZYYR57/2401.html}
}

@unpublished{PromptEngineering_2025_LeeBoonstra,
  title = {Prompt {{Engineering}}},
  author = {{Lee Boonstra}},
  date = {2025-02},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Prompt Engineering_2025_Lee Boonstra.pdf}
}

@online{PromptingAutoregressiveLarge_2023_Bhandari,
  title = {Prompting in {{Autoregressive Large Language Models}}},
  author = {Bhandari, Prabin},
  date = {2023-11-28},
  eprint = {2312.03740},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2312.03740},
  url = {http://arxiv.org/abs/2312.03740},
  urldate = {2024-03-02},
  abstract = {Autoregressive Large Language Models have transformed the landscape of Natural Language Processing. Pre-train and prompt paradigm has replaced the conventional approach of pre-training and fine-tuning for many downstream NLP tasks. This shift has been possible largely due to LLMs and innovative prompting techniques. LLMs have shown great promise for a variety of downstream tasks owing to their vast parameters and huge datasets that they are pre-trained on. However, in order to fully realize their potential, their outputs must be guided towards the desired outcomes. Prompting, in which a specific input or instruction is provided to guide the LLMs toward the intended output, has become a tool for achieving this goal. In this paper, we discuss the various prompting techniques that have been applied to fully harness the power of LLMs. We present a taxonomy of existing literature on prompting techniques and provide a concise survey based on this taxonomy. Further, we identify some open problems in the realm of prompting in autoregressive LLMs which could serve as a direction for future research.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,notion},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Prompting in Autoregressive Large Language Models_2023_Bhandari.pdf;/home/baldoinov/Zotero/storage/VKINHSGL/2312.html}
}

@article{PromptReportSystematic__SchulhoffEtAl,
  title = {The {{Prompt Report}}: {{A Systematic Survey}} of {{Prompting Techniques}}},
  author = {Schulhoff, Sander and Ilie, Michael and Balepur, Nishant and Kahadze, Konstantine and Liu, Amanda and Si, Chenglei and Li, Yinheng and Gupta, Aayush and Han, HyoJung and Schulhoff, Sevien and Dulepet, Pranav Sandeep and Vidyadhara, Saurav and Ki, Dayeon and Agrawal, Sweta and Pham, Chau and Li, Gerson Kroiz Feileen and Tao, Hudson and Srivastava, Ashay and Costa, Hevander Da and Gupta, Saloni and Rogers, Megan L and Goncearenco, Inna and Sarli, Giuseppe and Galynker, Igor and Peskoff, Denis and Carpuat, Marine and White, Jules and Anadkat, Shyamal and Hoyle, Alexander and Resnik, Philip},
  abstract = {Generative Artificial Intelligence (GenAI) systems are being increasingly deployed across all parts of industry and research settings. Developers and end users interact with these systems through the use of prompting or prompt engineering. While prompting is a widespread and highly researched concept, there exists conflicting terminology and a poor ontological understanding of what constitutes a prompt due to the area’s nascency. This paper establishes a structured understanding of prompts, by assembling a taxonomy of prompting techniques and analyzing their use. We present a comprehensive vocabulary of 33 vocabulary terms, a taxonomy of 58 text-only prompting techniques, and 40 techniques for other modalities. We further present a meta-analysis of the entire literature on natural language prefix-prompting.},
  langid = {english},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/projeto-heiwa/The Prompt Report_Schulhoff et al.pdf}
}

@book{ProtestantEthicSpirit_2001_Weber,
  title = {The {{Protestant Ethic}} and the {{Spirit}} of {{Capitalism}}},
  author = {Weber, Max},
  date = {2001-05-16},
  publisher = {Routledge},
  location = {London},
  doi = {10.4324/9780203995808},
  abstract = {Max Weber's best-known and most controversial work, The Protestant Ethic and the Spirit of Capitalism, first published in 1904, remains to this day a powerful and fascinating read. Weber's highly accessible style is just one of many reasons for his continuing popularity. The book contends that the Protestant ethic made possible and encouraged the development of capitalism in the West. Widely considered as the most informed work ever written on the social effects of advanced capitalism, The Protestant Ethic and the Spirit of Capitalism holds its own as one of the most significant books of the twentieth century. The book is one of those rare works of scholarship which no informed citizen can afford to ignore.},
  isbn = {978-0-203-99580-8},
  pagetotal = {320}
}

@online{ProximalPolicyOptimization_2017_SchulmanEtAl,
  title = {Proximal {{Policy Optimization Algorithms}}},
  author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  date = {2017-08-28},
  eprint = {1707.06347},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1707.06347},
  url = {http://arxiv.org/abs/1707.06347},
  urldate = {2024-06-25},
  abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Proximal Policy Optimization Algorithms_2017_Schulman et al.pdf;/home/baldoinov/Zotero/storage/YX62U3IS/1707.html}
}

@book{PublicFinancePublic_2015_Gruber,
  title = {Public Finance and Public Policy},
  author = {Gruber, Jonathan},
  date = {2015-12-28},
  edition = {5th ed. edição},
  publisher = {Worth Publishers},
  location = {New York},
  abstract = {Get to know how public policy is created, implemented, and researched through the inclusion of real-world examples among traditional topics in public finance with Public Finance and Public Policy.},
  isbn = {978-1-4641-4333-5},
  langid = {Inglês},
  keywords = {notion}
}

@book{PublicFinanceTheory_1989_MusgraveEtAl,
  title = {Public {{Finance}} in {{Theory}} and {{Practice}}},
  author = {Musgrave, Richard Abel and Musgrave, Peggy B. and Musgrave, Richard Abel},
  date = {1989},
  series = {{{McGraw-Hill}} International Editions {{Finance}} Series},
  edition = {5. ed},
  publisher = {McGraw-Hill},
  location = {New York, NY},
  isbn = {978-0-07-044127-9 978-0-07-044128-6 978-0-07-100227-1},
  langid = {english},
  pagetotal = {627}
}

@article{pusceddu_2014,
  title = {Chronic and Intensive Bottom Trawling Impairs Deep-Sea Biodiversity and Ecosystem Functioning},
  author = {Pusceddu, Antonio and Bianchelli, Silvia and Martín, Jacobo and Puig, Pere and Palanques, Albert and Masqué, Pere and Danovaro, Roberto},
  date = {2014},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {111},
  number = {24},
  pages = {8861--8866},
  publisher = {National Acad Sciences}
}

@online{PysentimientoPythonToolkit_2023_PerezEtAl,
  title = {Pysentimiento: {{A Python Toolkit}} for {{Opinion Mining}} and {{Social NLP}} Tasks},
  shorttitle = {Pysentimiento},
  author = {Pérez, Juan Manuel and Rajngewerc, Mariela and Giudici, Juan Carlos and Furman, Damián A. and Luque, Franco and Alemany, Laura Alonso and Martínez, María Vanina},
  date = {2023-10-25},
  eprint = {2106.09462},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2106.09462},
  urldate = {2023-11-01},
  abstract = {In recent years, the extraction of opinions and information from user-generated text has attracted a lot of interest, largely due to the unprecedented volume of content in Social Media. However, social researchers face some issues in adopting cutting-edge tools for these tasks, as they are usually behind commercial APIs, unavailable for other languages than English, or very complex to use for non-experts. To address these issues, we present pysentimiento, a comprehensive multilingual Python toolkit designed for opinion mining and other Social NLP tasks. This open-source library brings state-of-the-art models for Spanish, English, Italian, and Portuguese in an easy-to-use Python library, allowing researchers to leverage these techniques. We present a comprehensive assessment of performance for several pre-trained language models across a variety of tasks, languages, and datasets, including an evaluation of fairness in the results.},
  pubstate = {prepublished},
  version = {2},
  keywords = {Computer Science - Computation and Language,notion},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/projeto-heiwa/pysentimiento_2023_Perez et al.pdf;/home/baldoinov/Zotero/storage/CN6VFBMU/2106.html}
}

@book{PythonAlgorithmicTrading_2020_Hilpisch,
  title = {Python for {{Algorithmic Trading}}: From Idea to Cloud Deployment},
  shorttitle = {Python for Algorithmic Trading},
  author = {Hilpisch, Yves},
  date = {2020},
  publisher = {O'Reilly},
  location = {Beijing},
  isbn = {978-1-4920-5335-4},
  langid = {english},
  pagetotal = {358}
}

@book{PythonDataAnalysis_2018_McKinney,
  title = {Python for {{Data Analysis}}: Data Wrangling with Pandas, {{NumPy}}, and {{IPython}}},
  shorttitle = {Python for Data Analysis},
  author = {McKinney, Wes},
  date = {2018},
  edition = {Second edition},
  publisher = {O'Reilly Media, Inc},
  location = {Sebastopol, California},
  abstract = {"Get complete instructions for manipulating, processing, cleaning, and crunching datasets in Python. Updated for Python 3.6, the second edition of this hands-on guide is packed with practical case studies that show you how to solve a broad set of data analysis problems effectively. You'll learn the latest versions of pandas, NumPy, IPython, and Jupyter in the process"--Page 4 of cover},
  isbn = {978-1-4919-5766-0},
  pagetotal = {524},
  keywords = {Data mining,Data Mining,Datenanalyse,Datenmanagement,Programming languages (Electronic computers),Python (Computer program language),Python 3.6},
  annotation = {OCLC: ocn959595088}
}

@book{PythonDataScience_2016_Vanderplas,
  title = {Python {{Data Science Handbook}}: Essential Tools for Working with Data},
  shorttitle = {Python Data Science Handbook},
  author = {Vanderplas, Jacob T.},
  date = {2016},
  edition = {First edition},
  publisher = {O'Reilly Media, Inc},
  location = {Sebastopol, CA},
  isbn = {978-1-4919-1205-8},
  pagetotal = {529},
  keywords = {Data mining,Data Mining,Datenanalyse,Datenmanagement,Python,Python (Computer program language)},
  annotation = {OCLC: ocn915498936}
}

@book{PythonFinanceMastering_2019_Hilpisch,
  title = {Python for {{Finance}}: Mastering Data-Driven Finance},
  shorttitle = {Python for Finance},
  author = {Hilpisch, Yves J.},
  date = {2019},
  edition = {Second edition},
  publisher = {O'Reilly Media},
  location = {Sebastopol, CA},
  isbn = {978-1-4920-2433-0},
  pagetotal = {691},
  keywords = {Data processing,Finance,Financial engineering,Programming languages (Electronic computers),Python (Computer program language),Statistical methods Data processing}
}

@online{PyTorchImperativeStyle_2019_PaszkeEtAl,
  title = {{{PyTorch}}: {{An Imperative Style}}, {{High-Performance Deep Learning Library}}},
  shorttitle = {{{PyTorch}}},
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Köpf, Andreas and Yang, Edward and DeVito, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  date = {2019-12-03},
  eprint = {1912.01703},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1912.01703},
  url = {http://arxiv.org/abs/1912.01703},
  urldate = {2024-08-16},
  abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Mathematical Software,Statistics - Machine Learning},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/projeto-heiwa/PyTorch_2019_Paszke et al.pdf;/home/baldoinov/Zotero/storage/WTCZ888L/1912.html}
}

@article{Qlearning_1992_WatkinsDayan,
  title = {Q-Learning},
  author = {Watkins, Christopher J. C. H. and Dayan, Peter},
  date = {1992-05-01},
  journaltitle = {Machine Learning},
  shortjournal = {Mach Learn},
  volume = {8},
  number = {3},
  pages = {279--292},
  issn = {1573-0565},
  doi = {10.1007/BF00992698},
  url = {https://doi.org/10.1007/BF00992698},
  urldate = {2024-06-25},
  abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states.},
  langid = {english},
  keywords = {asynchronous dynamic programming,Q-learning,reinforcement learning,temporal differences},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Q-learning_1992_Watkins et al.pdf}
}

@article{QLearningTheoryApplications_2020_CliftonLaber,
  title = {Q-{{Learning}}: {{Theory}} and {{Applications}}},
  shorttitle = {Q-{{Learning}}},
  author = {Clifton, Jesse and Laber, Eric},
  date = {2020-03-07},
  journaltitle = {Annual Review of Statistics and Its Application},
  volume = {7},
  pages = {279--301},
  publisher = {Annual Reviews},
  issn = {2326-8298, 2326-831X},
  doi = {10.1146/annurev-statistics-031219-041220},
  url = {https://www.annualreviews.org/content/journals/10.1146/annurev-statistics-031219-041220},
  urldate = {2024-06-24},
  abstract = {Q-learning, originally an incremental algorithm for estimating an optimal decision strategy in an infinite-horizon decision problem, now refers to a general class of reinforcement learning methods widely used in statistics and artificial intelligence. In the context of personalized medicine, finite-horizon Q-learning is the workhorse for estimating optimal treatment strategies, known as treatment regimes. Infinite-horizon Q-learning is also increasingly relevant in the growing field of mobile health. In computer science, Q-learning methods have achieved remarkable performance in domains such as game-playing and robotics. In this article, we (a) review the history of Q-learning in computer science and statistics, (b) formalize finite-horizon Q-learning within the potential outcomes framework and discuss the inferential difficulties for which it is infamous, and (c) review variants of infinite-horizon Q-learning and the exploration-exploitation problem, which arises in decision problems with a long time horizon. We close by discussing issues arising with the use of Q-learning in practice, including arguments for combining Q-learning with direct-search methods; sample size considerations for sequential, multiple assignment randomized trials; and possibilities for combining Q-learning with model-based methods.},
  issue = {Volume 7, 2020},
  langid = {english},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Q-Learning_2020_Clifton et al.pdf;/home/baldoinov/Zotero/storage/Q2349XNY/annurev-statistics-031219-041220.html}
}

@video{QuerAprenderMontar_2022_GoncalvesJesusCopy,
  entrysubtype = {video},
  title = {Quer Aprender a Montar Uma Pregação?},
  shorttitle = {{{QUER APRENDER A MONTAR UMA PREGAÇÃO}}?},
  editor = {Gonçalves, Douglas and {JesusCopy}},
  editortype = {director},
  date = {2022-05-04},
  url = {https://www.youtube.com/watch?v=WTK2v6mSw3Q},
  urldate = {2024-09-13},
  abstract = {Neste vídeo vamos te mostrar passo a passo como elaborar um esboço. Quero te mostrar como faço para preparar um sermão. Sigo alguns métodos simples que tenho certeza que irão te ajudar muito. Espero que este pequeno vídeo te ajude e te inspire.  Para receber todos os Esboços que mostrei no vídeo, inscreva-se na Conferência LIDERE COMO JESUS que será dia 28/05/2022 - online e gratuito.  LINK DA CONFERÊNCIA (Todos os inscritos vão ganhar os Esboços): https://link.jesuscopy.com/conf\_lider... Siga a Valéria  (@val\_jesuscopy) e o Douglas (@douglas\_jesuscopy) no instagram. Para saber mais sobre o JesusCopy: 1. Se inscreva em nosso canal aqui do Youtube: ~~~/~@jesus\_copy~~ 2. Participe da nossa lista de emails: http://conteudo.jesuscopy.com/dicas-j... 3. Entre no nosso canal do TELEGRAM: https://t.me/jesuscopyoficial 4. Siga nosso Instagram: ~~/~jesus\_copy~~ 5. Curta nossa página no facebook: ~~/~jesuscopy~~ 6. Siga-nos no Twitter: ~~/~jesus\_copy~~ 7. Acesse nosso site: http://jesuscopy.com/ O que é o JesusCopy? Somos um movimento digital que existe para mobilizar pessoas ao entendimento inteligente do Evangelho que gera cópias autênticas de Jesus. Nossa missão é acelerar e facilitar o desenvolvimento de Cópias de Jesus que discipulam nações participando do avanço global do Reino de Deus.}
}

@book{QuimicaVolumeUnico_2021_UsbercoSalvador,
  title = {Quimica - Volume Unico - Livro Do Aluno},
  author = {Usberco, João and Salvador, Edgard},
  date = {2021-10-01},
  publisher = {Editora Saraiva},
  isbn = {978-85-02-04027-4},
  langid = {brazilian}
}

@book{RacismoCruzCristao_2012_Piper,
  title = {O Racismo, a Cruz e o Cristão: a Nova Linhagem em Cristo},
  shorttitle = {Racismo, a Cruz e o Cristão, o},
  author = {Piper, John},
  date = {2012-01-01},
  edition = {1ª edição},
  publisher = {Edições Vida Nova},
  abstract = {O racismo, o ódio e o sentimento de superioridade racial têm sido elementos trágicos da condição humana desde a Queda, no mundo inteiro. E a cada vez que esses elementos se manifestam, encontramos por trás deles, bem na raiz do pecado racial, um coração incrédulo que resiste à graça e à misericórdia de Deus.O evangelho de Jesus Cristo é a única esperança de chegarmos a soluções de fato significativas para o problema racial. É isso que John Piper nos mostra neste livro, quando lança a luz do evangelho sobre essa questão.Além de confessar seus próprios pecados e sua experiência pessoal com tensões raciais, ele conta também como Deus tem transformado sua vida e sua igreja. Piper expõe aos olhos dos leitores a realidade e a extensão do racismo e, a seguir, demonstra, a partir das Escrituras, como a luz do evangelho atravessa as trevas sombrias desse pecado tão destrutivo.},
  isbn = {978-85-275-0492-8},
  langid = {portuguese}
}

@video{RacismoOsDesafios_2024_JesusCopy,
  entrysubtype = {video},
  title = {Racismo e Os Desafios de Uma Imigrante Africana},
  editor = {{JesusCopy}},
  editortype = {director},
  namea = {Gonçalves, Douglas and Monteiro, Jacira},
  nameatype = {collaborator},
  date = {2024-04-15},
  url = {https://www.youtube.com/watch?v=9Tisidc-Bcc},
  urldate = {2024-10-25},
  abstract = {Explore a jornada de uma imigrante africana na escola evangélica brasileira, abordando identidade, racismo e resiliência. Da Guiné-Bissau ao Brasil, conheça sua adaptação cultural e a interseção de línguas. Uma narrativa inspiradora sobre superação, apoio familiar e construção de identidade em meio a desafios. Livro: O estigma da cor - https://ler.amazon.com.br/kp/embed?as... SE INSCREVA NO CANAL JESUSCOPY https://bit.ly/Jesus\_Copy 👉 Você ainda NÃO viu isso: ✔ Juliano Son e Douglas Gonçalves:  ~~~•~JULIANO~SON~-~JesusCopy~Podcast~\#127~~ ✔  Luiz Herminio - O Ministério Profético:  ~~~•~LUIZ~HERMINIO~-~JesusCopy~Podcast~\#122~~ ✔ Thamires Garcia - Meu Testemunho:   ~~~•~THAMIRES~GARCIA~(Segunda~participação...~~ ✔ Rodolfo Abrantes - A Recompensa:  ~~~•~RODOLFO~ABRANTES~-~A~RECOMPENSA~DE~EN...~~ SE INSCREVA NO CANAL JESUSCOPY 👍 https://bit.ly/Jesus\_Copy \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ O que é o JesusCopy? Somos um movimento digital que existe para mobilizar pessoas ao entendimento inteligente do Evangelho que gera cópias autênticas de Jesus.  Nossa missão é acelerar e facilitar o desenvolvimento de Cópias de Jesus que discipulam nações participando do avanço global do Reino de Deus. \#jesuscopy}
}

@article{RationalExpectationsMicroeconomic_1982_JordanRadner,
  title = {Rational Expectations in Microeconomic Models: {{An}} Overview},
  shorttitle = {Rational Expectations in Microeconomic Models},
  author = {Jordan, James S and Radner, Roy},
  date = {1982-04-01},
  journaltitle = {Journal of Economic Theory},
  shortjournal = {Journal of Economic Theory},
  volume = {26},
  number = {2},
  pages = {201--223},
  issn = {0022-0531},
  doi = {10.1016/0022-0531(82)90001-1},
  url = {https://www.sciencedirect.com/science/article/pii/0022053182900011},
  urldate = {2024-06-14},
  abstract = {This paper is an expository introduction to several topics of current research in the general equilibrium theory of rational expectations. More specifically, we discuss the existence of exact and approximate rational expectations equilibria, the implementation of equilibria, the behavior of learning and smoothing processes by which traders construct expectations from repeated observations of the market, and the lagged use of the information revealed by prices in an intertemporal sequence of markets. The purpose of this discussion is to introduce papers on these topics appearing in the Journal of Economic Theory Symposium on Rational Expectations in Microeconomic Models.},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Rational expectations in microeconomic models_1982_Jordan et al.pdf;/home/baldoinov/Zotero/storage/XZFB9BWM/0022053182900011.html}
}

@article{RecommenderSystemsElearning_2021_ZhangEtAl,
  title = {Recommender {{Systems}} in {{E-learning}}},
  author = {Zhang, Qian and Lu, Jie and Zhang, Guangquan},
  date = {2021-04-29},
  journaltitle = {Journal of Smart Environments and Green Computing},
  shortjournal = {jsegc},
  volume = {1},
  number = {2},
  pages = {76--89},
  publisher = {OAE Publishing Inc.},
  issn = {ISSN 2767-6595 (Online)},
  doi = {10.20517/jsegc.2020.06},
  url = {https://www.oaepublish.com/articles/jsegc.2020.06},
  urldate = {2024-08-16},
  abstract = {In this era when every aspect of society is accelerating, people are always seeking improvement to stay competitive in their careers. E-learning systems fit into the ever challenging situation and provide learners with remote learning opportunities and abundant learning resources. Facing with the numerous resources online, users need support in deciding which course to take, thus recommender systems are applied in E-learning to provide learners with personalized services by automatically identifying their preferences. This position paper systematically discusses the main recommendation techniques employed in in E-learning and identifies new research directions. Three main recommendation techniques are reviewed in this paper: content-based, collaborative filtering-based and knowledge-based recommendations. The basic mechanism of these technique together with how they are used to fulfill the specific requirements in the context of E-learning are highlighted and presented. The observations in this paper could support researchers and practitioners to better understand the current development and future directions of recommender systems in E-learning.},
  langid = {english},
  file = {/home/baldoinov/baldoinov/PDFs/Mackenzie/projeto-aplicado-iii/Recommender Systems in E-learning_2021_Zhang et al.pdf}
}

@thesis{ReconhecimentoEmocoesEm_2022_Salles,
  type = {Trabalho de Conclusão de Curso},
  title = {Reconhecimento de emoções em mineração de argumentos com deep learning},
  author = {Salles, Gabriel Tardochi},
  namea = {Bisacchi Coelho, Orlando},
  nameatype = {collaborator},
  date = {2022-06},
  institution = {Universidade Presbiteriana Mackenzie},
  url = {https://dspace.mackenzie.br/handle/10899/30890},
  urldate = {2024-09-02},
  abstract = {Como uma das áreas de pesquisa promissoras em Inteligência Artificial, a Mineração de Argumentos fornece maneiras automatizadas para a extração de informações de dados textuais não estruturados gerados no contexto de uma argumentação. Avanços na melhor compreensão das discussões podem facilitar a tomada de decisão baseada em dados, o que significa produtos de maior qualidade e oportunidades reais para aprimorar o bem social. Neste trabalho, nos concentramos na tarefa específica do reconhecimento de emoções granulares em discussões online. Usando técnicas e arquiteturas de Deep Learning, são obtidos resultados um pouco superiores aos já publicados na literatura.},
  langid = {brazilian},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/projeto-heiwa/Reconhecimento de emocoes em mineracao de argumentos com deep learning_2022_Salles.pdf}
}

@article{ReconhecimentoEmocoesEm_2022_SallesBisacchiCoelho,
  title = {Reconhecimento de emoções em mineração de argumentos com deep learning},
  author = {Salles, Gabriel Tardochi and Bisacchi Coelho, Orlando},
  date = {2022-06},
  publisher = {Universidade Presbiteriana Mackenzie},
  url = {https://dspace.mackenzie.br/handle/10899/30890},
  urldate = {2023-04-15},
  abstract = {Como uma das áreas de pesquisa promissoras em Inteligência  Artificial, a Mineração de Argumentos fornece maneiras automatizadas para a  extração de informações de dados textuais não estruturados gerados no  contexto de uma argumentação. Avanços na melhor compreensão das  discussões podem facilitar a tomada de decisão baseada em dados, o que  significa produtos de maior qualidade e oportunidades reais para aprimorar o  bem social. Neste trabalho, nos concentramos na tarefa específica do  reconhecimento de emoções granulares em discussões online. Usando  técnicas e arquiteturas de Deep Learning, são obtidos resultados um pouco  superiores aos já publicados na literatura.},
  langid = {brazilian},
  keywords = {notion},
  annotation = {Accepted: 2022-10-19T14:16:45Z},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/projeto-heiwa/Reconhecimento de emocoes em mineracao de argumentos com deep learning_2022_Salles et al.pdf}
}

@article{ReferenceArchitectureClassification_2015_PaakkonenPakkala,
  title = {Reference {{Architecture}} and {{Classification}} of {{Technologies}}, {{Products}} and {{Services}} for {{Big Data Systems}}},
  author = {Pääkkönen, Pekka and Pakkala, Daniel},
  date = {2015-12-01},
  journaltitle = {Big Data Research},
  shortjournal = {Big Data Research},
  volume = {2},
  number = {4},
  pages = {166--186},
  issn = {2214-5796},
  doi = {10.1016/j.bdr.2015.01.001},
  url = {https://www.sciencedirect.com/science/article/pii/S2214579615000027},
  urldate = {2025-03-09},
  abstract = {Many business cases exploiting big data have been realised in recent years; Twitter, LinkedIn, and Facebook are examples of companies in the social networking domain. Other big data use cases have focused on capturing of value from streaming of movies (Netflix), monitoring of network traffic, or improvement of processes in the manufacturing industry. Also, implementation architectures of the use cases have been published. However, conceptual work integrating the approaches into one coherent reference architecture has been limited. The contribution of this paper is technology independent reference architecture for big data systems, which is based on analysis of published implementation architectures of big data use cases. An additional contribution is classification of related implementation technologies and products/services, which is based on analysis of the published use cases and survey of related work. The reference architecture and associated classification are aimed for facilitating architecture design and selection of technologies or commercial solutions, when constructing big data systems.},
  keywords = {Big data,Classification,Literature survey,Reference architecture},
  file = {/home/baldoinov/baldoinov/PDFs/Mackenzie/ecossistemas-de-big-data-ii/Reference Architecture and Classification of Technologies, Products and_2015_Paakkonen et al.pdf}
}

@incollection{References_2021_Kissell,
  title = {References},
  booktitle = {Algorithmic {{Trading Methods}} ({{Second Edition}})},
  editor = {Kissell, Robert L.},
  date = {2021-01-01},
  pages = {569--576},
  publisher = {Academic Press},
  doi = {10.1016/B978-0-12-815630-8.16001-6},
  url = {https://www.sciencedirect.com/science/article/pii/B9780128156308160016},
  urldate = {2025-04-21},
  isbn = {978-0-12-815630-8}
}

@book{RegressionModelingStrategies_2015_Harrell,
  title = {Regression {{Modeling Strategies}}: {{With Applications}} to {{Linear Models}}, {{Logistic}} and {{Ordinal Regression}}, and {{Survival Analysis}}},
  shorttitle = {Regression {{Modeling Strategies}}},
  author = {Harrell, Frank E},
  date = {2015},
  series = {Springer {{Series}} in {{Statistics}}},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-19425-7},
  url = {https://link.springer.com/10.1007/978-3-319-19425-7},
  urldate = {2024-04-01},
  isbn = {978-3-319-19424-0 978-3-319-19425-7},
  langid = {english},
  file = {/home/baldoinov/baldoinov/PDFs/Livros/Regression Modeling Strategies_2015_Harrell.pdf}
}

@book{RegressionModelsMethods_2013_FahrmeirEtAl,
  title = {Regression: {{Models}}, {{Methods}} and {{Applications}}},
  shorttitle = {Regression},
  author = {Fahrmeir, Ludwig and Kneib, Thomas and Lang, Stefan and Marx, Brian},
  date = {2013},
  publisher = {Springer Berlin Heidelberg},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-34333-9},
  url = {https://link.springer.com/10.1007/978-3-642-34333-9},
  urldate = {2024-04-01},
  isbn = {978-3-642-34332-2 978-3-642-34333-9},
  langid = {english},
  file = {/home/baldoinov/baldoinov/PDFs/Livros/Regression_2013_Fahrmeir et al.pdf}
}

@video{ReinforcementLearningCommercial_2021_Chakravorty,
  entrysubtype = {video},
  title = {Reinforcement {{Learning}}'s {{Commercial Use Cases}} and {{Intellectual Ramifications}}},
  editor = {Chakravorty, Dibya},
  editortype = {director},
  date = {2021-09-24},
  url = {https://youtu.be/mw1YjZVjurA?si=z5c0n1qRm-z3gWgU},
  urldate = {2024-06-24},
  file = {/home/baldoinov/Zotero/storage/6LL5REZF/watch.html}
}

@article{ReinforcementLearningEconomics_2023_CharpentierEtAl,
  title = {Reinforcement {{Learning}} in {{Economics}} and {{Finance}}},
  author = {Charpentier, Arthur and Élie, Romuald and Remlinger, Carl},
  date = {2023-06-01},
  journaltitle = {Computational Economics},
  shortjournal = {Comput Econ},
  volume = {62},
  number = {1},
  pages = {425--462},
  issn = {1572-9974},
  doi = {10.1007/s10614-021-10119-4},
  url = {https://doi.org/10.1007/s10614-021-10119-4},
  urldate = {2023-08-18},
  abstract = {Reinforcement learning algorithms describe how an agent can learn an optimal action policy in a sequential decision process, through repeated experience. In a given environment, the agent policy provides him some running and terminal rewards. As in online learning, the agent learns sequentially. As in multi-armed bandit problems, when an agent picks an action, he can not infer ex-post the rewards induced by other action choices. In reinforcement learning, his actions have consequences: they influence not only rewards, but also future states of the world. The goal of reinforcement learning is to find an optimal policy – a mapping from the states of the world to the set of actions, in order to maximize cumulative reward, which is a long term strategy. Exploring might be sub-optimal on a short-term horizon but could lead to optimal long-term ones. Many problems of optimal control, popular in economics for more than forty years, can be expressed in the reinforcement learning framework, and recent advances in computational science, provided in particular by deep learning algorithms, can be used by economists in order to solve complex behavioral problems. In this article, we propose a state-of-the-art of reinforcement learning techniques, and present applications in economics, game theory, operation research and finance.},
  langid = {english},
  keywords = {C18,C41,C44,C54,C57,C61,C63,C68,C70,C90,Causality,Control,D40,D70,D83,Machine learning,Markov decision process,Multi-armed bandits,notion,Online-learning,Q-learning,Regret,Reinforcement learning,Rewards,Sequential learning},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Reinforcement Learning in Economics and Finance_2023_Charpentier et al.pdf}
}

@article{ReinforcementLearningInterpretation_2020_AlharinEtAl,
  title = {Reinforcement {{Learning Interpretation Methods}}: {{A Survey}}},
  shorttitle = {Reinforcement {{Learning Interpretation Methods}}},
  author = {Alharin, Alnour and Doan, Thanh-Nam and Sartipi, Mina},
  date = {2020},
  journaltitle = {IEEE Access},
  volume = {8},
  pages = {171058--171077},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2020.3023394},
  url = {https://ieeexplore.ieee.org/abstract/document/9194697},
  urldate = {2023-12-05},
  abstract = {Reinforcement Learning (RL) systems achieved outstanding performance in different domains such as Atari games, finance, healthcare, and self-driving cars. However, their black-box nature complicates their use, especially in critical applications such as healthcare. To solve this problem, researchers have proposed different approaches to interpret RL models. Some of these methods were adopted from machine learning, while others were designed specifically for RL. The main objective of this paper is to show and explain RL interpretation methods, the metrics used to classify them, and how these metrics were applied to understand the internal details of RL models. We reviewed papers that propose new RL interpretation methods, improve the old ones, or discuss the pros and cons of the existing methods.},
  eventtitle = {{{IEEE Access}}},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Reinforcement Learning Interpretation Methods_2020_Alharin et al.pdf;/home/baldoinov/Zotero/storage/VJ82QC2P/9194697.html}
}

@book{ReinforcementLearningIntroduction_2018_SuttonBarto,
  title = {Reinforcement {{Learning}}: {{An Introduction}}},
  shorttitle = {Reinforcement {{Learning}}, Second Edition},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  date = {2018-11-13},
  edition = {2nd edition},
  publisher = {Bradford Books},
  location = {Cambridge, Massachusetts},
  abstract = {The significantly expanded and updated new edition of a widely used text on reinforcement learning, one of the most active research areas in artificial intelligence.Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms. This second edition has been significantly expanded and updated, presenting new topics and updating coverage of other topics.Like the first edition, this second edition focuses on core online learning algorithms, with the more mathematical material set off in shaded boxes. Part I covers as much of reinforcement learning as possible without going beyond the tabular case for which exact solutions can be found. Many algorithms presented in this part are new to the second edition, including UCB, Expected Sarsa, and Double Learning. Part II extends these ideas to function approximation, with new sections on such topics as artificial neural networks and the Fourier basis, and offers expanded treatment of off-policy learning and policy-gradient methods. Part III has new chapters on reinforcement learning's relationships to psychology and neuroscience, as well as an updated case-studies chapter including AlphaGo and AlphaGo Zero, Atari game playing, and IBM Watson's wagering strategy. The final chapter discusses the future societal impacts of reinforcement learning.},
  isbn = {978-0-262-03924-6},
  langid = {english},
  pagetotal = {552},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Livros/Reinforcement Learning_2018_Sutton et al.pdf}
}

@book{ReinventingBazaarNatural_2003_McMillan,
  title = {Reinventing the Bazaar: A Natural History of Markets},
  shorttitle = {Reinventing the Bazaar},
  author = {McMillan, John},
  date = {2003-11-17},
  edition = {Reprint edição},
  publisher = {W. W. Norton \& Company},
  location = {New York, NY},
  abstract = {Clear, insightful, and nondogmatic, this book gives us a new appreciation for one of our most ubiquitous institutions.From the wild swings of the stock market to the online auctions of eBay to the unexpected twists of the world's post-Communist economies, markets have suddenly become quite visible. We now have occasion to ask, "What makes these institutions work? How important are they? How can we improve them?"Taking us on a lively tour of a world we once took for granted, John McMillan offers examples ranging from a camel trading fair in India to the \$20 million per day Aalsmeer flower market in the Netherlands to the global trade in AIDS drugs. Eschewing ideology, he shows us that markets are neither magical nor immoral. Rather, they are powerful if imperfect tools, the best we've found for improving our living standards.A New York Times Notable Book.},
  isbn = {978-0-393-32371-9},
  langid = {Inglês}
}

@incollection{RemarksFoundationsAgentBased_2006_Epstein,
  title = {Remarks on the {{Foundations}} of {{Agent-Based Generative Social Science}}},
  booktitle = {Handbook of {{Computational Economics}}},
  author = {Epstein, Joshua M.},
  editor = {Tesfatsion, L. and Judd, K. L.},
  date = {2006-01-01},
  volume = {2},
  pages = {1585--1604},
  publisher = {Elsevier},
  doi = {10.1016/S1574-0021(05)02034-4},
  url = {https://www.sciencedirect.com/science/article/pii/S1574002105020344},
  urldate = {2023-09-27},
  abstract = {This chapter treats a variety of epistemological issues surrounding generative explanation in the social sciences, and discusses the role of agent-based computational models in generative social science.},
  keywords = {agent-based modeling,generative social science,notion,philosophy of social science},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Remarks on the Foundations of Agent-Based Generative Social Science_2006_Epstein.pdf}
}

@article{ResourceLimitationsTaxonomy_2023_YoungHowatt,
  title = {Resource Limitations: {{A}} Taxonomy},
  shorttitle = {Resource Limitations},
  author = {Young, Michael E. and Howatt, Brian C.},
  date = {2023-03-01},
  journaltitle = {Behavioural Processes},
  shortjournal = {Behavioural Processes},
  volume = {206},
  pages = {104823},
  issn = {0376-6357},
  doi = {10.1016/j.beproc.2023.104823},
  url = {https://www.sciencedirect.com/science/article/pii/S0376635723000050},
  urldate = {2024-06-07},
  abstract = {Decision making within the context of resource limitations requires balancing the short-term benefits of obtaining a resource and the long-term consequences of depleting those resources. The present manuscript focuses on four types of tasks that share this tradeoff to develop a taxonomy that will encourage a deeper understanding of the psychological processes at play. The four types considered are foraging, common pool traps, deterioration traps, and a novel designation referred to as resource cliffs. All four will be shown to include two opposite processes – depletion of the resource and its replenishment over time. By considering the unique and shared features of these tasks, a taxonomy of features emerges that can be combined to not only create novel tasks but also to shift the research focus to task features rather than specific tasks. The paper closes with a consideration of current theoretical frameworks previously applied to one or more of these resource-limitation tasks as well as the promise of reinforcement learning as a unifying theory.},
  keywords = {Common pool traps,Depletion,Deterioration traps,Foraging,Reinforcement learning,Replenishment,Resource limitations,Tragedy of the commons},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Resource limitations_2023_Young et al.pdf}
}

@online{ReSTMeetsReAct_2023_AksitovEtAl,
  title = {{{ReST}} Meets {{ReAct}}: {{Self-Improvement}} for {{Multi-Step Reasoning LLM Agent}}},
  shorttitle = {{{ReST}} Meets {{ReAct}}},
  author = {Aksitov, Renat and Miryoosefi, Sobhan and Li, Zonglin and Li, Daliang and Babayan, Sheila and Kopparapu, Kavya and Fisher, Zachary and Guo, Ruiqi and Prakash, Sushant and Srinivasan, Pranesh and Zaheer, Manzil and Yu, Felix and Kumar, Sanjiv},
  date = {2023-12-15},
  eprint = {2312.10003},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2312.10003},
  url = {http://arxiv.org/abs/2312.10003},
  urldate = {2023-12-31},
  abstract = {Answering complex natural language questions often necessitates multi-step reasoning and integrating external information. Several systems have combined knowledge retrieval with a large language model (LLM) to answer such questions. These systems, however, suffer from various failure cases, and we cannot directly train them end-to-end to fix such failures, as interaction with external knowledge is non-differentiable. To address these deficiencies, we define a ReAct-style LLM agent with the ability to reason and act upon external knowledge. We further refine the agent through a ReST-like method that iteratively trains on previous trajectories, employing growing-batch reinforcement learning with AI feedback for continuous self-improvement and self-distillation. Starting from a prompted large model and after just two iterations of the algorithm, we can produce a fine-tuned small model that achieves comparable performance on challenging compositional question-answering benchmarks with two orders of magnitude fewer parameters.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,notion},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/ReST meets ReAct_2023_Aksitov et al.pdf;/home/baldoinov/Zotero/storage/3WQHFG6Y/2312.html}
}

@inproceedings{ResultsWNUT2017Shared_2017_DerczynskiEtAl,
  title = {Results of the {{WNUT2017 Shared Task}} on {{Novel}} and {{Emerging Entity Recognition}}},
  booktitle = {Proceedings of the 3rd {{Workshop}} on {{Noisy User-generated Text}}},
  author = {Derczynski, Leon and Nichols, Eric and family=Erp, given=Marieke, prefix=van, useprefix=true and Limsopatham, Nut},
  editor = {Derczynski, Leon and Xu, Wei and Ritter, Alan and Baldwin, Tim},
  date = {2017-09},
  pages = {140--147},
  publisher = {Association for Computational Linguistics},
  location = {Copenhagen, Denmark},
  doi = {10.18653/v1/W17-4418},
  url = {https://aclanthology.org/W17-4418},
  urldate = {2024-03-18},
  abstract = {This shared task focuses on identifying unusual, previously-unseen entities in the context of emerging discussions. Named entities form the basis of many modern approaches to other tasks (like event clustering and summarization), but recall on them is a real problem in noisy text - even among annotators. This drop tends to be due to novel entities and surface forms. Take for example the tweet “so.. kktny in 30 mins⁈” – even human experts find the entity `kktny' hard to detect and resolve. The goal of this task is to provide a definition of emerging and of rare entities, and based on that, also datasets for detecting these entities. The task as described in this paper evaluated the ability of participating entries to detect and classify novel and emerging named entities in noisy text.},
  eventtitle = {{{WNUT}} 2017},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/projeto-heiwa/Results of the WNUT2017 Shared Task on Novel and Emerging Entity Recognition_2017_Derczynski et al.pdf}
}

@online{RetrievalAugmentedGenerationKnowledgeIntensive_2021_LewisEtAl,
  title = {Retrieval-{{Augmented Generation}} for {{Knowledge-Intensive NLP Tasks}}},
  author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and Küttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rocktäschel, Tim and Riedel, Sebastian and Kiela, Douwe},
  date = {2021-04-12},
  eprint = {2005.11401},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2005.11401},
  url = {http://arxiv.org/abs/2005.11401},
  urldate = {2024-02-15},
  abstract = {Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,notion},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/projeto-heiwa/Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks_2021_Lewis et al.pdf;/home/baldoinov/Zotero/storage/T6AQTUG3/2005.html}
}

@online{RetrievalAugmentedGenerationLarge_2023_GaoEtAl,
  title = {Retrieval-{{Augmented Generation}} for {{Large Language Models}}: {{A Survey}}},
  shorttitle = {Retrieval-{{Augmented Generation}} for {{Large Language Models}}},
  author = {Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Haofen},
  date = {2023-12-18},
  eprint = {2312.10997},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2312.10997},
  url = {http://arxiv.org/abs/2312.10997},
  urldate = {2023-12-31},
  abstract = {Large language models (LLMs) demonstrate powerful capabilities, but they still face challenges in practical applications, such as hallucinations, slow knowledge updates, and lack of transparency in answers. Retrieval-Augmented Generation (RAG) refers to the retrieval of relevant information from external knowledge bases before answering questions with LLMs. RAG has been demonstrated to significantly enhance answer accuracy, reduce model hallucination, particularly for knowledge-intensive tasks. By citing sources, users can verify the accuracy of answers and increase trust in model outputs. It also facilitates knowledge updates and the introduction of domain-specific knowledge. RAG effectively combines the parameterized knowledge of LLMs with non-parameterized external knowledge bases, making it one of the most important methods for implementing large language models. This paper outlines the development paradigms of RAG in the era of LLMs, summarizing three paradigms: Naive RAG, Advanced RAG, and Modular RAG. It then provides a summary and organization of the three main components of RAG: retriever, generator, and augmentation methods, along with key technologies in each component. Furthermore, it discusses how to evaluate the effectiveness of RAG models, introducing two evaluation methods for RAG, emphasizing key metrics and abilities for evaluation, and presenting the latest automatic evaluation framework. Finally, potential future research directions are introduced from three aspects: vertical optimization, horizontal scalability, and the technical stack and ecosystem of RAG.},
  pubstate = {prepublished},
  version = {1},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,notion},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Retrieval-Augmented Generation for Large Language Models_2023_Gao et al.pdf;/home/baldoinov/Zotero/storage/TE5SJSS4/2312.html}
}

@article{RFernsImplementationRandom_2014_Kursa,
  title = {{{rFerns}}: {{An Implementation}} of the {{Random Ferns Method}} for {{General-Purpose Machine Learning}}},
  shorttitle = {{{rFerns}}},
  author = {Kursa, Miron B.},
  date = {2014-11-13},
  journaltitle = {Journal of Statistical Software},
  volume = {61},
  pages = {1--13},
  issn = {1548-7660},
  doi = {10.18637/jss.v061.i10},
  url = {https://doi.org/10.18637/jss.v061.i10},
  urldate = {2025-06-26},
  abstract = {Random ferns is a very simple yet powerful classification method originally introduced for specific computer vision tasks. In this paper, I show that this algorithm may be considered as a constrained decision tree ensemble and use this interpretation to introduce a series of modifications which enable the use of random ferns in general machine learning problems. Moreover, I extend the method with an internal error approximation and an attribute importance measure based on corresponding features of the random forest algorithm. I also present the R package rFerns containing an efficient implementation of this modified version of random ferns.},
  langid = {english},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/rFerns_2014_Kursa.pdf}
}

@article{richardson_2023,
  title = {Earth beyond Six of Nine Planetary Boundaries},
  author = {Richardson, K. and Steffen, W. and Lucht, W. and Bendtsen, J. and Cornell, S.E. and Donges, J.F. and Drüke, M. and Fetzer, I. and Bala, G. and family=Bloh, given=W., prefix=von, useprefix=true and Feulner, G.},
  date = {2023},
  journaltitle = {Science Advances},
  volume = {9},
  number = {37}
}

@inproceedings{RoBERTuitoPretrainedLanguage_2022_PerezEtAl,
  title = {{{RoBERTuito}}: A Pre-Trained Language Model for Social Media Text in {{Spanish}}},
  shorttitle = {{{RoBERTuito}}},
  booktitle = {Proceedings of the {{Thirteenth Language Resources}} and {{Evaluation Conference}}},
  author = {Pérez, Juan Manuel and Furman, Damián Ariel and Alonso Alemany, Laura and Luque, Franco M.},
  editor = {Calzolari, Nicoletta and Béchet, Frédéric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Odijk, Jan and Piperidis, Stelios},
  date = {2022-06},
  pages = {7235--7243},
  publisher = {European Language Resources Association},
  location = {Marseille, France},
  url = {https://aclanthology.org/2022.lrec-1.785},
  urldate = {2024-08-30},
  abstract = {Since BERT appeared, Transformer language models and transfer learning have become state-of-the-art for natural language processing tasks. Recently, some works geared towards pre-training specially-crafted models for particular domains, such as scientific papers, medical documents, user-generated texts, among others. These domain-specific models have been shown to improve performance significantly in most tasks; however, for languages other than English, such models are not widely available. In this work, we present RoBERTuito, a pre-trained language model for user-generated text in Spanish, trained on over 500 million tweets. Experiments on a benchmark of tasks involving user-generated text showed that RoBERTuito outperformed other pre-trained language models in Spanish. In addition to this, our model has some cross-lingual abilities, achieving top results for English-Spanish tasks of the Linguistic Code-Switching Evaluation benchmark (LinCE) and also competitive performance against monolingual models in English Twitter tasks. To facilitate further research, we make RoBERTuito publicly available at the HuggingFace model hub together with the dataset used to pre-train it.},
  eventtitle = {{{LREC}} 2022},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/projeto-heiwa/RoBERTuito_2022_Perez et al.pdf}
}

@inproceedings{RobustlyOptimizedBERT_2021_ZhuangEtAl,
  title = {A {{Robustly Optimized BERT Pre-training Approach}} with {{Post-training}}},
  booktitle = {Proceedings of the 20th {{Chinese National Conference}} on {{Computational Linguistics}}},
  author = {Zhuang, Liu and Wayne, Lin and Ya, Shi and Jun, Zhao},
  editor = {Li, Sheng and Sun, Maosong and Liu, Yang and Wu, Hua and Liu, Kang and Che, Wanxiang and He, Shizhu and Rao, Gaoqi},
  date = {2021-08},
  pages = {1218--1227},
  publisher = {Chinese Information Processing Society of China},
  location = {Huhhot, China},
  url = {https://aclanthology.org/2021.ccl-1.108},
  urldate = {2024-08-30},
  abstract = {In the paper we present a `pre-training'+`post-training'+`fine-tuning' three-stage paradigm which is a supplementary framework for the standard `pre-training'+`fine-tuning' languagemodel approach. Furthermore based on three-stage paradigm we present a language modelnamed PPBERT. Compared with original BERT architecture that is based on the standard two-stage paradigm we do not fine-tune pre-trained model directly but rather post-train it on the domain or task related dataset first which helps to better incorporate task-awareness knowl-edge and domain-awareness knowledge within pre-trained model also from the training datasetreduce bias. Extensive experimental results indicate that proposed model improves the perfor-mance of the baselines on 24 NLP tasks which includes eight GLUE benchmarks eight Su-perGLUE benchmarks six extractive question answering benchmarks. More remarkably our proposed model is a more flexible and pluggable model where post-training approach is able to be plugged into other PLMs that are based on BERT. Extensive ablations further validate the effectiveness and its state-of-the-art (SOTA) performance. The open source code pre-trained models and post-trained models are available publicly.},
  eventtitle = {{{CCL}} 2021},
  langid = {english},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/projeto-heiwa/A Robustly Optimized BERT Pre-training Approach with Post-training_2021_Zhuang et al.pdf}
}

@article{rockstrom_2009,
  title = {Planetary Boundaries: Exploring the Safe Operating Space for Humanity},
  author = {Rockström, J. and Steffen, W. and Noone, K. and Persson, Å. and Chapin III, F.S. and Lambin, E. and Lenton, T.M. and Scheffer, M. and Folke, C. and Schellnhuber, H.J. and Nykvist, B.},
  date = {2009},
  journaltitle = {Ecology and Society},
  volume = {14},
  number = {2}
}

@article{rockstrom_safe_2009,
  title = {A Safe Operating Space for Humanity},
  author = {Rockström, Johan and Steffen, Will and Noone, Kevin and Persson, Åsa and Chapin, F. Stuart and Lambin, Eric F. and Lenton, Timothy M. and Scheffer, Marten and Folke, Carl and Schellnhuber, Hans Joachim and Nykvist, Björn and family=Wit, given=Cynthia A., prefix=de, useprefix=true and Hughes, Terry and family=Leeuw, given=Sander, prefix=van der, useprefix=true and Rodhe, Henning and Sörlin, Sverker and Snyder, Peter K. and Costanza, Robert and Svedin, Uno and Falkenmark, Malin and Karlberg, Louise and Corell, Robert W. and Fabry, Victoria J. and Hansen, James and Walker, Brian and Liverman, Diana and Richardson, Katherine and Crutzen, Paul and Foley, Jonathan A.},
  date = {2009-09},
  journaltitle = {Nature},
  volume = {461},
  number = {7263},
  pages = {472--475},
  issn = {1476-4687},
  doi = {10.1038/461472a},
  url = {https://www.nature.com/articles/461472a},
  urldate = {2023-12-11},
  abstract = {Identifying and quantifying planetary boundaries that must not be transgressed could help prevent human activities from causing unacceptable environmental change, argue Johan Rockström and colleagues.},
  langid = {english},
  keywords = {Humanities and Social Sciences,multidisciplinary,Science}
}

@article{SantificacaoInclinarsePara_2011_Impacto,
  entrysubtype = {magazine},
  title = {Santificação é Inclinar-se Para O Espírito},
  author = {Impacto},
  date = {2011-11-23T01:16:49+00:00},
  journaltitle = {Impacto Publicações},
  url = {https://revistaimpacto.com.br/santificacao-e-inclinar-se-para-o-espirito/},
  urldate = {2025-02-02},
  abstract = {Voltando, então, para Romanos 8.1, podemos entender melhor estas palavras: “Portanto agora nenhuma condenação há para os que estão em Cristo Jesus.” É só na base de estar nele que não há condenação. A lei trouxe condenação, senti minha condição miserável, e aceitei a solução de estar em Cristo. O Espírito de Cristo entrou na […]},
  langid = {brazilian},
  file = {/home/baldoinov/baldoinov/PDFs/Apoio-Biblico/Santificacao e Inclinar-se Para O Espirito_2011_Impacto.pdf;/home/baldoinov/Zotero/storage/V4IDXBIV/santificacao-e-inclinar-se-para-o-espirito.html}
}

@article{SaudeSaneamentoNo_2005_MendoncaMotta,
  title = {Saúde e saneamento no Brasil},
  author = {family=Mendonça, given=Mario Jorge Cardoso, prefix=de, useprefix=false and family=Motta, given=Ronaldo Seroa, prefix=da, useprefix=false},
  date = {2005-04},
  journaltitle = {www.ipea.gov.br},
  shortjournal = {Texto para Discussão (TD) 1081: Saúde e saneamento no Brasil},
  publisher = {Instituto de Pesquisa Econômica Aplicada (Ipea)},
  url = {https://repositorio.ipea.gov.br/handle/11058/2079},
  urldate = {2024-07-02},
  abstract = {Foi significativa a redução da mortalidade infantil no Brasil associada às doenças de veiculação hídrica ao longo das últimas duas décadas. Usando um modelo de estrutura epidemiológica, nosso estudo demonstra que essa redução foi alcançada com a melhoria na cobertura dos serviços de saneamento e também devido ao acesso aos serviços de educação e saúde. Com base nos resultados econométricos estimamos o custo médio de salvar uma vida para cada tipo de serviço. Considerando esses custos, a contínua redução do analfabetismo garante a alternativa mais barata para baixar mais ainda a incidência desse tipo de mortalidade. Por outro lado, gastos defensivos de saúde apresentam custos quase equivalentes aos respectivos custos relacionados com a expansão dos serviços de saneamento quando se trata da mesma magnitude de redução dessa taxa de mortalidade.},
  langid = {brazilian},
  annotation = {Accepted: 2013-11-06T18:05:14Z},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/econometria-ii/Saude e saneamento no Brasil_2005_Mendonca et al.pdf}
}

@online{ScalableAgentBasedModeling_2023_WheelerVarner,
  title = {Scalable {{Agent-Based Modeling}} for {{Complex Financial Market Simulations}}},
  author = {Wheeler, Aaron and Varner, Jeffrey D.},
  date = {2023-12-22},
  eprint = {2312.14903},
  eprinttype = {arXiv},
  eprintclass = {q-fin},
  doi = {10.48550/arXiv.2312.14903},
  url = {http://arxiv.org/abs/2312.14903},
  urldate = {2024-01-26},
  abstract = {In this study, we developed a computational framework for simulating large-scale agent-based financial markets. Our platform supports trading multiple simultaneous assets and leverages distributed computing to scale the number and complexity of simulated agents. Heterogeneous agents make decisions in parallel, and their orders are processed through a realistic, continuous double auction matching engine. We present a baseline model implementation and show that it captures several known statistical properties of real financial markets (i.e., stylized facts). Further, we demonstrate these results without fitting models to historical financial data. Thus, this framework could be used for direct applications such as human-in-the-loop machine learning or to explore theoretically exciting questions about market microstructure's role in forming the statistical regularities of real markets. To the best of our knowledge, this study is the first to implement multiple assets, parallel agent decision-making, a continuous double auction mechanism, and intelligent agent types in a scalable real-time environment.},
  pubstate = {prepublished},
  keywords = {notion,Quantitative Finance - Computational Finance,Quantitative Finance - Statistical Finance,Quantitative Finance - Trading and Market Microstructure},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Scalable Agent-Based Modeling for Complex Financial Market Simulations_2023_Wheeler et al.pdf;/home/baldoinov/Zotero/storage/MRQDUV5W/2312.html}
}

@article{ScalefreeNetworks_2003_BarabasiBonabeau,
  title = {Scale-Free Networks},
  author = {Barabási, Albert-László and Bonabeau, Eric},
  date = {2003-05},
  journaltitle = {Scientific American},
  shortjournal = {Sci Am},
  volume = {288},
  number = {5},
  eprint = {12701331},
  eprinttype = {pmid},
  pages = {60--69},
  issn = {0036-8733},
  doi = {10.1038/scientificamerican0503-60},
  langid = {english},
  keywords = {Cells,Computer Security,Disease Outbreaks,Homeostasis,Humans,Internet,Mathematics,Nerve Net,Neural Networks Computer,notion,Proteins,Social Behavior,Transportation},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Scale-free networks_2003_Barabasi et al.pdf}
}

@article{ScaleFreeNetworksDecade_2009_Barabasi,
  title = {Scale-{{Free Networks}}: {{A Decade}} and {{Beyond}}},
  shorttitle = {Scale-{{Free Networks}}},
  author = {Barabási, Albert-László},
  date = {2009-07-24},
  journaltitle = {Science},
  volume = {325},
  number = {5939},
  pages = {412--413},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.1173299},
  url = {https://www.science.org/doi/10.1126/science.1173299},
  urldate = {2023-11-30},
  abstract = {For decades, we tacitly assumed that the components of such complex systems as the cell, the society, or the Internet are randomly wired together. In the past decade, an avalanche of research has shown that many real networks, independent of their age, function, and scope, converge to similar architectures, a universality that allowed researchers from different disciplines to embrace network theory as a common paradigm. The decade-old discovery of scale-free networks was one of those events that had helped catalyze the emergence of network science, a new research field with its distinct set of challenges and accomplishments.},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Scale-Free Networks_2009_Barabasi.pdf}
}

@online{SciAgentsAutomatingScientific_2024_GhafarollahiBuehler,
  title = {{{SciAgents}}: {{Automating}} Scientific Discovery through Multi-Agent Intelligent Graph Reasoning},
  shorttitle = {{{SciAgents}}},
  author = {Ghafarollahi, Alireza and Buehler, Markus J.},
  date = {2024-09-09},
  url = {https://arxiv.org/abs/2409.05556v1},
  urldate = {2024-09-10},
  abstract = {A key challenge in artificial intelligence is the creation of systems capable of autonomously advancing scientific understanding by exploring novel domains, identifying complex patterns, and uncovering previously unseen connections in vast scientific data. In this work, we present SciAgents, an approach that leverages three core concepts: (1) the use of large-scale ontological knowledge graphs to organize and interconnect diverse scientific concepts, (2) a suite of large language models (LLMs) and data retrieval tools, and (3) multi-agent systems with in-situ learning capabilities. Applied to biologically inspired materials, SciAgents reveals hidden interdisciplinary relationships that were previously considered unrelated, achieving a scale, precision, and exploratory power that surpasses traditional human-driven research methods. The framework autonomously generates and refines research hypotheses, elucidating underlying mechanisms, design principles, and unexpected material properties. By integrating these capabilities in a modular fashion, the intelligent system yields material discoveries, critique and improve existing hypotheses, retrieve up-to-date data about existing research, and highlights their strengths and limitations. Our case studies demonstrate scalable capabilities to combine generative AI, ontological representations, and multi-agent modeling, harnessing a `swarm of intelligence' similar to biological systems. This provides new avenues for materials discovery and accelerates the development of advanced materials by unlocking Nature's design principles.},
  langid = {english},
  pubstate = {prepublished},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/SciAgents_2024_Ghafarollahi et al.pdf}
}

@online{SearchingActivationFunctions_2017_RamachandranEtAl,
  title = {Searching for {{Activation Functions}}},
  author = {Ramachandran, Prajit and Zoph, Barret and Le, Quoc V.},
  date = {2017-10-27},
  eprint = {1710.05941},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1710.05941},
  url = {http://arxiv.org/abs/1710.05941},
  urldate = {2023-12-08},
  abstract = {The choice of activation functions in deep networks has a significant effect on the training dynamics and task performance. Currently, the most successful and widely-used activation function is the Rectified Linear Unit (ReLU). Although various hand-designed alternatives to ReLU have been proposed, none have managed to replace it due to inconsistent gains. In this work, we propose to leverage automatic search techniques to discover new activation functions. Using a combination of exhaustive and reinforcement learning-based search, we discover multiple novel activation functions. We verify the effectiveness of the searches by conducting an empirical evaluation with the best discovered activation function. Our experiments show that the best discovered activation function, \$f(x) = x \textbackslash cdot \textbackslash text\{sigmoid\}(\textbackslash beta x)\$, which we name Swish, tends to work better than ReLU on deeper models across a number of challenging datasets. For example, simply replacing ReLUs with Swish units improves top-1 classification accuracy on ImageNet by 0.9\textbackslash\% for Mobile NASNet-A and 0.6\textbackslash\% for Inception-ResNet-v2. The simplicity of Swish and its similarity to ReLU make it easy for practitioners to replace ReLUs with Swish units in any neural network.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,notion},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Searching for Activation Functions_2017_Ramachandran et al.pdf;/home/baldoinov/Zotero/storage/AI2X2GM5/1710.html}
}

@online{SegmentEverythingEverywhere_2023_ZouEtAl,
  title = {Segment {{Everything Everywhere All}} at {{Once}}},
  author = {Zou, Xueyan and Yang, Jianwei and Zhang, Hao and Li, Feng and Li, Linjie and Gao, Jianfeng and Lee, Yong Jae},
  date = {2023-05-01},
  eprint = {2304.06718},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2304.06718},
  url = {http://arxiv.org/abs/2304.06718},
  urldate = {2023-05-16},
  abstract = {Despite the growing demand for interactive AI systems, there have been few comprehensive studies on human-AI interaction in visual understanding e.g. segmentation. Inspired by the development of prompt-based universal interfaces for LLMs, this paper presents SEEM, a promptable, interactive model for Segmenting Everything Everywhere all at once in an image. SEEM has four desiderata: i) Versatility: by introducing a versatile prompting engine for different types of prompts, including points, boxes, scribbles, masks, texts, and referred regions of another image; ii) Compositionality: by learning a joint visual-semantic space for visual and textual prompts to compose queries on the fly for inference as shown in Fig 1; iii)Interactivity: by incorporating learnable memory prompts to retain dialog history information via mask-guided cross-attention; and iv) Semantic-awareness: by using a text encoder to encode text queries and mask labels for open-vocabulary segmentation.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,notion},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Segment Everything Everywhere All at Once_2023_Zou et al.pdf;/home/baldoinov/Zotero/storage/FX2WYSFC/2304.html}
}

@article{SegregationDynamicsReinforcement_2020_SertEtAl,
  title = {Segregation Dynamics with Reinforcement Learning and Agent Based Modeling},
  author = {Sert, Egemen and Bar-Yam, Yaneer and Morales, Alfredo J.},
  date = {2020-07-16},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {10},
  number = {1},
  pages = {11771},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-020-68447-8},
  url = {https://www.nature.com/articles/s41598-020-68447-8},
  urldate = {2023-06-12},
  abstract = {Societies are complex. Properties of social systems can be explained by the interplay and weaving of individual actions. Rewards are key to understand people’s choices and decisions. For instance, individual preferences of where to live may lead to the emergence of social segregation. In this paper, we combine Reinforcement Learning (RL) with Agent Based Modeling (ABM) in order to address the self-organizing dynamics of social segregation and explore the space of possibilities that emerge from considering different types of rewards. Our model promotes the creation of interdependencies and interactions among multiple agents of two different kinds that segregate from each other. For this purpose, agents use Deep Q-Networks to make decisions inspired on the rules of the Schelling Segregation model and rewards for interactions. Despite the segregation reward, our experiments show that spatial integration can be achieved by establishing interdependencies among agents of different kinds. They also reveal that segregated areas are more probable to host older people than diverse areas, which attract younger ones. Through this work, we show that the combination of RL and ABM can create an artificial environment for policy makers to observe potential and existing behaviors associated to rules of interactions and rewards.},
  issue = {1},
  langid = {english},
  keywords = {Computational science,Nonlinear phenomena,notion},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Segregation dynamics with reinforcement learning and agent based modeling_2020_Sert et al.pdf;/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Segregation dynamics with reinforcement learning and agent based modeling_2020_Sert et al2.pdf}
}

@book{SegurancaInformacaoPrincipios_2014_Machado,
  title = {Segurança da informação: Princípios e controle de ameaças},
  shorttitle = {Segurança da informação},
  author = {Machado, Felipe Nery Rodrigues},
  date = {2014-01-15},
  edition = {1ª edição},
  publisher = {Editora Érica - Sob Demanda},
  abstract = {Objetiva e de fácil entendimento, esta obra proporciona ao aluno conhecer os elementos básicos que compõem os instrumentos que buscam garantir os princípios básicos de integridade, confidencialidade e disponibilidade da informação. Expõe as ameaças aos sistemas informatizados e aos computadores pessoais; e fornece informações sobre os principais instrumentos de proteção, como firewalls, detecção de intrusões e programas antivírus. Aborda a conexão à internet, os endereços IP e alguns aspectos de segurança de ambientes em que se utilizam redes e computadores. Por fim, explica como informações pessoais podem ser afetadas em smartphones e tablets. O conteúdo pode ser aplicado para os cursos técnicos em Informática, Informática para Internet, Manutenção e Suporte em Informática, Redes de Computadores, entre outros.Possui material de apoio ao professor.},
  isbn = {978-85-365-0784-2},
  langid = {portuguese}
}

@online{SelfSupervisedLearningImages_2023_AssranEtAl,
  title = {Self-{{Supervised Learning}} from {{Images}} with a {{Joint-Embedding Predictive Architecture}}},
  author = {Assran, Mahmoud and Duval, Quentin and Misra, Ishan and Bojanowski, Piotr and Vincent, Pascal and Rabbat, Michael and LeCun, Yann and Ballas, Nicolas},
  date = {2023-04-13},
  eprint = {2301.08243},
  eprinttype = {arXiv},
  eprintclass = {cs, eess},
  doi = {10.48550/arXiv.2301.08243},
  url = {http://arxiv.org/abs/2301.08243},
  urldate = {2023-06-17},
  abstract = {This paper demonstrates an approach for learning highly semantic image representations without relying on hand-crafted data-augmentations. We introduce the Image-based Joint-Embedding Predictive Architecture (I-JEPA), a non-generative approach for self-supervised learning from images. The idea behind I-JEPA is simple: from a single context block, predict the representations of various target blocks in the same image. A core design choice to guide I-JEPA towards producing semantic representations is the masking strategy; specifically, it is crucial to (a) sample target blocks with sufficiently large scale (semantic), and to (b) use a sufficiently informative (spatially distributed) context block. Empirically, when combined with Vision Transformers, we find I-JEPA to be highly scalable. For instance, we train a ViT-Huge/14 on ImageNet using 16 A100 GPUs in under 72 hours to achieve strong downstream performance across a wide range of tasks, from linear classification to object counting and depth prediction.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing,notion},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Self-Supervised Learning from Images with a Joint-Embedding Predictive_2023_Assran et al.pdf;/home/baldoinov/Zotero/storage/NX979247/2301.html}
}

@article{SentimentAnalysisModel_2017_AbiramiAskarunisa,
  title = {Sentiment Analysis Model to Emphasize the Impact of Online Reviews in Healthcare Industry},
  author = {Abirami, A.M. and Askarunisa, A.},
  date = {2017-01-01},
  journaltitle = {Online Information Review},
  volume = {41},
  number = {4},
  pages = {471--486},
  publisher = {Emerald Publishing Limited},
  issn = {1468-4527},
  doi = {10.1108/OIR-08-2015-0289},
  url = {https://doi.org/10.1108/OIR-08-2015-0289},
  urldate = {2024-02-27},
  abstract = {Purpose The purpose of this paper is to develop a systematic approach to extract users’ feelings and emotions about their experiences in hospitals from online reviews and rank the places using multi-criteria decision making (MCDM) techniques based on the aggregated sentiment score. Design/methodology/approach The proposed model used a linguistic approach to extract the sentiment words from the free text. It used term frequency-inverse document frequency values to represent features of various places in bag-of-words format. Sentiment dictionary is used to calculate senti-scores. It used different MCDM techniques like simple additive weight and Technique for Order Preference by Similarity to Ideal Solution (TOPSIS) methods for ranking hospitals based on their aggregated senti-score. Findings Statistical correlation analysis between the rankings of places reveals that the TOPSIS method is the most suitable ranking technique among other MCDM techniques. By improving the senti-score, one can bring their enterprise to the top position. Research limitations/implications Data set is collected from different websites like Twitter, Facebook, etc., for various services/features. Moderate amount of reviews are collected for each place. But not all users give their views on the social media websites. It would be essential to collect responses from all the customers who avail different services at different places. Practical implications The sentiment analysis model proposed in this paper enables B2C and C2C commerce. Business may take suitable measures to overcome their issues/problems raised by the consumer. Consumers can share and educate other consumers about their experiences. Social implications The development of internet has strong influence in all types of industries like healthcare. The availability of internet has changed the way of accessing the information and sharing their experience with others. This paper recognizes the use and impact of social media on the healthcare industry by analyzing the users’ feelings expressed in the form of free text. A suitable decision-making technique is applied to rank the places, which enables the users to plan their treatment place in a better way. Originality/value The paper develops a novel approach by applying the TOPSIS method to rank the different alternative places of the healthcare industry by using the senti-score derived from the users’ feelings, emotions and experiences expressed in the form of free text.},
  keywords = {MCDM technique,Sentiment analysis,Social media analysis,TOPSIS method},
  file = {/home/baldoinov/Zotero/storage/JM68K443/html.html}
}

@online{SentimentAnalysisUsing_2023_Atta,
  title = {Sentiment {{Analysis}} Using {{NLP}} in {{Python}}: {{A Practical Guide}} with {{NLTK}}, {{TextBlob}} and {{VADER}}},
  shorttitle = {Sentiment {{Analysis}} Using {{NLP}} in {{Python}}},
  author = {Atta, Soumen},
  date = {2023-04-21T10:56:27},
  url = {https://blog.devgenius.io/sentiment-analysis-using-nlp-in-python-a-practical-guide-with-nltk-textblob-and-vader-2fb3b4555d23},
  urldate = {2023-09-21},
  abstract = {Sentiment analysis is a powerful tool in Natural Language Processing (NLP) that helps us to understand and quantify the emotional tone and…},
  langid = {english},
  organization = {Medium},
  keywords = {notion},
  file = {/home/baldoinov/Zotero/storage/Y6EADJU4/sentiment-analysis-using-nlp-in-python-a-practical-guide-with-nltk-textblob-and-vader-2fb3b4555.html}
}

@online{SettingUbersTransactional_2023_Govindarajan,
  title = {Setting {{Uber}}’s {{Transactional Data Lake}} in {{Motion}} with {{Incremental ETL Using Apache Hudi}}},
  author = {Govindarajan, Vinoth},
  date = {2023-03-16T16:30:00+00:00},
  url = {https://www.uber.com/en-IN/blog/ubers-lakehouse-architecture/},
  urldate = {2025-03-09},
  abstract = {The Global Data Warehouse team at Uber democratizes data for all of Uber with a unified, petabyte-scale, centrally modeled data lake. The data lake consists of foundational fact, dimension, and aggregate tables developed using dimensional data modeling techniques that can be accessed by engineers and data scientists in a self-serve manner to power data engineering, data science, machine learning, and reporting across Uber. The ETL (extract, transform, load) pipelines that compute these tables are thus mission-critical to Uber’s apps and services, powering core platform features like rider safety, ETA predictions, fraud detection, and more. At Uber, data freshness is a key business requirement. Uber invests heavily in engineering efforts that process data as quickly as possible to keep it up to date with the happenings in the physical world.},
  langid = {english},
  organization = {Uber Blog},
  file = {/home/baldoinov/baldoinov/PDFs/Mackenzie/ecossistemas-de-big-data-ii/Setting Uber’s Transactional Data Lake in Motion with Incremental ETL Using_2023_Govindarajan.pdf;/home/baldoinov/Zotero/storage/X6QHGVFF/ubers-lakehouse-architecture.html}
}

@book{SexoNamoroRelacionamentos_2017_HiestandThomas,
  title = {Sexo, Namoro E Relacionamentos},
  author = {Hiestand, Gerald and Thomas, Jay},
  editor = {Sabino, Felipe},
  date = {2017},
  edition = {1ª edição},
  publisher = {Monergismo},
  abstract = {Em um período de tanta confusão sobre sexo, namoro e relacionamentos, este livro fornece conselhos úteis e oportunos. Ele esclarece a natureza dos relacionamentos e serve como fonte de encorajamento, mostrando que a pureza não se encontra fora do nosso alcance. Trata-se de um excelente livro que propõe o que os autores chamam de “amizades especiais”.— Tim Challies Autor, Desintoxicação sexualQuando combatemos erros e desvios teológicos, argumentamos que a Bíblia é nossa única regra de fé e prática, como de fato ela é. Ao nos depararmos com heresias como o teísmo aberto, apresentamos sua refutação a partir da Escritura. Ao aconselharmos casais com problemas, abrimos o santo Livro para lembrá-los de que o marido deve amar a esposa como Cristo amou a igreja, e a mulher deve se submeter ao marido, como a igreja é submissa a Cristo. Contudo, quando o tema é o relacionamento entre um homem e uma mulher não casados, em nossos dias, a Bíblia estranhamente é deixada de lado. Neste livro, Gerald e Jay nos lembram de algo que já deveríamos saber: a Bíblia é o padrão para tudo. Os autores apresentam uma perspectiva bíblica sobre o assunto, que se recusa a ceder à cultura. É claramente uma mensagem contracultural, como o Sermão do Monte de nosso Senhor. Trata-se de uma leitura obrigatória para todos os solteiros, de ambos os sexos, que desejam honrar a Deus. — Felipe Sabino Editor, Editora Monergismo},
  isbn = {978-85-69980-05-6},
  langid = {portuguese}
}

@book{ShadowViolencePolitics_2012_NorthEtAl,
  title = {In the Shadow of Violence: Politics, Economics, and the Problems of Development},
  shorttitle = {In the Shadow of Violence},
  editor = {North, Douglass C. and Wallis, John Joseph and Webb, Steven B. and Weingast, Barry R.},
  date = {2012-11-12},
  edition = {Illustrated edição},
  publisher = {Cambridge University Press},
  location = {Cambridge},
  abstract = {This book applies the conceptual framework of Douglass C. North, John Joseph Wallis, and Barry R. Weingast's Violence and Social Orders (Cambridge University Press, 2009) to nine developing countries. The cases show how political control of economic privileges is used to limit violence and coordinate coalitions of powerful organizations. Rather than castigating politicians and elites as simply corrupt, the case studies illustrate why development is so difficult to achieve in societies where the role of economic organizations is manipulated to provide political balance and stability. The volume develops the idea of limited-access social order as a dynamic social system in which violence is constantly a threat, and political and economic outcomes result from the need to control violence rather than promoting economic growth or political rights.},
  isbn = {978-1-107-01421-3},
  langid = {Inglês}
}

@article{SharedPictureBook_2020_DowdallEtAl,
  title = {Shared {{Picture Book Reading Interventions}} for {{Child Language Development}}: {{A Systematic Review}} and {{Meta-Analysis}}},
  shorttitle = {Shared {{Picture Book Reading Interventions}} for {{Child Language Development}}},
  author = {Dowdall, Nicholas and Melendez-Torres, G. J. and Murray, Lynne and Gardner, Frances and Hartford, Leila and Cooper, Peter J.},
  date = {2020},
  journaltitle = {Child Development},
  volume = {91},
  number = {2},
  pages = {e383-e399},
  issn = {1467-8624},
  doi = {10.1111/cdev.13225},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cdev.13225},
  urldate = {2024-09-11},
  abstract = {Interventions that train parents to share picture books with children are seen as a strategy for supporting child language development. We conducted meta-analyses using robust variance estimation modeling on results from 19 RCTs (Ntotal = 2,594; Mchildage = 1–6 years). Overall, book-sharing interventions had a small sized effect on both expressive language (d = 0.41) and receptive language (d = 0.26). They had a large effect on caregiver book-sharing competence (d = 1.01). The impact of the intervention on child language was moderated by intervention dosage, with lower dosage associated with a minimal impact. Child age and caregiver education level were unrelated to child outcome. This review and meta-analysis confirms the promise of book-sharing interventions for enhancing and accelerating child language development.},
  langid = {english},
  file = {/home/baldoinov/baldoinov/PDFs/Mackenzie/projeto-aplicado-iii/Shared Picture Book Reading Interventions for Child Language Development_2020_Dowdall et al.pdf;/home/baldoinov/Zotero/storage/X6Z7JPXQ/cdev.html}
}

@online{SimilarityNotAll_2024_GanEtAl,
  title = {Similarity Is {{Not All You Need}}: {{Endowing Retrieval Augmented Generation}} with {{Multi Layered Thoughts}}},
  shorttitle = {Similarity Is {{Not All You Need}}},
  author = {Gan, Chunjing and Yang, Dan and Hu, Binbin and Zhang, Hanxiao and Li, Siyuan and Liu, Ziqi and Shen, Yue and Ju, Lin and Zhang, Zhiqiang and Gu, Jinjie and Liang, Lei and Zhou, Jun},
  date = {2024-05-30},
  eprint = {2405.19893},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2405.19893},
  url = {http://arxiv.org/abs/2405.19893},
  urldate = {2024-06-10},
  abstract = {In recent years, large language models (LLMs) have made remarkable achievements in various domains. However, the untimeliness and cost of knowledge updates coupled with hallucination issues of LLMs have curtailed their applications in knowledge intensive tasks, where retrieval augmented generation (RAG) can be of help. Nevertheless, existing retrieval augmented models typically use similarity as a bridge between queries and documents and follow a retrieve then read procedure. In this work, we argue that similarity is not always the panacea and totally relying on similarity would sometimes degrade the performance of retrieval augmented generation. To this end, we propose MetRag, a Multi layEred Thoughts enhanced Retrieval Augmented Generation framework. To begin with, beyond existing similarity oriented thought, we embrace a small scale utility model that draws supervision from an LLM for utility oriented thought and further come up with a smarter model by comprehensively combining the similarity and utility oriented thoughts. Furthermore, given the fact that the retrieved document set tends to be huge and using them in isolation makes it difficult to capture the commonalities and characteristics among them, we propose to make an LLM as a task adaptive summarizer to endow retrieval augmented generation with compactness-oriented thought. Finally, with multi layered thoughts from the precedent stages, an LLM is called for knowledge augmented generation. Extensive experiments on knowledge-intensive tasks have demonstrated the superiority of MetRag.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Similarity is Not All You Need_2024_Gan et al.pdf;/home/baldoinov/Zotero/storage/IE43CJF8/2405.html}
}

@online{SimpleEconomicsOpen_2000_LernerTriole,
  type = {Working Paper},
  title = {The {{Simple Economics}} of {{Open Source}}},
  author = {Lerner, Josh and Triole, Jean},
  date = {2000-03},
  series = {Working {{Paper Series}}},
  number = {7600},
  eprint = {7600},
  eprinttype = {National Bureau of Economic Research},
  doi = {10.3386/w7600},
  url = {https://www.nber.org/papers/w7600},
  urldate = {2023-12-03},
  abstract = {There has been a recent surge of interest in open source software development, which involves developers at many different locations and organizations sharing code to develop and refine programs. To an economist, the behavior of individual programmers and commercial companies engaged in open source projects is initially startling. This paper makes a preliminary exploration of the economics of open source software. We highlight the extent to which labor economics, especially the literature on career concerns,' can explain many of these projects' features. Aspects of the future of open source development process, however, remain somewhat difficult to predict with off-the-shelf' economic models.},
  pubstate = {prepublished},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/The Simple Economics of Open Source_2000_Lerner et al.pdf}
}

@online{SimplifyingTransformerBlocks_2023_HeHofmann,
  title = {Simplifying {{Transformer Blocks}}},
  author = {He, Bobby and Hofmann, Thomas},
  date = {2023-11-03},
  eprint = {2311.01906},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2311.01906},
  url = {http://arxiv.org/abs/2311.01906},
  urldate = {2023-11-28},
  abstract = {A simple design recipe for deep Transformers is to compose identical building blocks. But standard transformer blocks are far from simple, interweaving attention and MLP sub-blocks with skip connections \& normalisation layers in precise arrangements. This complexity leads to brittle architectures, where seemingly minor changes can significantly reduce training speed, or render models untrainable. In this work, we ask to what extent the standard transformer block can be simplified? Combining signal propagation theory and empirical observations, we motivate modifications that allow many block components to be removed with no loss of training speed, including skip connections, projection or value parameters, sequential sub-blocks and normalisation layers. In experiments on both autoregressive decoder-only and BERT encoder-only models, our simplified transformers emulate the per-update training speed and performance of standard transformers, while enjoying 15\% faster training throughput, and using 15\% fewer parameters.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,notion},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Simplifying Transformer Blocks_2023_He et al.pdf;/home/baldoinov/Zotero/storage/32RGK76B/2311.html}
}

@inproceedings{SimulatingEconomicImpact_2024_BrusatinEtAl,
  title = {Simulating the Economic Impact of Rationality through Reinforcement Learning and Agent-Based Modelling},
  author = {Brusatin, Simone and Padoan, Tommaso and Coletta, Andrea and Gatti, Domenico Delli and Glielmo, Aldo},
  date = {2024-05-03},
  url = {https://www.semanticscholar.org/paper/Simulating-the-economic-impact-of-rationality-and-Brusatin-Padoan/9d59fe87afed5f7f62c2dd1ac4a5dade318c7898},
  urldate = {2024-05-14},
  abstract = {Agent-based models (ABMs) are simulation models used in economics to overcome some of the limitations of traditional frameworks based on general equilibrium assumptions. However, agents within an ABM follow predetermined, not fully rational, behavioural rules which can be cumbersome to design and difficult to justify. Here we leverage multi-agent reinforcement learning (RL) to expand the capabilities of ABMs with the introduction of fully rational agents that learn their policy by interacting with the environment and maximising a reward function. Specifically, we propose a 'Rational macro ABM' (R-MABM) framework by extending a paradigmatic macro ABM from the economic literature. We show that gradually substituting ABM firms in the model with RL agents, trained to maximise profits, allows for a thorough study of the impact of rationality on the economy. We find that RL agents spontaneously learn three distinct strategies for maximising profits, with the optimal strategy depending on the level of market competition and rationality. We also find that RL agents with independent policies, and without the ability to communicate with each other, spontaneously learn to segregate into different strategic groups, thus increasing market power and overall profits. Finally, we find that a higher degree of rationality in the economy always improves the macroeconomic environment as measured by total output, depending on the specific rational policy, this can come at the cost of higher instability. Our R-MABM framework is general, it allows for stable multi-agent learning, and represents a principled and robust direction to extend existing economic simulators.},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Simulating the economic impact of rationality through reinforcement learning_2024_Brusatin et al.pdf}
}

@book{Simulation_2022_Ross,
  title = {Simulation},
  author = {Ross, Sheldon M.},
  date = {2022-11-06},
  edition = {6th ed. edição},
  publisher = {Academic Press},
  location = {London, United Kingdom},
  abstract = {Simulation, Sixth Edition continues to introduce aspiring and practicing actuaries, engineers, computer scientists and others to the practical aspects of constructing computerized simulation studies to analyze and interpret real phenomena. Readers will learn to apply the results of these analyses to problems in a wide variety of fields to obtain effective, accurate solutions and make predictions. By explaining how a computer can be used to generate random numbers and how to use these random numbers to generate the behavior of a stochastic model over time, this book presents the statistics needed to analyze simulated data and validate simulation models.},
  isbn = {978-0-323-85739-0},
  langid = {Inglês},
  keywords = {notion}
}

@book{SimulationSocialScientist_2005_GilbertTroitzsch,
  title = {Simulation for the Social Scientist},
  author = {Gilbert, Professor Nigel and Troitzsch, Klaus G.},
  date = {2005-02-01},
  edition = {2nd ed. edição},
  publisher = {Open University Press},
  location = {Maidenhead, England ; New York, NY},
  abstract = {Simulation for the Social Scientist is a practical textbook on the techniques of building computer simulations to assist understanding of social and economic issues and problems. This authoritative book details all the common approaches to social simulation, to provide social scientists with an appreciation of the literature and allow those with some programming skills to create their own simulations.},
  isbn = {978-0-335-21600-0},
  langid = {Inglês},
  keywords = {notion}
}

@article{sinclair_1997,
  title = {Self‐regulation versus Command and Control? {{Beyond}} False Dichotomies},
  author = {Sinclair, D.},
  date = {1997},
  journaltitle = {Law \& Policy},
  volume = {19},
  number = {4},
  pages = {529--559}
}

@book{SingularUniverseReality_2014_UngerSmolin,
  title = {The {{Singular Universe}} and the {{Reality}} of {{Time}}: {{A Proposal}} in {{Natural Philosophy}}},
  shorttitle = {The {{Singular Universe}} and the {{Reality}} of {{Time}}},
  author = {Unger, Roberto Mangabeira and Smolin, Lee},
  date = {2014},
  publisher = {Cambridge University Press},
  location = {Cambridge},
  doi = {10.1017/CBO9781139696487},
  url = {https://www.cambridge.org/core/books/singular-universe-and-the-reality-of-time/65D8DD2BFA21BA6FC2210B1DADE286C6},
  urldate = {2024-06-17},
  abstract = {Cosmology is in crisis. The more we discover, the more puzzling the universe appears to be. How and why are the laws of nature what they are? A philosopher and a physicist, world-renowned for their radical ideas in their fields, argue for a revolution. To keep cosmology scientific, we must replace the old view in which the universe is governed by immutable laws by a new one in which laws evolve. Then we can hope to explain them. The revolution that Roberto Mangabeira Unger and Lee Smolin propose relies on three central ideas. There is only one universe at a time. Time is real: everything in the structure and regularities of nature changes sooner or later. Mathematics, which has trouble with time, is not the oracle of nature and the prophet of science; it is simply a tool with great power and immense limitations. The argument is readily accessible to non-scientists as well as to the physicists and cosmologists whom it challenges.},
  isbn = {978-1-107-07406-4},
  file = {/home/baldoinov/Zotero/storage/K2KRLFQC/65D8DD2BFA21BA6FC2210B1DADE286C6.html}
}

@book{SistemasBancoDados_2019_ElmasriNavathe,
  title = {Sistemas de Banco de Dados},
  author = {Elmasri, Ramez and Navathe, Shamkant B.},
  date = {2019},
  edition = {7ª edição},
  publisher = {Pearson Universidades},
  abstract = {Sistemas de banco de dados apresenta de forma profunda e atualizada os aspectos mais importantes dos sistemas e aplicações de banco de dados, bem como das tecnologias relacionadas. Esta obra de referência introduz os conceitos fundamentais necessários para projetar, usar e implementar sistemas de banco de dados, enfatizando fundamentos de modelagem, projeto, linguagens e modelos fornecidos pelos sistemas de gerenciamento e técnicas de implementação. Nesta edição, foram introduzidos capítulos novos sobre os avanços recentes em sistemas de banco de dados e processamento Big Data, introdução a bancos de dados NOSQL e tecnologias para processamento de big data. Além disso, foram incluídos novos exemplos e o conteúdo sobre processamento e otimização de consulta e suas técnicas, estratégias e algoritmos para processamento foi expandido, e foram acrescentadas novas construções SQL. Com conteúdo atualizado, esta obra é ideal para estudantes de análise de redes, análise de sistemas processamento de dados e engenharia da computação que queiram dominar o assunto e destacar-se na área.},
  isbn = {978-85-430-2500-1},
  langid = {portuguese}
}

@book{SistemasOperacionaisModernos_2023_TanenbaumBos,
  title = {Sistemas Operacionais Modernos},
  author = {Tanenbaum, Andrw S. and Bos, Herbert},
  date = {2023-01-12},
  publisher = {Pearson Education},
  isbn = {978-85-430-0567-6},
  langid = {portuguese}
}

@online{SituationalAwareness_2024_Aschenbrenner,
  title = {Situational {{Awareness}}},
  author = {Aschenbrenner, Leopold},
  date = {2024},
  langid = {english},
  pubstate = {prepublished},
  file = {/home/baldoinov/Zotero/storage/GTASHJ8A/Aschenbrenner - Situational Awareness.pdf}
}

@online{SixThingsConsider_2023_AWS,
  title = {Six {{Things}} to {{Consider When Moving From Batch}} to {{Real-time Data Stream Processing}}},
  author = {{AWS}},
  date = {2023-07-11},
  url = {https://community.aws/content/2iCl0on6KVn5djWtujytfHAUhQ8/six-things-to-consider-when-moving-from-batch-to-real-time-data-stream-processing},
  urldate = {2025-03-15},
  abstract = {Learn how we evolved from batch into using real-time streaming analytics and understand what to consider when moving your organization to real-time streaming.},
  langid = {english},
  organization = {Community.aws},
  file = {/home/baldoinov/Zotero/storage/QGDXESWV/six-things-to-consider-when-moving-from-batch-to-real-time-data-stream-processing.html}
}

@online{SmartAgentBasedModeling_2023_WuEtAl,
  title = {Smart {{Agent-Based Modeling}}: {{On}} the {{Use}} of {{Large Language Models}} in {{Computer Simulations}}},
  shorttitle = {Smart {{Agent-Based Modeling}}},
  author = {Wu, Zengqing and Peng, Run and Han, Xu and Zheng, Shuyuan and Zhang, Yixin and Xiao, Chuan},
  date = {2023-12-14},
  eprint = {2311.06330},
  eprinttype = {arXiv},
  eprintclass = {cs, econ, q-fin},
  doi = {10.48550/arXiv.2311.06330},
  url = {http://arxiv.org/abs/2311.06330},
  urldate = {2024-07-03},
  abstract = {Computer simulations offer a robust toolset for exploring complex systems across various disciplines. A particularly impactful approach within this realm is Agent-Based Modeling (ABM), which harnesses the interactions of individual agents to emulate intricate system dynamics. ABM's strength lies in its bottom-up methodology, illuminating emergent phenomena by modeling the behaviors of individual components of a system. Yet, ABM has its own set of challenges, notably its struggle with modeling natural language instructions and common sense in mathematical equations or rules. This paper seeks to transcend these boundaries by integrating Large Language Models (LLMs) like GPT into ABM. This amalgamation gives birth to a novel framework, Smart Agent-Based Modeling (SABM). Building upon the concept of smart agents -- entities characterized by their intelligence, adaptability, and computation ability -- we explore in the direction of utilizing LLM-powered agents to simulate real-world scenarios with increased nuance and realism. In this comprehensive exploration, we elucidate the state of the art of ABM, introduce SABM's potential and methodology, and present three case studies (source codes available at https://github.com/Roihn/SABM), demonstrating the SABM methodology and validating its effectiveness in modeling real-world systems. Furthermore, we cast a vision towards several aspects of the future of SABM, anticipating a broader horizon for its applications. Through this endeavor, we aspire to redefine the boundaries of computer simulations, enabling a more profound understanding of complex systems.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computational Engineering Finance and Science,Computer Science - Multiagent Systems,Economics - General Economics},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Smart Agent-Based Modeling_2023_Wu et al.pdf;/home/baldoinov/Zotero/storage/5EVQ969U/2311.html}
}

@article{SmartCacheContent_2020_LiEtAl,
  title = {A {{Smart Cache Content Update Policy Based}} on {{Deep Reinforcement Learning}}},
  author = {Li, Lincan and Kwong, Chiew Foong and Liu, Qianyu and Wang, Jing},
  date = {2020},
  journaltitle = {Wireless Communications and Mobile Computing},
  volume = {2020},
  number = {1},
  pages = {8836592},
  issn = {1530-8677},
  doi = {10.1155/2020/8836592},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1155/2020/8836592},
  urldate = {2024-06-26},
  abstract = {This paper proposes a DRL-based cache content update policy in the cache-enabled network to improve the cache hit ratio and reduce the average latency. In contrast to the existing policies, a more practical cache scenario is considered in this work, in which the content requests vary by both time and location. Considering the constraint of the limited cache capacity, the dynamic content update problem is modeled as a Markov decision process (MDP). Besides that, the deep Q-learning network (DQN) algorithm is utilised to solve the MDP problem. Specifically, the neural network is optimised to approximate the Q value where the training data are chosen from the experience replay memory. The DQN agent derives the optimal policy for the cache decision. Compared with the existing policies, the simulation results show that our proposed policy is 56\%–64\% improved in terms of the cache hit ratio and 56\%–59\% decreased in terms of the average latency.},
  langid = {english},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/A Smart Cache Content Update Policy Based on Deep Reinforcement Learning_2020_Li et al.pdf;/home/baldoinov/Zotero/storage/Y8DGVF4W/8836592.html}
}

@inproceedings{SocioDojoBuildingLifelong_2023_ChengChin,
  title = {{{SocioDojo}}: {{Building Lifelong Analytical Agents}} with {{Real-world Text}} and {{Time Series}}},
  shorttitle = {{{SocioDojo}}},
  author = {Cheng, Junyan and Chin, Peter},
  date = {2023-10-13},
  url = {https://openreview.net/forum?id=s9z0HzWJJp},
  urldate = {2024-07-03},
  abstract = {We introduce SocioDojo, an open-ended lifelong learning environment for developing ready-to-deploy autonomous agents capable of performing human-like analysis and decision-making on societal topics such as economics, finance, politics, and culture. It consists of (1) information sources from news, social media, reports, etc., (2) a knowledge base built from books, journals, and encyclopedias, plus a toolbox of Internet and knowledge graph search interfaces, (3) 30K high-quality time series in finance, economy, society, and polls, which support a novel task called "hyperportfolio", that can reliably and scalably evaluate societal analysis and decision-making power of agents, inspired by portfolio optimization with time series as assets to "invest". We also propose a novel Analyst-Assistant-Actuator architecture for the hyperportfolio task, and a Hypothesis \& Proof prompting for producing in-depth analyses on input news, articles, etc. to assist decision-making. We perform experiments and ablation studies to explore the factors that impact performance. The results show that our proposed method achieves improvements of 32.4\% and 30.4\% compared to the state-of-the-art method in the two experimental settings.},
  eventtitle = {The {{Twelfth International Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/SocioDojo_2023_Cheng et al.pdf}
}

@article{SoftwareEngineeringPerspective_2021_Giray,
  title = {A Software Engineering Perspective on Engineering Machine Learning Systems: {{State}} of the Art and Challenges},
  shorttitle = {A Software Engineering Perspective on Engineering Machine Learning Systems},
  author = {Giray, Görkem},
  date = {2021-10-01},
  journaltitle = {Journal of Systems and Software},
  shortjournal = {Journal of Systems and Software},
  volume = {180},
  pages = {111031},
  issn = {0164-1212},
  doi = {10.1016/j.jss.2021.111031},
  url = {https://www.sciencedirect.com/science/article/pii/S016412122100128X},
  urldate = {2024-03-01},
  abstract = {Context: Advancements in machine learning (ML) lead to a shift from the traditional view of software development, where algorithms are hard-coded by humans, to ML systems materialized through learning from data. Therefore, we need to revisit our ways of developing software systems and consider the particularities required by these new types of systems. Objective: The purpose of this study is to systematically identify, analyze, summarize, and synthesize the current state of software engineering (SE) research for engineering ML systems. Method: I performed a systematic literature review (SLR). I systematically selected a pool of 141 studies from SE venues and then conducted a quantitative and qualitative analysis using the data extracted from these studies. Results: The non-deterministic nature of ML systems complicates all SE aspects of engineering ML systems. Despite increasing interest from 2018 onwards, the results reveal that none of the SE aspects have a mature set of tools and techniques. Testing is by far the most popular area among researchers. Even for testing ML systems, engineers have only some tool prototypes and solution proposals with weak experimental proof. Many of the challenges of ML systems engineering were identified through surveys and interviews. Researchers should conduct experiments and case studies, ideally in industrial environments, to further understand these challenges and propose solutions. Conclusion: The results may benefit (1) practitioners in foreseeing the challenges of ML systems engineering; (2) researchers and academicians in identifying potential research questions; and (3) educators in designing or updating SE courses to cover ML systems engineering.},
  keywords = {Deep learning,Machine learning,notion,Software development,Software engineering,Software process,Systematic literature review},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/A software engineering perspective on engineering machine learning systems_2021_Giray.pdf;/home/baldoinov/Zotero/storage/82RGHYYQ/S016412122100128X.html}
}

@inproceedings{SOLERSemanticLinguistic_2014_GarridoEtAl,
  title = {{{SOLE-R}}: {{A Semantic}} and {{Linguistic Approach}} for {{Book Recommendations}}},
  shorttitle = {{{SOLE-R}}},
  booktitle = {2014 {{IEEE}} 14th {{International Conference}} on {{Advanced Learning Technologies}}},
  author = {Garrido, Angel L. and Pera, Maria Soledad and Ilarri, Sergio},
  date = {2014-07},
  pages = {524--528},
  issn = {2161-377X},
  doi = {10.1109/ICALT.2014.155},
  url = {https://ieeexplore.ieee.org/document/6901530/?arnumber=6901530},
  urldate = {2024-08-16},
  abstract = {Reading is a fundamental skill that each person needs to develop during early childhood and continue to enhance into adulthood. While children/teenagers depend on this skill to advance academically and become educated individuals, adults are expected to acquire a certain level of proficiency in reading so that they can engage in social/civic activities and successfully participate in the workforce. A step towards assisting individuals to become lifelong readers is to provide them adequate reading selections which can cultivate their intellectual and emotional growth. With that in mind, we have developed SOLE-R, a topic map-based tool that yields book recommendations. SOLE-R takes advantage of lexical and semantic resources to infer the likes/dislikes of a reader and thus is not restricted by the syntactic constraints imposed on existing recommenders. Furthermore, SOLE-R relies on publicly-accessible data on books to perform an in-depth analysis of the preferences of a reader that goes beyond book content or reading patterns explored by existing recommenders. We have verified the correctness of SOLE-R using a popular benchmark dataset. In addition, we have compared its performance with (state-of-the-art) recommendation strategies to further demonstrate the effectiveness of SOLE-R.},
  eventtitle = {2014 {{IEEE}} 14th {{International Conference}} on {{Advanced Learning Technologies}}},
  keywords = {books,Books,Collaboration,Libraries,Materials,Niobium,Ontologies,recommendation systems,Semantics,topic maps},
  file = {/home/baldoinov/baldoinov/PDFs/Mackenzie/projeto-aplicado-iii/SOLE-R_2014_Garrido et al.pdf;/home/baldoinov/Zotero/storage/BFFTATEC/6901530.html}
}

@article{SpatialMachineLearning_2022_Kopczewska,
  title = {Spatial Machine Learning: New Opportunities for Regional Science},
  shorttitle = {Spatial Machine Learning},
  author = {Kopczewska, Katarzyna},
  date = {2022-06-01},
  journaltitle = {The Annals of Regional Science},
  shortjournal = {Ann Reg Sci},
  volume = {68},
  number = {3},
  pages = {713--755},
  issn = {1432-0592},
  doi = {10.1007/s00168-021-01101-x},
  url = {https://doi.org/10.1007/s00168-021-01101-x},
  urldate = {2025-05-14},
  abstract = {This paper is a methodological guide to using machine learning in the spatial context. It provides an overview of the existing spatial toolbox proposed in the literature: unsupervised learning, which deals with clustering of spatial data, and supervised learning, which displaces classical spatial econometrics. It shows the potential of using this developing methodology, as well as its pitfalls. It catalogues and comments on the usage of spatial clustering methods (for locations and values, both separately and jointly) for mapping, bootstrapping, cross-validation, GWR modelling and density indicators. It provides details of spatial machine learning models, which are combined with spatial data integration, modelling, model fine-tuning and predictions to deal with spatial autocorrelation and big data. The paper delineates “already available” and “forthcoming” methods and gives inspiration for transplanting modern quantitative methods from other thematic areas to research in regional science.},
  langid = {english},
  keywords = {C31,C49,Geoinformatics,R10,Regional and Spatial Economics,Regional Geography,Spatial Demography,Spatial Economics,World Regional Geography},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Spatial machine learning_2022_Kopczewska.pdf}
}

@unpublished{SpeechLanguageProcessing_2024_JurafskyMartin,
  title = {Speech and {{Language Processing}}: {{An Introduction}} to {{Natural Language Processing}}, {{Computational Linguistics}}, and {{Speech Recognition}}},
  author = {Jurafsky, Daniel and Martin, James},
  date = {2024-08},
  file = {/home/baldoinov/baldoinov/PDFs/Livros/Speech and Language Processing_2024_Jurafsky et al.pdf}
}

@article{SQLDatabasesNoSQL_2010_Stonebraker,
  title = {{{SQL}} Databases v. {{NoSQL}} Databases},
  author = {Stonebraker, Michael},
  date = {2010-04},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {53},
  number = {4},
  pages = {10--11},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/1721654.1721659},
  url = {https://dl.acm.org/doi/10.1145/1721654.1721659},
  urldate = {2024-05-04},
  abstract = {The               Communications               Web site, http://cacm.acm.org, features more than a dozen bloggers in the BLOG@CACM community. In each issue of               Communications               , we'll publish excerpts from selected posts.                                         twitter                          Follow us on Twitter at http://twitter.com/blogCACM             Michael Stonebraker considers several performance arguments in favor of NoSQL databases---and finds them insufficient.},
  langid = {english},
  file = {/home/baldoinov/baldoinov/PDFs/Mackenzie/topicos-de-banco-de-dados/SQL databases v_2010_Stonebraker.pdf}
}

@article{StanceDetectionSurvey_2020_KucukCan,
  title = {Stance {{Detection}}: {{A Survey}}},
  shorttitle = {Stance {{Detection}}},
  author = {Küçük, Dilek and Can, Fazli},
  date = {2020-02-06},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {53},
  number = {1},
  pages = {12:1--12:37},
  issn = {0360-0300},
  doi = {10.1145/3369026},
  url = {https://doi.org/10.1145/3369026},
  urldate = {2023-12-14},
  abstract = {Automatic elicitation of semantic information from natural language texts is an important research problem with many practical application areas. Especially after the recent proliferation of online content through channels such as social media sites, news portals, and forums; solutions to problems such as sentiment analysis, sarcasm/controversy/veracity/rumour/fake news detection, and argument mining gained increasing impact and significance, revealed with large volumes of related scientific publications. In this article, we tackle an important problem from the same family and present a survey of stance detection in social media posts and (online) regular texts. Although stance detection is defined in different ways in different application settings, the most common definition is “automatic classification of the stance of the producer of a piece of text, towards a target, into one of these three classes: \{Favor, Against, Neither\}.” Our survey includes definitions of related problems and concepts, classifications of the proposed approaches so far, descriptions of the relevant datasets and tools, and related outstanding issues. Stance detection is a recent natural language processing topic with diverse application areas, and our survey article on this newly emerging topic will act as a significant resource for interested researchers and practitioners.},
  keywords = {deep learning,notion,social media analysis,Stance detection,Twitter},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/projeto-heiwa-revisao-de-literatura/Stance Detection_2020_Kucuk et al.pdf}
}

@article{StandardProtocolDescribing_2006_GrimmEtAl,
  title = {A Standard Protocol for Describing Individual-Based and Agent-Based Models},
  author = {Grimm, Volker and Berger, Uta and Bastiansen, Finn and Eliassen, Sigrunn and Ginot, Vincent and Giske, Jarl and Goss-Custard, John and Grand, Tamara and Heinz, Simone K. and Huse, Geir and Huth, Andreas and Jepsen, Jane U. and Jørgensen, Christian and Mooij, Wolf M. and Müller, Birgit and Pe’er, Guy and Piou, Cyril and Railsback, Steven F. and Robbins, Andrew M. and Robbins, Martha M. and Rossmanith, Eva and Rüger, Nadja and Strand, Espen and Souissi, Sami and Stillman, Richard A. and Vabø, Rune and Visser, Ute and DeAngelis, Donald L.},
  date = {2006-09-15},
  journaltitle = {Ecological Modelling},
  shortjournal = {Ecological Modelling},
  volume = {198},
  number = {1},
  pages = {115--126},
  issn = {0304-3800},
  doi = {10.1016/j.ecolmodel.2006.04.023},
  url = {https://www.sciencedirect.com/science/article/pii/S0304380006002043},
  urldate = {2024-06-06},
  abstract = {Simulation models that describe autonomous individual organisms (individual based models, IBM) or agents (agent-based models, ABM) have become a widely used tool, not only in ecology, but also in many other disciplines dealing with complex systems made up of autonomous entities. However, there is no standard protocol for describing such simulation models, which can make them difficult to understand and to duplicate. This paper presents a proposed standard protocol, ODD, for describing IBMs and ABMs, developed and tested by 28 modellers who cover a wide range of fields within ecology. This protocol consists of three blocks (Overview, Design concepts, and Details), which are subdivided into seven elements: Purpose, State variables and scales, Process overview and scheduling, Design concepts, Initialization, Input, and Submodels. We explain which aspects of a model should be described in each element, and we present an example to illustrate the protocol in use. In addition, 19 examples are available in an Online Appendix. We consider ODD as a first step for establishing a more detailed common format of the description of IBMs and ABMs. Once initiated, the protocol will hopefully evolve as it becomes used by a sufficiently large proportion of modellers.},
  keywords = {Agent-based model,Individual-based model,Model description,Scientific communication,Standardization},
  file = {/home/baldoinov/Zotero/storage/VC8I6END/S0304380006002043.html}
}

@incollection{StatisticalInferenceTechniques_2019_DeCanditiis,
  title = {Statistical {{Inference Techniques}}},
  booktitle = {Encyclopedia of {{Bioinformatics}} and {{Computational Biology}}},
  author = {De Canditiis, Daniela},
  editor = {Ranganathan, Shoba and Gribskov, Michael and Nakai, Kenta and Schönbach, Christian},
  date = {2019-01-01},
  pages = {698--705},
  publisher = {Academic Press},
  location = {Oxford},
  doi = {10.1016/B978-0-12-809633-8.20357-9},
  url = {https://www.sciencedirect.com/science/article/pii/B9780128096338203579},
  urldate = {2025-07-04},
  abstract = {This article presents the most common hypothesis testing procedures useful for bioinformatics applications. The hypothesis tests can be divided into two categories: parametric and nonparametric. The first category includes those tests based on the assumption of knowing the distribution of the sampling population(s) and inference is drawn on one or more unknown parameter(s); the second category contains those tests that are “distribution-free”, hence generally requiring much less assumptions. For each test, we will present the mathematical assumptions under which it is applicable and the test statistics used for making inference.},
  isbn = {978-0-12-811432-2},
  keywords = {Bernoulli,Binomial,Chi-square,Fisher,Gaussian,P-value,T-student,Test Statistics},
  file = {/home/baldoinov/Zotero/storage/DFMLI7KQ/B9780128096338203579.html}
}

@online{StatisticalLearningTheory_2008_vonLuxburgSchoelkopf,
  title = {Statistical {{Learning Theory}}: {{Models}}, {{Concepts}}, and {{Results}}},
  shorttitle = {Statistical {{Learning Theory}}},
  author = {family=Luxburg, given=Ulrike, prefix=von, useprefix=true and Schoelkopf, Bernhard},
  date = {2008-10-27},
  eprint = {0810.4752},
  eprinttype = {arXiv},
  eprintclass = {math, stat},
  doi = {10.48550/arXiv.0810.4752},
  url = {http://arxiv.org/abs/0810.4752},
  urldate = {2024-02-21},
  abstract = {Statistical learning theory provides the theoretical basis for many of today's machine learning algorithms. In this article we attempt to give a gentle, non-technical overview over the key ideas and insights of statistical learning theory. We target at a broad audience, not necessarily machine learning researchers. This paper can serve as a starting point for people who want to get an overview on the field before diving into technical details.},
  pubstate = {prepublished},
  keywords = {Mathematics - Statistics Theory,notion,Statistics - Machine Learning},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Statistical Learning Theory_2008_von Luxburg et al.pdf;/home/baldoinov/Zotero/storage/8MYHTEUD/0810.html}
}

@article{StatisticalModelingTwo_2001_Breiman,
  title = {Statistical {{Modeling}}: {{The Two Cultures}}},
  author = {Breiman, Leo},
  date = {2001},
  journaltitle = {Statistical Science},
  volume = {16},
  number = {3},
  eprint = {2676681},
  eprinttype = {jstor},
  pages = {199--215},
  url = {http://www.jstor.org/stable/2676681},
  abstract = {Thereare twoculturesin the use ofstatisticalmodelingto reachconclusionsfromdata. One assumes thatthe data are generated bya givenstochasticdata model.The otheruses algorithmimc odelsand treatsthe data mechanismas unknownT. he statisticalcommunityhas beencommittetdothealmostexclusiveuse ofdata models.Thiscommitmenthas ledtoirrelevantheoryq,uestionableconclusionsa,nd has kept statisticiansfromworkingon a largerangeofinterestingcurrentproblems.Algorithmimc odelingb, othin theoryand practice,has developed rapidlyin fieldsoutsidestatisticsI.t can be used bothon largecomplex data sets and as a moreaccurateand informativaelternativeto data modelingon smallerdata sets. If our goal as a fieldis to use data to solveproblemst,henwe need to moveawayfromexclusivedependence on data modelsand adopta morediverseset oftools.},
  langid = {english},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Statistical Modeling_2001_Breiman.pdf}
}

@book{StatisticsEssentialsDummies_2010_Rumsey,
  title = {Statistics {{Essentials}} for {{Dummies}}},
  author = {Rumsey, Deborah},
  date = {2010},
  publisher = {Wiley},
  location = {Hoboken, N.J},
  isbn = {978-0-470-61839-4},
  langid = {english},
  pagetotal = {178}
}

@article{steffen_2015,
  title = {Planetary Boundaries: {{Guiding}} Human Development on a Changing Planet},
  author = {Steffen, W. and Richardson, K. and Rockström, J. and Cornell, S.E. and Fetzer, I. and Bennett, E.M. and Biggs, R. and Carpenter, S.R. and De Vries, W. and De Wit, C.A. and Folke, C.},
  date = {2015},
  journaltitle = {Science (New York, N.Y.)},
  shortjournal = {Science},
  volume = {347},
  number = {6223}
}

@article{steffen_2015b,
  title = {The Trajectory of the {{Anthropocene}}: The Great Acceleration},
  author = {Steffen, W. and Broadgate, W. and Deutsch, L. and Gaffney, O. and Ludwig, C.},
  date = {2015},
  journaltitle = {The Anthropocene Review},
  volume = {2},
  pages = {81--98}
}

@online{StoppingHyperinflationLessons_1985_Dornbusch,
  type = {Working Paper},
  title = {Stopping {{Hyperinflation}}: {{Lessons}} from the {{German Inflation Experience}} of the 1920s},
  shorttitle = {Stopping {{Hyperinflation}}},
  author = {Dornbusch, Rudiger},
  date = {1985-08},
  series = {Working {{Paper Series}}},
  number = {1675},
  eprint = {1675},
  eprinttype = {National Bureau of Economic Research},
  doi = {10.3386/w1675},
  url = {https://www.nber.org/papers/w1675},
  urldate = {2024-06-04},
  abstract = {The special role of money in the hyper inflation process, and particularly in the stabilization phase, has now been reconsidered in a bestselling essay by Sargent. The message is that credible fiscal stabilizationis the sine qua non of stopping inflation. This is definitely not viewed as being in conflict with the monetary hypothesis, but it does represent a shift of emphasis. We draw attention to a third aspect of the hyperinflation process, and the stablization, namely exchange rate and interest rate policy. Even though a government may accomplish all the right measures in terms of budget stablization or control of money creation, there remains the problem of making these measures credible and hence being able to actually achieve them. We argue that exchange rate and interest rate policy in the transition have traditionally formed the vehicle for establishing that credibility by a de facto stablization. We make that point by discussing the events of the German hyperinflation. In that case the stablization was a much more diffuse, accidental matter than a reading of the classics reveals with exchange rate policy playing a key role. Immensely high interest rates in the face of a sharply appreciating free market exchange rate wiped out adverse speculation thus helping to establish stablization. The real exchange rate sharply appreciated in the final stage and persisted at an appreciated level well into the post-stabilization phase. It reflects the reverse of the coin of real depreciation in the capital flight phase.},
  pubstate = {prepublished},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/economia-brasileira-ii/Stopping Hyperinflation_1985_Dornbusch.pdf}
}

@online{Streaming101World_2015_Akidau,
  title = {Streaming 101: {{The}} World beyond Batch},
  shorttitle = {Streaming 101},
  author = {Akidau, Tyler},
  date = {2015-08-05T00:00:00-04:00},
  url = {https://www.oreilly.com/radar/the-world-beyond-batch-streaming-101/},
  urldate = {2025-03-09},
  abstract = {A high-level tour of modern data-processing concepts.},
  langid = {american},
  organization = {O’Reilly Media},
  file = {/home/baldoinov/baldoinov/PDFs/Mackenzie/ecossistemas-de-big-data-ii/Streaming 101_2015_Akidau.pdf;/home/baldoinov/Zotero/storage/SPZGSWW4/the-world-beyond-batch-streaming-101.html}
}

@article{StrengthWeakLearnability_1990_Schapire,
  title = {The Strength of Weak Learnability},
  author = {Schapire, Robert E.},
  date = {1990-06-01},
  journaltitle = {Machine Learning},
  shortjournal = {Mach Learn},
  volume = {5},
  number = {2},
  pages = {197--227},
  issn = {1573-0565},
  doi = {10.1007/BF00116037},
  url = {https://doi.org/10.1007/BF00116037},
  urldate = {2024-04-16},
  abstract = {This paper addresses the problem of improving the accuracy of an hypothesis output by a learning algorithm in the distribution-free (PAC) learning model. A concept class islearnable (orstrongly learnable) if, given access to a source of examples of the unknown concept, the learner with high probability is able to output an hypothesis that is correct on all but an arbitrarily small fraction of the instances. The concept class isweakly learnable if the learner can produce an hypothesis that performs only slightly better than random guessing. In this paper, it is shown that these two notions of learnability are equivalent.},
  langid = {english},
  keywords = {learnability theory,learning from examples,Machine learning,PAC learning,polynomial-time identification},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/machine-learning-w-pytorch-n-sklearn/The strength of weak learnability_1990_Schapire.pdf}
}

@article{StructuredPredictionModels_2020_BoltuzicSnajder,
  title = {Structured Prediction Models for Argumentative Claim Parsing from Text},
  author = {Boltužić, Filip and Šnajder, Jan},
  date = {2020-07-02},
  journaltitle = {Automatika},
  volume = {61},
  number = {3},
  pages = {361--370},
  publisher = {Taylor \& Francis},
  issn = {0005-1144},
  doi = {10.1080/00051144.2020.1761101},
  url = {https://doi.org/10.1080/00051144.2020.1761101},
  urldate = {2023-12-14},
  abstract = {The internet abounds with opinions expressed in text. While a number of natural language processing techniques have been proposed for opinion analysis from text, most offer only a shallow analysis without providing any insights into reasons supporting the opinions. In online discussions, however, opinions are typically expressed as arguments, consisting of a set of claims endowed with internal semantic structure amenable to deeper analysis. In this article, we introduce the task of argumentative claim parsing (ACP), which aims at extracting semantic structures of claims from argumentative text. The task is split into two subtasks: claim segmentation and claim structuring. We present a new dataset on two discussion topics with claims manually annotated for both subtasks. Inspired by structured prediction approaches, we propose a number of supervised machine learning models for the ACP task, including deep learning, chain classifier, and joint learning models. Our experiments reveal that claim segmentation is a relatively feasible task, with the best-performing model achieving up to 0.37 and 0.79 exact and lenient macro-averaged F1-score, respectively. Claim structuring, however, proved to be a more challenging task, with the best-performing models achieving at most 0.08 macro-averaged F1-score.},
  keywords = {argumentation mining,deep learning,machine learning,natural language processing,notion,Opinion mining,structured prediction},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/projeto-heiwa-revisao-de-literatura/Structured prediction models for argumentative claim parsing from text_2020_Boltuzic et al.pdf}
}

@book{StructureInterpretationComputer_1996_AbelsonEtAl,
  title = {Structure and {{Interpretation}} of {{Computer Programs}}},
  author = {Abelson, Harold and Sussman, Gerald Jay and Sussman, Julie},
  date = {1996-09-01},
  edition = {2nd edition},
  publisher = {The MIT Press},
  location = {Cambridge, Mass.},
  abstract = {Structure and Interpretation of Computer Programs has had a dramatic impact on computer science curricula over the past decade. This long-awaited revision contains changes throughout the text. There are new implementations of most of the major programming systems in the book, including the interpreters and compilers, and the authors have incorporated many small changes that reflect their experience teaching the course at MIT since the first edition was published. A new theme has been introduced that emphasizes the central role played by different approaches to dealing with time in computational models: objects with state, concurrent programming, functional programming and lazy evaluation, and nondeterministic programming. There are new example sections on higher-order procedures in graphics and on applications of stream processing in numerical programming, and many new exercises. In addition, all the programs have been reworked to run in any Scheme implementation that adheres to the IEEE standard.},
  isbn = {978-0-262-51087-5},
  langid = {english},
  pagetotal = {657},
  file = {/home/baldoinov/baldoinov/PDFs/Livros/Structure and Interpretation of Computer Programs_1996_Abelson et al.pdf}
}

@article{SubstantialBiasIgnoring_2003_GoulderWilliamsIII,
  title = {The {{Substantial Bias}} from {{Ignoring General Equilibrium Effects}} in {{Estimating Excess Burden}}, and a {{Practical Solution}}},
  author = {Goulder, Lawrence~H. and Williams III, Roberton~C.},
  date = {2003-08},
  journaltitle = {Journal of Political Economy},
  volume = {111},
  number = {4},
  pages = {898--927},
  publisher = {The University of Chicago Press},
  issn = {0022-3808},
  doi = {10.1086/375378},
  url = {https://www.journals.uchicago.edu/doi/10.1086/375378},
  urldate = {2023-08-20},
  abstract = {We show that under typical conditions the simple “excess‐burden triangle” formula substantially underestimates the excess burden of commodity taxes, in some cases by a factor of 10 or more. This formula performs poorly because it ignores general equilibrium interactions—most important, interactions between the taxed commodity and the labor market. Many prior studies have shown that general equilibrium interactions affect excess burden but have not appreciated the bias associated with ignoring these interactions or the quantitative importance of this bias. We derive an implementable alternative to the simple formula. This alternative formula captures interactions that the simple formula omits; as a result it is both unbiased and usually more accurate.},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/The Substantial Bias from Ignoring General Equilibrium Effects in Estimating_2003_Goulder et al.pdf}
}

@article{SufficientStatisticsRevisited_2021_Kleven,
  title = {Sufficient {{Statistics Revisited}}},
  author = {Kleven, Henrik J.},
  date = {2021},
  journaltitle = {Annual Review of Economics},
  volume = {13},
  number = {1},
  pages = {515--538},
  doi = {10.1146/annurev-economics-060220-023547},
  url = {https://doi.org/10.1146/annurev-economics-060220-023547},
  urldate = {2023-10-01},
  abstract = {This article reviews and generalizes the sufficient statistics approach to policy evaluation. The idea of the approach is that the welfare effect of policy changes can be expressed in terms of estimable reduced-form elasticities, allowing for policy evaluation without estimating the structural primitives of fully specified models. The approach relies on three assumptions: that policy changes are small, that government policy is the only source of market imperfection, and that a set of high-level restrictions on the environment and on preferences can be used to reduce the number of elasticities to be estimated. We generalize the approach in all three dimensions. It is possible to develop transparent sufficient statistics formulas under very general conditions, but the estimation requirements increase greatly. Starting from such general formulas elucidates that feasible empirical implementations are in fact structural approaches.},
  keywords = {discrete reforms,elasticities,fiscal externality,JEL D04,JEL D10,JEL D60,JEL H20,JEL H30,notion,structural approach,sufficient statistics,welfare analysis},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Sufficient Statistics Revisited_2021_Kleven.pdf}
}

@article{SufficientStatisticsWelfare_2009_Chetty,
  title = {Sufficient {{Statistics}} for {{Welfare Analysis}}: {{A Bridge Between Structural}} and {{Reduced-Form Methods}}},
  shorttitle = {Sufficient {{Statistics}} for {{Welfare Analysis}}},
  author = {Chetty, Raj},
  date = {2009-09-01},
  journaltitle = {Annual Review of Economics},
  volume = {1},
  pages = {451--488},
  publisher = {Annual Reviews},
  issn = {1941-1383, 1941-1391},
  doi = {10.1146/annurev.economics.050708.142910},
  url = {https://www.annualreviews.org/content/journals/10.1146/annurev.economics.050708.142910},
  urldate = {2024-06-10},
  abstract = {The debate between structural and reduced-form approaches has generated substantial controversy in applied economics. This article reviews a recent literature in public economics that combines the advantages of reduced-form strategies—transparent and credible identification—with an important advantage of structural models—the ability to make predictions about counterfactual outcomes and welfare. This literature has developed formulas for the welfare consequences of various policies that are functions of reduced-form elasticities rather than structural primitives. I present a general framework that shows how many policy questions can be answered by estimating a small set of sufficient statistics using program-evaluation methods. I use this framework to synthesize the modern literature on taxation, social insurance, and behavioral welfare economics. Finally, I discuss problems in macroeconomics, labor, development, and industrial organization that could be tackled using the sufficient statistic approach.},
  issue = {Volume 1, 2009},
  langid = {english},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Sufficient Statistics for Welfare Analysis_2009_Chetty.pdf;/home/baldoinov/Zotero/storage/Q9UKJVIA/annurev.economics.050708.html}
}

@video{SUPERAULACOMO_2025_AguinaldoRamos,
  entrysubtype = {video},
  title = {{{SUPER AULA}} - {{COMO MONTAR O SISTEMA DE SOM DA SUA IGREJA}}},
  editor = {{Aguinaldo Ramos}},
  editortype = {director},
  date = {2025-03-24},
  url = {https://www.youtube.com/watch?v=p547nEm9gLU},
  urldate = {2025-08-17},
  abstract = {✅ ADQUIRA SEU INGRESSO - Workshop “da Montagem à Mixagem”}
}

@article{SurveyAgentbasedModelling_2023_Platas-LopezEtAl,
  title = {A Survey on Agent-Based Modelling Assisted by Machine Learning},
  author = {Platas-López, Alejandro and Guerra-Hernández, Alejandro and Quiroz-Castellanos, Marcela and Cruz-Ramirez, Nicandro},
  date = {2023-05-13},
  journaltitle = {Expert Systems},
  volume = {n/a},
  number = {n/a},
  pages = {e13325},
  issn = {1468-0394},
  doi = {10.1111/exsy.13325},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/exsy.13325},
  urldate = {2023-09-18},
  abstract = {Agent-based models have diversified their applications across various domains due to the ease with which different phenomena can be represented and simulated. These models incorporate heterogeneous, autonomous agents, local interactions, bounded rationality, and often feature explicit spatial representations. However, certain challenges have been identified in their application, including the complexity of design, difficulty in calibrating parameters, and interpreting and analysing results. Therefore, incorporating machine learning (ML) tools in the various stages of the agent-based modelling and simulation process presents a promising approach for current and future research. The main hypothesis of this study is that integrating ML techniques and tools into agent-based modelling can help address challenges encountered during different stages of implementation, ultimately leading to more accurate and effective simulations. The methodology employed in this study involves a comprehensive search and analysis of relevant literature on the topic. This survey reviews significant developments in the integration of ML into the agent-based modelling and simulation process in recent years. The results of this study summarize the fundamental concepts of ML and its applications in agent-based modelling, and provide insights into the prospects and challenges for ML-assisted agent-based modelling in the near future.},
  langid = {english},
  keywords = {agent-based modelling,critics,development stages,machine-learning,neural networks,notion,reinforcement learning},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/A survey on agent-based modelling assisted by machine learning_2023_Platas-Lopez et al.pdf;/home/baldoinov/Zotero/storage/NVNPHYGJ/exsy.html}
}

@inproceedings{SurveyClassicalDeep_2021_REtAl,
  title = {A {{Survey}} on {{Classical}} and {{Deep Learning}} Based {{Intermittent Time Series Forecasting Methods}}},
  booktitle = {2021 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {R, Karthikeswaren and Kayathwal, Kanishka and Dhama, Gaurav and Arora, Ankur},
  date = {2021-07},
  pages = {1--7},
  issn = {2161-4407},
  doi = {10.1109/IJCNN52387.2021.9533963},
  url = {https://ieeexplore.ieee.org/document/9533963},
  urldate = {2024-10-18},
  abstract = {Demand forecasting is a fundamental aspect of inventory and supply chain management. Due to the sporadic nature of the demand, demand forecasting involves dealing with intermittent time series in domains such as retail, manufacturing. Conventional forecasting methods do not work well for intermittent time series due to inherent sparsity in such series. Researchers have proposed multiple methods to deal with intermittent time series such as Croston and its variants. Our work aims to provide an insight into the various forecasting methods traditionally known to work well for forecasting intermittent series. We have also explored deep learning methods that have been proposed in recent literature. These methods are thoroughly reviewed and explained in this survey paper. Additionally, experiments are done on two publicly available datasets to compare the performance of the traditional methods with deep learning models. Furthermore, a hybrid model made of independent classification and regression trees has been implemented and studied as well. We provide a comprehensive evaluation that aims at selecting the appropriate method, given the dataset, context, and objectives that have to be met by the forecasting practitioner/researcher.},
  eventtitle = {2021 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  keywords = {deep learning,Deep learning,Demand forecasting,forecasting,intermittent demand,Predictive models,Productivity,Supply chain management,Supply chains,survey,Time series analysis},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Recovery/Modelo de Incidencia de Acao Contra/A Survey on Classical and Deep Learning based Intermittent Time Series_2021_R et al.pdf;/home/baldoinov/Zotero/storage/6ZCN7HDI/9533963.html}
}

@article{SurveyClusteringAlgorithms_2005_XuWunsch,
  title = {Survey of Clustering Algorithms},
  author = {Xu, Rui and Wunsch, D.},
  date = {2005-05},
  journaltitle = {IEEE Transactions on Neural Networks},
  volume = {16},
  number = {3},
  pages = {645--678},
  issn = {1941-0093},
  doi = {10.1109/TNN.2005.845141},
  url = {https://ieeexplore.ieee.org/document/1427769},
  urldate = {2024-09-06},
  abstract = {Data analysis plays an indispensable role for understanding various phenomena. Cluster analysis, primitive exploration with little or no prior knowledge, consists of research developed across a wide variety of communities. The diversity, on one hand, equips us with many tools. On the other hand, the profusion of options causes confusion. We survey clustering algorithms for data sets appearing in statistics, computer science, and machine learning, and illustrate their applications in some benchmark data sets, the traveling salesman problem, and bioinformatics, a new field attracting intensive efforts. Several tightly related topics, proximity measure, and cluster validation, are also discussed.},
  eventtitle = {{{IEEE Transactions}} on {{Neural Networks}}},
  keywords = {Adaptive resonance theory (ART),Application software,Bioinformatics,cluster validation,clustering,clustering algorithm,Clustering algorithms,Computer science,Data analysis,Humans,Machine learning,Machine learning algorithms,neural networks,proximity,self-organizing feature map (SOFM),Statistics,Traveling salesman problems},
  file = {/home/baldoinov/baldoinov/PDFs/Mackenzie/aprendizado-de-maquina-ii/Survey of clustering algorithms_2005_Xu et al.pdf;/home/baldoinov/Zotero/storage/W6B745VH/1427769.html}
}

@online{SurveyDeepLearning_2023_SchneiderVlachos,
  title = {A {{Survey}} of {{Deep Learning}}: {{From Activations}} to {{Transformers}}},
  shorttitle = {A {{Survey}} of {{Deep Learning}}},
  author = {Schneider, Johannes and Vlachos, Michalis},
  date = {2023-08-09},
  eprint = {2302.00722},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2302.00722},
  url = {http://arxiv.org/abs/2302.00722},
  urldate = {2023-11-30},
  abstract = {The past decade has witnessed remarkable advancements in deep learning, owing to the emergence of various architectures, layers, objectives, and optimization techniques. These consist of a multitude of variations of attention, normalization, skip connections, transformer, and self-supervised learning methods, among others. Our goal is to furnish a comprehensive survey of significant recent contributions in these domains to individuals with a fundamental grasp of deep learning. Our aspiration is that an integrated and comprehensive approach of influential recent works will facilitate the formation of new connections between different areas of deep learning. In our discussion, we discuss multiple patterns that summarize the key strategies for many of the successful innovations over the last decade. We also include a discussion on recent commercially built, closed-source models such as OpenAI's GPT-4 and Google's PaLM 2.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,notion},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/A Survey of Deep Learning_2023_Schneider et al.pdf;/home/baldoinov/Zotero/storage/SCUUFXB5/2302.html}
}

@online{SurveyLargeLanguage_2023_WangEtAl,
  title = {A {{Survey}} on {{Large Language Model}} Based {{Autonomous Agents}}},
  author = {Wang, Lei and Ma, Chen and Feng, Xueyang and Zhang, Zeyu and Yang, Hao and Zhang, Jingsen and Chen, Zhiyuan and Tang, Jiakai and Chen, Xu and Lin, Yankai and Zhao, Wayne Xin and Wei, Zhewei and Wen, Ji-Rong},
  date = {2023-08-22},
  eprint = {2308.11432},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2308.11432},
  url = {http://arxiv.org/abs/2308.11432},
  urldate = {2023-09-28},
  abstract = {Autonomous agents have long been a prominent research focus in both academic and industry communities. Previous research in this field often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from human learning processes, and thus makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of web knowledge, large language models (LLMs) have demonstrated remarkable potential in achieving human-level intelligence. This has sparked an upsurge in studies investigating LLM-based autonomous agents. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of the field of LLM-based autonomous agents from a holistic perspective. More specifically, we first discuss the construction of LLM-based autonomous agents, for which we propose a unified framework that encompasses a majority of the previous work. Then, we present a comprehensive overview of the diverse applications of LLM-based autonomous agents in the fields of social science, natural science, and engineering. Finally, we delve into the evaluation strategies commonly used for LLM-based autonomous agents. Based on the previous studies, we also present several challenges and future directions in this field. To keep track of this field and continuously update our survey, we maintain a repository of relevant references at https://github.com/Paitesanshi/LLM-Agent-Survey.},
  pubstate = {prepublished},
  version = {1},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,notion},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/A Survey on Large Language Model based Autonomous Agents_2023_Wang et al.pdf;/home/baldoinov/Zotero/storage/AJDTV9QV/2308.html}
}

@online{SurveyLargeLanguage_2023_ZhaoEtAl,
  title = {A {{Survey}} of {{Large Language Models}}},
  author = {Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and Du, Yifan and Yang, Chen and Chen, Yushuo and Chen, Zhipeng and Jiang, Jinhao and Ren, Ruiyang and Li, Yifan and Tang, Xinyu and Liu, Zikang and Liu, Peiyu and Nie, Jian-Yun and Wen, Ji-Rong},
  date = {2023-03-31},
  url = {https://arxiv.org/abs/2303.18223v13},
  urldate = {2024-03-03},
  abstract = {Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size. Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT, which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. In this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/A Survey of Large Language Models_2023_Zhao et al.pdf}
}

@article{SurveyNamedEntity_2023_JehangirEtAl,
  title = {A Survey on {{Named Entity Recognition}} —~Datasets, Tools, and Methodologies},
  author = {Jehangir, Basra and Radhakrishnan, Saravanan and Agarwal, Rahul},
  date = {2023-06-01},
  journaltitle = {Natural Language Processing Journal},
  shortjournal = {Natural Language Processing Journal},
  volume = {3},
  pages = {100017},
  issn = {2949-7191},
  doi = {10.1016/j.nlp.2023.100017},
  url = {https://www.sciencedirect.com/science/article/pii/S2949719123000146},
  urldate = {2024-03-18},
  abstract = {Natural language processing (NLP) is crucial in the current processing of data because it takes into account many sources, formats, and purposes of data as well as information from various sectors of our economy, government, and private and public lives. We perform a variety of NLP operations on the text in order to complete certain tasks. One of them is NER (Named Entity Recognition). An act of recognizing and categorizing named entities that are presented in a text document is known as named entity recognition. The purpose of NER is to find references of rigid designators in the text which belong to established semantic kinds like a person, place, organization, etc. It acts as a cornerstone for many Information Extraction-related activities. In this work, we present a thorough analysis of several methodologies for NER ranging from unsupervised learning, rule-based, supervised learning, and various Deep Learning based approaches. We examine the most relevant datasets, tools, and deep learning approaches like Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Bidirectional Long Short Term Memory, Transfer learning approaches, and numerous other approaches currently being used in present-day NER problem environments and their applications. Finally, we outline the difficulties NER systems encounter and future directions.},
  keywords = {Bidirectional Long Short Term Memory,Convolutional Neural Network,Deep Learning,Named Entity Recognition,Natural language processing,Recurrent Neural Networks},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/projeto-heiwa/A survey on Named Entity Recognition — datasets, tools, and methodologies_2023_Jehangir et al.pdf}
}

@article{SurveyTransformers_2022_LinEtAl,
  title = {A Survey of Transformers},
  author = {Lin, Tianyang and Wang, Yuxin and Liu, Xiangyang and Qiu, Xipeng},
  date = {2022-01-01},
  journaltitle = {AI Open},
  shortjournal = {AI Open},
  volume = {3},
  pages = {111--132},
  issn = {2666-6510},
  doi = {10.1016/j.aiopen.2022.10.001},
  url = {https://www.sciencedirect.com/science/article/pii/S2666651022000146},
  urldate = {2023-12-07},
  abstract = {Transformers have achieved great success in many artificial intelligence fields, such as natural language processing, computer vision, and audio processing. Therefore, it is natural to attract lots of interest from academic and industry researchers. Up to the present, a great variety of Transformer variants (a.k.a. X-formers) have been proposed, however, a systematic and comprehensive literature review on these Transformer variants is still missing. In this survey, we provide a comprehensive review of various X-formers. We first briefly introduce the vanilla Transformer and then propose a new taxonomy of X-formers. Next, we introduce the various X-formers from three perspectives: architectural modification, pre-training, and applications. Finally, we outline some potential directions for future research.},
  keywords = {Deep learning,notion,Pre-trained models,Self-attention,Transformer},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/A survey of transformers_2022_Lin et al.pdf}
}

@article{SustainableOperationsIndustrial_2020_YazanFraccascia,
  title = {Sustainable Operations of Industrial Symbiosis: An Enterprise Input-Output Model Integrated by Agent-Based Simulation},
  shorttitle = {Sustainable Operations of Industrial Symbiosis},
  author = {Yazan, Devrim Murat and Fraccascia, Luca},
  date = {2020-01-17},
  journaltitle = {International Journal of Production Research},
  volume = {58},
  number = {2},
  pages = {392--414},
  publisher = {Taylor \& Francis},
  issn = {0020-7543},
  doi = {10.1080/00207543.2019.1590660},
  url = {https://doi.org/10.1080/00207543.2019.1590660},
  urldate = {2023-09-19},
  abstract = {Industrial symbiosis (IS) is a key for implementing circular economy. Through IS, wastes produced by one company are used as inputs by other companies. The operations of IS suffers from uncertainty barriers since wastes are not produced upon demand but emerge as secondary outputs. Such an uncertainty, triggered by waste supply-demand quantity mismatch, influences IS business dynamics. Accordingly, companies have difficulty to foresee potential costs and benefits of implementing IS. The paper adopts an enterprise input-output model providing a cost–benefit analysis of IS integrated to an agent-based model to simulate how companies share the total economic benefits stemming from IS. The proposed model allows to explore the space of cooperation, defined as the operationally favourable conditions to operate IS in an economically win-win manner. This approach, as a decision-support tool, allows the user to understand whether the IS relationship is created and how should the cost-sharing policy be. The proposed model is applied to a numerical example. Findings show that cost-sharing strategies are dramatically affected by waste supply-demand mismatch and by the relationship between saved and additional costs to run IS. Apart from methodological and theoretical contributions, the paper proposes managerial and practical implications for business strategy development in IS.},
  keywords = {agent-based simulation,circular economy,enterprise input-output,Industrial symbiosis,notion,sustainable operations},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Sustainable operations of industrial symbiosis_2020_Yazan et al.pdf}
}

@online{SWARMParallelismTraining_2023_RyabininEtAl,
  title = {{{SWARM Parallelism}}: {{Training Large Models Can Be Surprisingly Communication-Efficient}}},
  shorttitle = {{{SWARM Parallelism}}},
  author = {Ryabinin, Max and Dettmers, Tim and Diskin, Michael and Borzunov, Alexander},
  date = {2023-01-27},
  url = {https://arxiv.org/abs/2301.11913v2},
  urldate = {2024-05-14},
  abstract = {Many deep learning applications benefit from using large models with billions of parameters. Training these models is notoriously expensive due to the need for specialized HPC clusters. In this work, we consider alternative setups for training large models: using cheap "preemptible" instances or pooling existing resources from multiple regions. We analyze the performance of existing model-parallel algorithms in these conditions and find configurations where training larger models becomes less communication-intensive. Based on these findings, we propose SWARM parallelism, a model-parallel training algorithm designed for poorly connected, heterogeneous and unreliable devices. SWARM creates temporary randomized pipelines between nodes that are rebalanced in case of failure. We empirically validate our findings and compare SWARM parallelism with existing large-scale training approaches. Finally, we combine our insights with compression strategies to train a large Transformer language model with 1B shared parameters (approximately 13B before sharing) on preemptible T4 GPUs with less than 200Mb/s network.},
  langid = {english},
  pubstate = {prepublished},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/SWARM Parallelism_2023_Ryabinin et al.pdf}
}

@book{SyncEmergingScience_2008_Strogatz,
  title = {Sync: The Emerging Science of Spontaneous Order},
  shorttitle = {Sync},
  author = {Strogatz, Steven},
  date = {2008-04-01},
  publisher = {Penguin Books, Limited},
  location = {London},
  abstract = {'SYNC' IS A STORY OF A DAZZLING KIND OF ORDER IN THE UNIVERSE, THE HARMONY THAT COMES FROM CYCLES IN SYNC. THE TENDENCY TO SYCHRONIZE IS ONE OF THE MOST FAR- REACHING DRIVES IN ALL OF NATURE. IT EXTENDS FROM PEOPLE TO PLANETS, FROM ANIMALS TO ATOMS. IN 'SYNC' PROFESSOR STEVEN STROGATZ CONSIDERS A RANGE OF APPLICATIONS - HUMAN SLEEP AND CIRCADIAN RHYTHMS, MENSTRUAL SYNCHRONY, INSECT OUTBREAKS, SUPERCONDUCTORS, LASERS, SECRET CODES, HEART ARRHYTHMIAS AND FADS - CONNECTING ALL TRHOUGH AN EXPLORATION OF THE SAME MATHEMATICAL THEME: SELF- ORGANISATION, OR THE SPONTANEOUS EMERGENCE OF ORDER OUT OF CHAOS. FOCUSED ENOUGH TO PRESENT A COHERENT WORLD UNTO THEMSELVES, STROGATZ'S CHOSEN TOPICS TOUCH ON SEVERAL OF THE HOTTEST DIRECTIONS IN CONTEMPORARY SCIENCE.},
  isbn = {978-0-14-100763-2},
  langid = {Inglês},
  keywords = {notion}
}

@article{SynergisticIntegrationMachine_2023_ZhangEtAl,
  title = {Synergistic {{Integration Between Machine Learning}} and {{Agent-Based Modeling}}: {{A Multidisciplinary Review}}},
  shorttitle = {Synergistic {{Integration Between Machine Learning}} and {{Agent-Based Modeling}}},
  author = {Zhang, Wei and Valencia, Andrea and Chang, Ni-Bin},
  date = {2023-05},
  journaltitle = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {34},
  number = {5},
  pages = {2170--2190},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2021.3106777},
  abstract = {Agent-based modeling (ABM) involves developing models in which agents make adaptive decisions in a changing environment. Machine-learning (ML) based inference models can improve sequential decision-making by learning agents’ behavioral patterns. With the aid of ML, this emerging area can extend traditional agent-based schemes that hardcode agents’ behavioral rules into an adaptive model. Even though there are plenty of studies that apply ML in ABMs, the generalized applicable scenarios, frameworks, and procedures for implementations are not well addressed. In this article, we provide a comprehensive review of applying ML in ABM based on four major scenarios, i.e., microagent-level situational awareness learning, microagent-level behavior intervention, macro-ABM-level emulator, and sequential decision-making. For these four scenarios, the related algorithms, frameworks, procedures of implementations, and multidisciplinary applications are thoroughly investigated. We also discuss how ML can improve prediction in ABMs by trading off the variance and bias and how ML can improve the sequential decision-making of microagent and macrolevel policymakers via a mechanism of reinforced behavioral intervention. At the end of this article, future perspectives of applying ML in ABMs are discussed with respect to data acquisition and quality issues, the possible solution of solving the convergence problem of reinforcement learning, interpretable ML applications, and bounded rationality of ABM.},
  eventtitle = {{{IEEE Transactions}} on {{Neural Networks}} and {{Learning Systems}}},
  keywords = {Adaptation models,Agent-based modeling (ABM),behavioral intervention,Biological system modeling,Cognition,Decision making,machine learning (ML),Neutrons,notion,Numerical models,Predictive models,reinforcement learning (RL)},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Synergistic Integration Between Machine Learning and Agent-Based Modeling_2023_Zhang et al.pdf;/home/baldoinov/Zotero/storage/4CXTDR9M/9527397.html}
}

@incollection{SystematicSoftwareReuse_2016_SouzaEtAl,
  title = {Toward {{Systematic Software Reuse}}: {{From Concept}} to {{Modular Software Implementation}}},
  shorttitle = {Toward {{Systematic Software Reuse}}},
  booktitle = {Transdisciplinary {{Engineering}}: {{Crossing Boundaries}}},
  author = {Souza, Eric C. and Takase, Fabio K. and Costa, Rafael L. and Aguchiku, Fabio S.},
  date = {2016},
  pages = {818--827},
  publisher = {IOS Press},
  doi = {10.3233/978-1-61499-703-0-818},
  url = {https://ebooks.iospress.nl/doi/10.3233/978-1-61499-703-0-818},
  urldate = {2025-02-27},
  file = {/home/baldoinov/baldoinov/PDFs/Mackenzie/topicos-de-engenharia-de-software/Toward Systematic Software Reuse_2016_Souza et al.pdf}
}

@online{TakeYourLocal_2022_Freitas,
  title = {Take Your Local Development Experience to the next Level with {{Dev Containers}}},
  author = {Freitas, Cassio Lamarck Silva},
  date = {2022-12-03T19:23:13},
  url = {https://towardsdev.com/take-your-local-development-experience-to-the-next-level-with-dev-containers-30ae679c90f0},
  urldate = {2024-04-01},
  abstract = {Build shareable development environments with dependencies, databases and webservers in a matter of minutes.},
  langid = {english},
  organization = {Medium},
  file = {/home/baldoinov/Zotero/storage/T9GKBZED/take-your-local-development-experience-to-the-next-level-with-dev-containers-30ae679c90f0.html}
}

@article{TaxAIDynamicEconomic_2023_MiEtAl,
  title = {{{TaxAI}}: {{A Dynamic Economic Simulator}} and {{Benchmark}} for {{Multi-Agent Reinforcement Learning}}},
  shorttitle = {{{TaxAI}}},
  author = {Mi, Qirui and Xia, Siyu and Song, Yan and Zhang, Haifeng and Zhu, Shenghao and Wang, Jun},
  date = {2023},
  publisher = {[object Object]},
  doi = {10.48550/ARXIV.2309.16307},
  url = {https://arxiv.org/abs/2309.16307},
  urldate = {2024-05-14},
  abstract = {Taxation and government spending are crucial tools for governments to promote economic growth and maintain social equity. However, the difficulty in accurately predicting the dynamic strategies of diverse self-interested households presents a challenge for governments to implement effective tax policies. Given its proficiency in modeling other agents in partially observable environments and adaptively learning to find optimal policies, Multi-Agent Reinforcement Learning (MARL) is highly suitable for solving dynamic games between the government and numerous households. Although MARL shows more potential than traditional methods such as the genetic algorithm and dynamic programming, there is a lack of large-scale multi-agent reinforcement learning economic simulators. Therefore, we propose a MARL environment, named \textbackslash textbf\{TaxAI\}, for dynamic games involving \$N\$ households, government, firms, and financial intermediaries based on the Bewley-Aiyagari economic model. Our study benchmarks 2 traditional economic methods with 7 MARL methods on TaxAI, demonstrating the effectiveness and superiority of MARL algorithms. Moreover, TaxAI's scalability in simulating dynamic interactions between the government and 10,000 households, coupled with real-data calibration, grants it a substantial improvement in scale and reality over existing simulators. Therefore, TaxAI is the most realistic economic simulator for optimal tax policy, which aims to generate feasible recommendations for governments and individuals.},
  version = {2},
  keywords = {Computational Engineering Finance and Science (cs.CE),FOS: Computer and information sciences},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/TaxAI_2023_Mi et al.pdf}
}

@incollection{TaxationEconomicEfficiency_2002_AuerbachHines,
  title = {Taxation and {{Economic Efficiency}}},
  booktitle = {Handbook of {{Public Economics}}},
  author = {Auerbach, Alan J. and Hines, James R.},
  editor = {Auerbach, Alan J. and Feldstein, Martin},
  date = {2002-01-01},
  volume = {3},
  pages = {1347--1421},
  publisher = {Elsevier},
  doi = {10.1016/S1573-4420(02)80025-8},
  url = {https://www.sciencedirect.com/science/article/pii/S1573442002800258},
  urldate = {2023-09-28},
  abstract = {This chapter analyzes the distortions created by taxation and the features of tax systems that minimize such distortions (subject to achieving other government objectives). It starts with a review of the theory and practice of deadweight loss measurement, followed by characterizations of optimal commodity taxation and optimal linear and nonlinear income taxation. The framework is then extended to a variety of settings, initially consisting of optimal taxation in the presence of externalities or public goods. The optimal tax analysis is subsequently applied to situations in which product markets are imperfectly competitive. This is followed by consideration of the features of optimal intertemporal taxation. The purpose of the chapter is not only to provide an up-to-date review and analysis of the optimal taxation literature, but also to identify important cross-cutting themes within that literature.},
  keywords = {deadweight loss,excess burden,H21,marginal cost of public funds,notion,optimal taxation,Ramsey taxation},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Taxation and Economic Efficiency_2002_Auerbach et al.pdf}
}

@article{TaxationMobileHighIncome_2019_CasalEtAl,
  title = {Taxation with {{Mobile High-Income Agents}}: {{Experimental Evidence}} on {{Tax Compliance}} and {{Equity Perceptions}}},
  shorttitle = {Taxation with {{Mobile High-Income Agents}}},
  author = {Casal, Sandro and Grimm, Veronika and Schächtele, Simeon},
  date = {2019-12},
  journaltitle = {Games},
  volume = {10},
  number = {4},
  pages = {42},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2073-4336},
  doi = {10.3390/g10040042},
  url = {https://www.mdpi.com/2073-4336/10/4/42},
  urldate = {2024-06-11},
  abstract = {In a laboratory experiment on tax compliance, we model a situation in which high-income taxpayers can leave a tax system that finances a public good. We compare low-income taxpayers’ compliance decisions and equity perceptions across treatments in which they are informed or not informed about the mobility option of high-income taxpayers. This allows us to test if low-income taxpayers regard the mobility option as a rationale for implementing a regressive tax schedule. To investigate if a potential ‘justification effect’ of the mobility option depends on the causes of income heterogeneity, we also varied whether income was allocated based on relative performance in a prior ability task or at random. Interestingly, although the performance-based allocation itself was judged to be fairer, we observed higher compliance under the random allocation mechanism. However, compliance and equity perceptions did not significantly differ by the information treatment variation, regardless of the source of income inequality. The results indicate that the threat of losing high-income taxpayers’ contributions does not lead low-income taxpayers to view the regressive tax schedule more favorably. This suggests that taking the differential mobility options as given and altering tax schedules accordingly may not be perceived as an adequate policy response.},
  issue = {4},
  langid = {english},
  keywords = {fairness perceptions,heterogeneous income,laboratory experiment,mobility,optimal taxation,tax compliance},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Taxation with Mobile High-Income Agents_2019_Casal et al.pdf}
}

@incollection{TaxationResourceAllocation_1964_Harberger,
  title = {Taxation, {{Resource Allocation}}, and {{Welfare}}},
  booktitle = {The {{Role}} of {{Direct}} and {{Indirect Taxes}} in the {{Federal Reserve System}}},
  author = {Harberger, Arnold},
  date = {1964},
  pages = {25--80},
  publisher = {Princeton University Press},
  url = {https://www.nber.org/books-and-chapters/role-direct-and-indirect-taxes-federal-reserve-system/taxation-resource-allocation-and-welfare},
  urldate = {2023-09-28},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Taxation, Resource Allocation, and Welfare_1964_Harberger.pdf}
}

@online{TechdrivenWayManage_2022_LuisMoneda,
  title = {A Tech-Driven Way to Manage Your Career},
  author = {{Luis Moneda}},
  date = {2022},
  url = {https://datascienceleadership.com/docs/technical-leadership/tech-driven-career},
  urldate = {2024-08-07},
  abstract = {How to self-manage and make the technical leadership career exciting},
  langid = {american},
  organization = {Data Science Leadership},
  file = {/home/baldoinov/baldoinov/PDFs/Diversos/A tech-driven way to manage your career_2022_Luis Moneda.pdf;/home/baldoinov/Zotero/storage/JKYLDL8A/tech-driven-career.html}
}

@article{TechnologyReadinessLevels_2022_LavinEtAl,
  title = {Technology Readiness Levels for Machine Learning Systems},
  author = {Lavin, Alexander and Gilligan-Lee, Ciarán M. and Visnjic, Alessya and Ganju, Siddha and Newman, Dava and Ganguly, Sujoy and Lange, Danny and Baydin, Atílím Güneş and Sharma, Amit and Gibson, Adam and Zheng, Stephan and Xing, Eric P. and Mattmann, Chris and Parr, James and Gal, Yarin},
  date = {2022-10-20},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {13},
  number = {1},
  pages = {6039},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-022-33128-9},
  url = {https://www.nature.com/articles/s41467-022-33128-9},
  urldate = {2024-05-05},
  abstract = {The development and deployment of machine learning systems can be executed easily with modern tools, but the process is typically rushed and means-to-an-end. Lack of diligence can lead to technical debt, scope creep and misaligned objectives, model misuse and failures, and expensive consequences. Engineering systems, on the other hand, follow well-defined processes and testing standards to streamline development for high-quality, reliable results. The extreme is spacecraft systems, with mission critical measures and robustness throughout the process. Drawing on experience in both spacecraft engineering and machine learning (research through product across domain areas), we’ve developed a proven systems engineering approach for machine learning and artificial intelligence: the Machine Learning Technology Readiness Levels framework defines a principled process to ensure robust, reliable, and responsible systems while being streamlined for machine learning workflows, including key distinctions from traditional software engineering, and a lingua franca for people across teams and organizations to work collaboratively on machine learning and artificial intelligence technologies. Here we describe the framework and elucidate with use-cases from physics research to computer vision apps to medical diagnostics.},
  langid = {english},
  keywords = {Aerospace engineering,Computational science,Information technology,Software,Technology},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Technology readiness levels for machine learning systems_2022_Lavin et al.pdf}
}

@book{TecnologiaInovacaoPoliticas_2024_MarquesEtAl,
  title = {Tecnologia e Inovação: Políticas públicas e aspectos regulatórios do setor},
  shorttitle = {TECNOLOGIA E INOVAÇÃO},
  author = {Marques, Jamile Sabatini and Roque, Paulo Milliet and Lopes, Roseli de Deus},
  editor = {Ramos, Joice Adinete and Savelli, Clara and Matias, Heloisa Catani Mariani Pavoni and Pires, Daniela de Sá Jacobina},
  date = {2024},
  edition = {1ª edição},
  publisher = {Think Tank da ABES - Associação Brasileira das Empresas de Software},
  abstract = {A Associação Brasileira das Empresas de Software (ABES) como porta-voz das empresas intensivas em conhecimento com relação direta ou indireta com o setor de tecnologia da informação (TI) busca sempre estar envolvida com os temas que as impactam, com o propósito de estabelecer iniciativas que maximizem as oportunidades ou mesmo as criem, bem como mitiguem os empecilhos. Promover um Brasil mais digital e menos desigual perpassa pela análise do contexto em que estamos inseridos com o olhar direcionado a executar ações que gerem desenvolvimento socioeconômico sustentável.TUDO É SOFTWARE. As relações humanas e empresariais de uma forma ou de outra são afetadas por soluções que envolvem software. Sob essa perspectiva, é reforçada a relevância que a ABES tem ao desempenhar o seu papel de intermediadora e de representante das empresas que atuam de diferentes modos na área de software, tendo em vista que os efeitos promovidos por elas são marcantes e influenciam fortemente as dinâmicas social e econômica.Desse modo, ao criar o Think Tank - Centro de Inteligência, Políticas Públicas e Inovação em parceria com Instituto de Estudos Avançados da Universidade de São Paulo (IEA/USP), a ABES expandiu e diversificou as suas frentes de trabalho de modo a viabilizar a aproximação entre empresas e universidade, fator de grande necessidade e importância para o incremento da inovação no Brasil.Poder divulgar o livro TECNOLOGIA E INOVAÇÃO: Políticas públicas e aspectos regulatórios do setor, uma coletânea de artigos de especialistas para um Brasil mais digital e menos desigual como um dos resultados gerados a partir dessa parceria é algo bastante motivador e que traduz a essência do trabalho colaborativo para o alcance de conquistas mais perenes e com repercussões significativas.A diversidade dos assuntos abordados nos artigos é ampla e rica, pois traz percepções interrelacionadas e complementares sobre um mesmo fator que impacta a sociedade e a economia de formas diferentes, o que demonstra que é preciso haver discussões pautadas e fundamentadas em torno desses aspectos para definição de diretrizes e de posicionamentos mais coesos.A ABES considera que um Brasil mais digital e menos desigual é possível por intermédio do impulsionamento do setor brasileiro de tecnologia. Portanto, a ABES tem no Think Tank uma importante vertente para fomentar e atingir interesses relacionados à construção de um cenário de negócios apto à inovação, ético, dinâmico, sustentável e competitivo globalmente para o país.},
  langid = {portuguese},
  file = {/home/baldoinov/baldoinov/PDFs/Livros/Tecnologia e Inovacao_2024_Marques et al.pdf}
}

@online{TemporalFusionTransformers_2020_LimEtAl,
  title = {Temporal {{Fusion Transformers}} for {{Interpretable Multi-horizon Time Series Forecasting}}},
  author = {Lim, Bryan and Arik, Sercan O. and Loeff, Nicolas and Pfister, Tomas},
  date = {2020-09-27},
  eprint = {1912.09363},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.1912.09363},
  url = {http://arxiv.org/abs/1912.09363},
  urldate = {2024-11-03},
  abstract = {Multi-horizon forecasting problems often contain a complex mix of inputs -- including static (i.e. time-invariant) covariates, known future inputs, and other exogenous time series that are only observed historically -- without any prior information on how they interact with the target. While several deep learning models have been proposed for multi-step prediction, they typically comprise black-box models which do not account for the full range of inputs present in common scenarios. In this paper, we introduce the Temporal Fusion Transformer (TFT) -- a novel attention-based architecture which combines high-performance multi-horizon forecasting with interpretable insights into temporal dynamics. To learn temporal relationships at different scales, the TFT utilizes recurrent layers for local processing and interpretable self-attention layers for learning long-term dependencies. The TFT also uses specialized components for the judicious selection of relevant features and a series of gating layers to suppress unnecessary components, enabling high performance in a wide range of regimes. On a variety of real-world datasets, we demonstrate significant performance improvements over existing benchmarks, and showcase three practical interpretability use-cases of TFT.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Recovery/Modelo de Incidencia de Acao Contra/Temporal Fusion Transformers for Interpretable Multi-horizon Time Series_2020_Lim et al.pdf;/home/baldoinov/Zotero/storage/2UC47ZZY/1912.html}
}

@article{TempoTempoTempo_2021_Lins,
  title = {Tempo, tempo, tempo: regressão de Cox na ciência política},
  shorttitle = {Tempo, tempo, tempo},
  author = {Lins, Rodrigo},
  date = {2021-04-08},
  journaltitle = {Revista Política Hoje},
  pages = {395--433},
  issn = {0104-7094},
  doi = {10.51359/1808-8708.2021.246485},
  url = {https://periodicos.ufpe.br/revistas/index.php/politicahoje/article/view/246485},
  urldate = {2025-02-04},
  abstract = {Este artigo tem como objetivo apresentar o modelo de riscos proporcional de Cox. Para tanto, será feito um debate teórico geral dos modelos de sobrevivência, assim como a apresentação de termos comuns para a técnica. Além disso, será feita uma apresentação passo a passo de como elaborar um modelo de Cox no software R. Espera-se que este trabalho ajude na difusão da técnica dentro das ciências sociais.},
  langid = {portuguese},
  keywords = {Análise de Sobrevivência,Modelo de Cox,R},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Tempo, tempo, tempo_2021_Lins.pdf}
}

@book{TestingStatisticalHypotheses_2022_LehmannRomano,
  title = {Testing {{Statistical Hypotheses}}},
  author = {Lehmann, E.L. and Romano, Joseph P.},
  date = {2022},
  series = {Springer {{Texts}} in {{Statistics}}},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-70578-7},
  url = {https://link.springer.com/10.1007/978-3-030-70578-7},
  urldate = {2024-04-01},
  isbn = {978-3-030-70577-0 978-3-030-70578-7},
  langid = {english},
  file = {/home/baldoinov/baldoinov/PDFs/Livros/Testing Statistical Hypotheses_2022_Lehmann et al.pdf}
}

@incollection{TheoryExcessBurden_1985_Auerbach,
  title = {The Theory of Excess Burden and Optimal Taxation},
  booktitle = {Handbook of {{Public Economics}}},
  author = {Auerbach, Alan J.},
  date = {1985-01-01},
  volume = {1},
  pages = {61--127},
  publisher = {Elsevier},
  doi = {10.1016/S1573-4420(85)80005-7},
  url = {https://www.sciencedirect.com/science/article/pii/S1573442085800057},
  urldate = {2023-09-28},
  abstract = {The theory of excess burden and optimal commodity taxation is one of the oldest subjects of study in public finance, dating back to Dupuit (1844), and yet is also closely associated with the rapid analytical development of the field which commenced in the early 1970s. This chapter presents the chronological development of the concept of excess burden and the related study of optimal tax theory. A main objective is to uncover the interrelationships among various apparently distinct results, so as to bring out the basic structure of the entire problem. The various measures of excess burden, focusing on issues of approximation, informational requirements and aggregation over individuals, and the effects of a more general technology than the commonly supposed one with fixed producer prices are developed. Some of the empirical attempts to estimate various deadweight losses are reviewed. The issue of tax reform, as distinct from de novo tax design.},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/The theory of excess burden and optimal taxation_1985_Auerbach.pdf;/home/baldoinov/Zotero/storage/DT74LHMD/S1573442085800057.html}
}

@book{ThinkPythonHow_2016_Downey,
  title = {Think {{Python}}: {{How}} to {{Think Like}} a {{Computer Scientist}}},
  shorttitle = {Think {{Python}}},
  author = {Downey, Allen},
  date = {2016-01-26},
  edition = {2nd edition},
  publisher = {O'Reilly Media},
  location = {Beijing Boston Farnham Sebastopol Tokyo},
  abstract = {If you want to learn how to program, working with Python is an excellent way to start. This hands-on guide takes you through the language a step at a time, beginning with basic programming concepts before moving on to functions, recursion, data structures, and object-oriented design. This second edition and its supporting code have been updated for Python 3.Through exercises in each chapter, you’ll try out programming concepts as you learn them. Think Python is ideal for students at the high school or college level, as well as self-learners, home-schooled students, and professionals who need to learn programming basics. Beginners just getting their feet wet will learn how to start with Python in a browser.Start with the basics, including language syntax and semanticsGet a clear definition of each programming conceptLearn about values, variables, statements, functions, and data structures in a logical progressionDiscover how to work with files and databasesUnderstand objects, methods, and object-oriented programmingUse debugging techniques to fix syntax, runtime, and semantic errorsExplore interface design, data structures, and GUI-based programs through case studies},
  isbn = {978-1-4919-3936-9},
  langid = {english},
  pagetotal = {289},
  file = {/home/baldoinov/baldoinov/PDFs/Livros/Think Python_2016_Downey.pdf}
}

@book{TimeSeriesAnalysis_1994_Hamilton,
  title = {Time {{Series Analysis}}},
  author = {Hamilton, James D.},
  date = {1994},
  publisher = {Princeton University Press},
  location = {Princeton, N.J},
  isbn = {978-0-691-04289-3},
  langid = {english},
  pagetotal = {799},
  keywords = {Time-series analysis},
  file = {/home/baldoinov/baldoinov/PDFs/Livros/Time Series Analysis_1994_Hamilton.pdf}
}

@book{TimeSeriesAnalysis_2017_ShumwayStoffer,
  title = {Time {{Series Analysis}} and {{Its Applications}}: {{With R Examples}}},
  shorttitle = {Time {{Series Analysis}} and {{Its Applications}}},
  author = {Shumway, Robert H. and Stoffer, David S.},
  date = {2017},
  series = {Springer {{Texts}} in {{Statistics}}},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-52452-8},
  url = {https://link.springer.com/10.1007/978-3-319-52452-8},
  urldate = {2024-04-01},
  isbn = {978-3-319-52451-1 978-3-319-52452-8},
  langid = {english},
  file = {/home/baldoinov/baldoinov/PDFs/Livros/Time Series Analysis and Its Applications_2017_Shumway et al.pdf}
}

@book{TinyPythonProjects_2020_Youens-Clark,
  title = {Tiny {{Python Projects}}},
  author = {Youens-Clark, Ken},
  date = {2020},
  publisher = {Manning},
  location = {Shelter Island, NY},
  isbn = {978-1-61729-751-9},
  langid = {english},
  pagetotal = {413}
}

@book{TomandoDecisoesSegundo_2024_HeberCarlosdeCamposJunior,
  title = {Tomando decisões segundo a vontade de Deus},
  shorttitle = {Tomando decisões segundo a vontade de Deus},
  author = {{Heber Carlos de Campos Júnior}},
  editor = {Filho, Tiago J. Santos},
  date = {2024-03-15},
  edition = {3ª edição},
  publisher = {Editora Fiel},
  location = {São José dos Campos, SP},
  abstract = {Como Saber qual a vontade de Deus para sua vida? Como entender o que Deus quer de nós, quando precisamos tomar decisões difíceis? Há alguma orientação nas Escrituras que ajudam os cristãos a reconhecerem a vontade de Deus? Deus manifesta sua vontade para cada decisão da vida do Homem? Essas são perguntas legítimas e muitos cristãos a enfrentam diariamente, especialmente diante de dilemas e decisões difíceis. Neste livro, o Dr. Heber Campos Jr., oferece orientações bíblicos e práticas que ajudarão o leitor a entender como submeter seus planos, projetos, aspirações e decisões à vontade de Deus.},
  isbn = {9786557233306},
  langid = {portuguese}
}

@article{TopicModelingAlgorithms_2023_AbdelrazekEtAl,
  title = {Topic Modeling Algorithms and Applications: {{A}} Survey},
  shorttitle = {Topic Modeling Algorithms and Applications},
  author = {Abdelrazek, Aly and Eid, Yomna and Gawish, Eman and Medhat, Walaa and Hassan, Ahmed},
  date = {2023-02-01},
  journaltitle = {Information Systems},
  shortjournal = {Information Systems},
  volume = {112},
  pages = {102131},
  issn = {0306-4379},
  doi = {10.1016/j.is.2022.102131},
  url = {https://www.sciencedirect.com/science/article/pii/S0306437922001090},
  urldate = {2024-03-27},
  abstract = {Topic modeling is used in information retrieval to infer the hidden themes in a collection of documents and thus provides an automatic means to organize, understand and summarize large collections of textual information. Topic models also offer an interpretable representation of documents used in several downstream Natural Language Processing (NLP) tasks. Modeling techniques vary from probabilistic graphical models to the more recent neural models. This paper surveys topic models from four aspects. The first aspect categorizes different topic modeling techniques into four categories: algebraic, fuzzy, probabilistic, and neural. We review the wide variety of available models from each category, highlight differences and similarities between models and model categories using a unified perspective, investigate these models’ characteristics and limitations, and discuss their proper use cases. The second aspect illustrates six criteria for proper evaluation of topic models, from modeling quality to interpretability, stability, efficiency, and beyond. Topic modeling has found applications in various disciplines, owing to its interpretability. We examine these applications along with some popular software tools which provide an implementation of some models. The fourth aspect reviews available datasets and benchmarks. Using two benchmark datasets, we conducted experiments to compare seven topic models along the proposed metrics. The discussion highlights the differences between the models and their relative suitability for various applications. It notes the relationship between evaluation metrics and proposes four key aspects to help decide which model to use for an application. Our discussion also shows that the research trends move towards developing and tuning neural topic models and leveraging the power of pre-trained language models. Finally, it highlights research gaps in developing unified benchmarks and evaluation metrics.},
  keywords = {Evaluation,LDA,Neural,Probabilistic,Topic modeling},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Topic modeling algorithms and applications_2023_Abdelrazek et al.pdf}
}

@online{TradingGraphNeural_2025_Wu,
  title = {Trading {{Graph Neural Network}}},
  author = {Wu, Xian},
  date = {2025-04-10},
  eprint = {2504.07923},
  eprinttype = {arXiv},
  eprintclass = {q-fin},
  doi = {10.48550/arXiv.2504.07923},
  url = {http://arxiv.org/abs/2504.07923},
  urldate = {2025-06-24},
  abstract = {This paper proposes a new algorithm -- Trading Graph Neural Network (TGNN) that can structurally estimate the impact of asset features, dealer features and relationship features on asset prices in trading networks. It combines the strength of the traditional simulated method of moments (SMM) and recent machine learning techniques -- Graph Neural Network (GNN). It outperforms existing reduced-form methods with network centrality measures in prediction accuracy. The method can be used on networks with any structure, allowing for heterogeneity among both traders and assets.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Economics - General Economics,Quantitative Finance - Economics,Quantitative Finance - Pricing of Securities,Quantitative Finance - Trading and Market Microstructure},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Trading Graph Neural Network_2025_Wu.pdf;/home/baldoinov/Zotero/storage/VPC57LB8/2504.html}
}

@online{TrainingServingMachine_2022_BaresiQuattrocchi,
  title = {Training and {{Serving Machine Learning Models}} at {{Scale}}},
  author = {Baresi, Luciano and Quattrocchi, Giovanni},
  date = {2022-11-10},
  eprint = {2211.05516},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2211.05516},
  urldate = {2024-09-17},
  abstract = {In recent years, Web services are becoming more and more intelligent (e.g., in understanding user preferences) thanks to the integration of components that rely on Machine Learning (ML). Before users can interact (inference phase) with an ML-based service (ML-Service), the underlying ML model must learn (training phase) from existing data, a process that requires long-lasting batch computations. The management of these two, diverse phases is complex and meeting time and quality requirements can hardly be done with manual approaches.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Software Engineering},
  file = {/home/baldoinov/Zotero/storage/6RMTMMD2/Baresi and Quattrocchi - 2022 - Training and Serving Machine Learning Models at Sc.pdf}
}

@online{TransformersScratch__Rohrer,
  title = {Transformers from {{Scratch}}},
  author = {Rohrer, Brandon},
  url = {https://e2eml.school/transformers.html},
  urldate = {2023-09-21},
  keywords = {notion},
  file = {/home/baldoinov/Zotero/storage/UMKPA8B8/transformers.html}
}

@book{TruthfulArtData_2016_Cairo,
  title = {The Truthful Art: Data, Charts, and Maps for Communication},
  shorttitle = {Truthful Art, The},
  author = {Cairo, Alberto},
  date = {2016},
  edition = {1ª edição},
  publisher = {New Riders},
  location = {Place of publication not identified},
  abstract = {No matter what your actual job title, you are―or soon will be―a data worker.Every day, at work, home, and school, we are bombarded with vast amounts of free data collected and shared by everyone and everything from our co-workers to our calorie counters. In this highly anticipated follow-up to The Functional Art―Alberto Cairo's foundational guide to understanding information graphics and visualization―the respected data visualization professor explains in clear terms how to work with data, discover the stories hidden within, and share those stories with the world in the form of charts, maps, and infographics. In The Truthful Art, Cairo transforms elementary principles of data and scientific reasoning into tools that you can use in daily life to interpret data sets and extract stories from them. The Truthful Art explains:• The role infographics and data visualization play in our world• Basic principles of data and scientific reasoning that anyone can master• How to become a better critical thinker• Step-by-step processes that will help you evaluate any data visualization (including your own)• How to create and use effective charts, graphs, and data maps to explain data to any audienceThe Truthful Art is also packed with inspirational and educational real-world examples of data visualizations from such leading publications as The New York Times, The Wall Street Journal, Estado de São Paulo (Brazil), Berliner Morgenpost (Germany), and many more.},
  isbn = {978-0-321-93407-9},
  langid = {Inglês}
}

@book{TruthLinearRegression_2024_Shalizi,
  title = {The {{Truth About Linear Regression}}},
  author = {Shalizi, Cosma Rohilla},
  date = {2024},
  langid = {english},
  file = {/home/baldoinov/baldoinov/PDFs/Livros/The Truth about Linear Regression_2024_Shalizi.pdf}
}

@online{TuneTransformersUsing_2023_JacobParnell,
  title = {Tune {{Transformers}} Using {{PyTorch Lightning}} and {{HuggingFace}} | by {{Jacob Parnell}} - {{Freedium}}},
  author = {{Jacob Parnell}},
  date = {2023-01-08},
  url = {https://medium.com/@jacobparnell/tune-transformers-using-pytorch-lightning-and-huggingface-f056373ff0e3},
  urldate = {2024-07-26},
  file = {/home/baldoinov/Zotero/storage/P4UXGVD8/tune-transformers-using-pytorch-lightning-and-huggingface-f056373ff0e3.html}
}

@article{TutorialSupportVector_1998_Burges,
  title = {A {{Tutorial}} on {{Support Vector Machines}} for {{Pattern Recognition}}},
  author = {Burges, Christopher J.C.},
  date = {1998-06-01},
  journaltitle = {Data Mining and Knowledge Discovery},
  shortjournal = {Data Mining and Knowledge Discovery},
  volume = {2},
  number = {2},
  pages = {121--167},
  issn = {1573-756X},
  doi = {10.1023/A:1009715923555},
  url = {https://doi.org/10.1023/A:1009715923555},
  urldate = {2024-02-26},
  abstract = {The tutorial starts with an overview of the concepts of VC dimension and structural risk minimization. We then describe linear Support Vector Machines (SVMs) for separable and non-separable data, working through a non-trivial example in detail. We describe a mechanical analogy, and discuss when SVM solutions are unique and when they are global. We describe how support vector training can be practically implemented, and discuss in detail the kernel mapping technique which is used to construct SVM solutions which are nonlinear in the data. We show how Support Vector machines can have very large (even infinite) VC dimension by computing the VC dimension for homogeneous polynomial and Gaussian radial basis function kernels. While very high VC dimension would normally bode ill for generalization performance, and while at present there exists no theory which shows that good generalization performance is guaranteed for SVMs, there are several arguments which support the observed high accuracy of SVMs, which we review. Results of some experiments which were inspired by these arguments are also presented. We give numerous examples and proofs of most of the key theorems. There is new material, and I hope that the reader will find that even old material is cast in a fresh light.},
  langid = {english},
  keywords = {notion,pattern recognition,statistical learning theory,support vector machines,VC dimension},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/machine-learning-w-pytorch-n-sklearn/A Tutorial on Support Vector Machines for Pattern Recognition_1998_Burges.pdf}
}

@inproceedings{Tweet_Eleicoes_2022UmDataset_2024_SilvaEtAl,
  title = {Tweet\_Eleições\_2022: Um dataset de tweets durante as eleições presidenciais brasileiras de 2022},
  shorttitle = {Tweet\_Eleições\_2022},
  booktitle = {Brazilian Workshop on Social Network Analysis and Mining (BraSNAM)},
  author = {family=Silva, given=Luciano José, prefix=da, useprefix=false and family=Santos, given=Livia A., prefix=dos, useprefix=false and Araujo, Renata and Coelho, Orlando B. and Correa, Ana Grasielle D. and Oliveira, Ivan Carlos A.},
  date = {2024-07-21},
  pages = {193--199},
  publisher = {SBC},
  issn = {2595-6094},
  doi = {10.5753/brasnam.2024.1940},
  url = {https://sol.sbc.org.br/index.php/brasnam/article/view/29343},
  urldate = {2024-08-16},
  abstract = {In this paper we present Tweet\_Eleições\_2022, a dataset containing tweets related to political episodes that occurred during the 2022 Brazilian presidential elections. The dataset is mainly applicable to research that is interested in data from this Brazilian historical period, and has significant value\&nbsp;for research that uses social network data in Brazilian Portuguese.},
  eventtitle = {Brazilian Workshop on Social Network Analysis and Mining (BraSNAM)},
  langid = {portuguese},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/projeto-heiwa/Tweet_Eleicoes_2022_2024_Silva et al.pdf}
}

@article{TweetiesSquabblingPositive_2016_BoscEtAl,
  title = {Tweeties {{Squabbling}}: {{Positive}} and {{Negative Results}} in {{Applying Argument Mining}} on {{Social Media}}},
  shorttitle = {Tweeties {{Squabbling}}},
  author = {Bosc, T. and Cabrio, E. and Villata, S.},
  date = {2016},
  journaltitle = {Frontiers in Artificial Intelligence and Applications},
  volume = {287},
  pages = {21--32},
  issn = {0922-6389},
  doi = {10.3233/978-1-61499-686-6-21},
  abstract = {The problem of understanding the stream of messages exchanged on social media such as Facebook and Twitter is becoming a major challenge for automated systems. The tremendous amount of data exchanged on these platforms as well as the specific form of language adopted by social media users constitute a new challenging context for existing argument mining techniques. In this paper, we describe an ongoing work towards the creation of a complete argument mining pipeline over Twitter messages: (i) we identify which tweets can be considered as arguments and which cannot, (ii) over the set of tweet-arguments, we group them by topic, and (iii) we predict whether such tweets support or attack each other. The final goal is to compute the set of tweets which are widely recognized as accepted, and the different (possibly conflicting) viewpoints that emerge on a topic, given a stream of messages. © 2016 The authors and IOS Press.},
  isbn = {9781614996859},
  langid = {english},
  keywords = {✔️,Argument mining,notion,Social media,Supervised classification approaches},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/projeto-heiwa-revisao-de-literatura/Tweeties Squabbling_2016_Bosc et al.pdf}
}

@online{TwitterAnnouncesNew_2023_Porter,
  title = {Twitter Announces New {{API}} Pricing, Posing a Challenge for Small Developers},
  author = {Porter, Jon},
  date = {2023-03-30T09:35:58},
  url = {https://www.theverge.com/2023/3/30/23662832/twitter-api-tiers-free-bot-novelty-accounts-basic-enterprice-monthly-price},
  urldate = {2024-02-27},
  abstract = {Free, basic, and enterprise tiers are offered.},
  langid = {english},
  organization = {The Verge},
  keywords = {notion},
  file = {/home/baldoinov/Zotero/storage/EMGRQLBJ/twitter-api-tiers-free-bot-novelty-accounts-basic-enterprice-monthly-price.html}
}

@article{TwitterSentimentAnalysis_2011_KouloumpisEtAl,
  title = {Twitter {{Sentiment Analysis}}: {{The Good}} the {{Bad}} and the {{OMG}}!},
  shorttitle = {Twitter {{Sentiment Analysis}}},
  author = {Kouloumpis, Efthymios and Wilson, Theresa and Moore, Johanna},
  date = {2011},
  journaltitle = {Proceedings of the International AAAI Conference on Web and Social Media},
  volume = {5},
  number = {1},
  pages = {538--541},
  issn = {2334-0770},
  doi = {10.1609/icwsm.v5i1.14185},
  url = {https://ojs.aaai.org/index.php/ICWSM/article/view/14185},
  urldate = {2023-10-04},
  abstract = {In this paper, we investigate the utility of linguistic features for detecting the sentiment of Twitter messages.  We evaluate the usefulness of existing lexical resources as well as features that capture information about the informal and creative language used in microblogging.  We take a supervied approach to the problem, but leverage existing hashtags in the Twitter data for building training data.},
  issue = {1},
  langid = {english},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/projeto-heiwa/Twitter Sentiment Analysis_2011_Kouloumpis et al.pdf}
}

@inproceedings{TwitterSentimentAnalysis_2015_ShukriEtAl,
  title = {Twitter Sentiment Analysis: {{A}} Case Study in the Automotive Industry},
  shorttitle = {Twitter Sentiment Analysis},
  booktitle = {2015 {{IEEE Jordan Conference}} on {{Applied Electrical Engineering}} and {{Computing Technologies}} ({{AEECT}})},
  author = {Shukri, Sarah E. and Yaghi, Rawan I. and Aljarah, Ibrahim and Alsawalqah, Hamad},
  date = {2015-11},
  pages = {1--5},
  doi = {10.1109/AEECT.2015.7360594},
  url = {https://ieeexplore.ieee.org/abstract/document/7360594},
  urldate = {2024-02-27},
  abstract = {Sentiment analysis is one of the fastest growing areas which uses the natural language processing, text mining and computational linguistic to extract useful information to help in the decision making process. In the recent years, social media websites have been spreading widely, and their users are increasing rapidly. Automotive industry is one of the largest economic sectors in the world with more than 90 million cars and vehicles. Automotive industry is highly competitive and requires that sellers, automotive companies, carefully analyze and attend to consumers' opinions in order to achieve a competitive advantage in the market. Analysing consumers' opinions using social media data can be very great way for the automotive companies to enhance their marketing targets and objectives. In this paper, a sentiment analyses on a case study in the automotive industry is presented. Text mining and sentiment analysis are used to analyze unstructured tweets on Twitter to extract the polarity, and emotions classification towards the automotive classes such as Mercedes, Audi and BMW. We can note from the emotions classification results that, “joy” category is better for BMW comparing to Mercedes and Audi, The “sadness” percentage is larger for Audi and Mercedes comparing to BMW. Furthermore, we can note from the polarity classification that BMW has 72\% positive tweets compared 79\% for Mercedes and 83\% for Audi. In addition, the results show that BMW has 8\% negative polarity compared 18\% for Mercedes and 16\% for Audi.},
  eventtitle = {2015 {{IEEE Jordan Conference}} on {{Applied Electrical Engineering}} and {{Computing Technologies}} ({{AEECT}})},
  keywords = {Automotive,Automotive engineering,Classification,Companies,Industries,Media,Sentiment analysis,Sentiment Analysis,Text mining,Twitter},
  file = {/home/baldoinov/Zotero/storage/RA4INRAX/7360594.html}
}

@online{TwitterSentimentAnalysis_2015_SouzaEtAl,
  title = {Twitter {{Sentiment Analysis Applied}} to {{Finance}}: {{A Case Study}} in the {{Retail Industry}}},
  shorttitle = {Twitter {{Sentiment Analysis Applied}} to {{Finance}}},
  author = {Souza, Thársis Tuani Pinto and Kolchyna, Olga and Treleaven, Philip C. and Aste, Tomaso},
  date = {2015-07-11},
  eprint = {1507.00784},
  eprinttype = {arXiv},
  eprintclass = {cs, q-fin},
  doi = {10.48550/arXiv.1507.00784},
  url = {http://arxiv.org/abs/1507.00784},
  urldate = {2024-02-27},
  abstract = {This paper presents a financial analysis over Twitter sentiment analytics extracted from listed retail brands. We investigate whether there is statistically-significant information between the Twitter sentiment and volume, and stock returns and volatility. Traditional newswires are also considered as a proxy for the market sentiment for comparative purpose. The results suggest that social media is indeed a valuable source in the analysis of the financial dynamics in the retail sector even when compared to mainstream news such as the Wall Street Journal and Dow Jones Newswires.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computers and Society,Computer Science - Social and Information Networks,Quantitative Finance - Computational Finance},
  file = {/home/baldoinov/baldoinov/PDFs/Mackenzie/projeto-aplicado-ii/Twitter Sentiment Analysis Applied to Finance_2015_Souza et al.pdf;/home/baldoinov/Zotero/storage/VR6IRSZY/1507.html}
}

@article{TwitterSentimentClassification_2009_GoEtAl,
  title = {Twitter Sentiment Classification Using Distant Supervision},
  author = {Go, Alec and Bhayani, Richa and Huang, Lei},
  date = {2009},
  journaltitle = {CS224N project report, Stanford},
  volume = {1},
  number = {12},
  pages = {2009},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/projeto-heiwa/Twitter sentiment classification using distant supervision_2009_Go et al.pdf}
}

@online{UberEngineeringsIncremental_2017_PrasannaRajaperumal,
  title = {Uber {{Engineering}}’s {{Incremental Processing Framework}} on {{Hadoop}}},
  author = {{Prasanna Rajaperumal}},
  date = {2017-03-12},
  url = {https://www.uber.com/en-BR/blog/hoodie/},
  urldate = {2025-03-09},
  file = {/home/baldoinov/Zotero/storage/XFXXQVX4/hoodie.html}
}

@online{UbersBigData_2018_Uber,
  title = {Uber’s {{Big Data Platform}}: 100+ {{Petabytes}} with {{Minute Latency}}},
  shorttitle = {Uber’s {{Big Data Platform}}},
  author = {{Uber}},
  date = {2018-10-17T09:00:08+00:00},
  url = {https://www.uber.com/en-IN/blog/uber-big-data-platform/},
  urldate = {2025-03-09},
  abstract = {Responsible for cleaning, storing, and serving over 100 petabytes of analytical data, Uber's Hadoop platform ensures data reliability, scalability, and ease-of-use with minimal latency.},
  langid = {english},
  organization = {Uber Blog},
  file = {/home/baldoinov/Zotero/storage/FR6LAX99/uber-big-data-platform.html}
}

@inproceedings{UlyssesSDBrStanceDetection_2022_MaiaEtAl,
  title = {{{UlyssesSD-Br}}: {{Stance Detection}} in~{{Brazilian Political Polls}}},
  shorttitle = {{{UlyssesSD-Br}}},
  booktitle = {Progress in {{Artificial Intelligence}}},
  author = {Maia, Dyonnatan F. and Silva, Nádia F. F. and Souza, Ellen P. R. and Nunes, Augusto S. and Procópio, Lucas C. and Sampaio, Guthemberg da S. and Dias, Márcio de S. and Alves, Adrio O. and Maia, Dyéssica F. and Ribeiro, Ingrid A. and Pereira, Fabíola S. F. and family=Carvalho, given=André P. de L. F., prefix=de, useprefix=true},
  editor = {Marreiros, Goreti and Martins, Bruno and Paiva, Ana and Ribeiro, Bernardete and Sardinha, Alberto},
  date = {2022},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {85--95},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-031-16474-3_8},
  abstract = {Political bill comments published in digital media may reveal the issuer’s stances. Through this, we can identify and group the polarity of these public opinions. The automatic stance detection task involves viewing the text and the target topic. Due to the diversity and emergence of new bills, the challenge approached is to estimate the polarity of a new topic. Thus, this paper evaluates cross-target stance detection with many-to-one approaches in a collected Portuguese dataset of the political pool from the Brazilian Chamber of Deputies website. We proposed a new corpus for the bills’ opinion domain and tested it in several models, where we achieved the best result with the mBERT model in classification with the joint input topic and comment method. We verify that the mBERT model successfully handled cross-target tasks with this corpus among the tested algorithms.},
  isbn = {978-3-031-16474-3},
  langid = {english},
  keywords = {Cross-target,notion,Political comments,Stance detection},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/projeto-heiwa/UlyssesSD-Br_2022_Maia et al.pdf}
}

@book{UmCursoCalculo_2012_Guidorizzi,
  title = {Um Curso de Cálculo Volume II},
  author = {Guidorizzi, Hamilton Luiz},
  date = {2012-09-28},
  publisher = {Ltc-Livros Tecnicos E Cientificos Editora Lda},
  isbn = {978-85-216-2245-1},
  langid = {brazilian}
}

@book{UmCursoCalculo_2012_Guidorizzia,
  title = {Um Curso de Cálculo Volume IV},
  author = {Guidorizzi, Hamilton Luiz},
  date = {2012-09-28},
  publisher = {Ltc-Livros Tecnicos E Cientificos Editora Lda},
  isbn = {978-85-216-2247-5},
  langid = {brazilian}
}

@book{UmCursoCalculo_2012_Guidorizzib,
  title = {Um Curso de Cálculo Volume III},
  author = {Guidorizzi, Hamilton Luiz},
  date = {2012-09-28},
  publisher = {Ltc-Livros Tecnicos E Cientificos Editora Lda},
  isbn = {978-85-216-2246-8},
  langid = {brazilian}
}

@book{UmCursoCalculo_2012_Guidorizzic,
  title = {Um Curso de Cálculo Volume I},
  author = {Guidorizzi, Hamilton Luiz},
  date = {2012-09-28},
  publisher = {Ltc-Livros Tecnicos E Cientificos Editora Lda},
  isbn = {978-85-216-2244-4},
  langid = {brazilian}
}

@inproceedings{UmPipelinePreProcessamento_2024_SantosEtAl,
  title = {Um Pipeline de Pré-Processamento de Dados Textuais em Português para Análise de Redes Sociais},
  booktitle = {Simpósio Brasileiro de Tecnologia da Informação e da Linguagem Humana (STIL)},
  author = {family=Santos, given=Livia A., prefix=dos, useprefix=false and Coelho, Orlando B. and Araujo, Renata and Oliveira, Ivan Carlos A.},
  date = {2024-11-17},
  pages = {463--468},
  publisher = {SBC},
  issn = {0000-0000},
  doi = {10.5753/stil.2024.245373},
  url = {https://sol.sbc.org.br/index.php/stil/article/view/31163},
  urldate = {2024-11-21},
  abstract = {Preprocessing is a fundamental step in processing textual data, especially when working with text analysis, data mining or machine learning. In particular, textual data from social networks offers challenges to pre-processing, mainly due to its informal structure. This article presents a pipeline to perform 9 basic processing activities to guarantee the quality and consistency of brazilian Portuguese textual data sets extracted from social networks. Tests were conducted on datasets containing 8,000, 20,000, and 60,000 tweets, demonstrating the pipeline's performance in terms of accuracy, noise reduction, and processing time.},
  eventtitle = {Simpósio Brasileiro de Tecnologia da Informação e da Linguagem Humana (STIL)},
  langid = {portuguese},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/projeto-heiwa/Um Pipeline de Pre-Processamento de Dados Textuais em Portugues para Analise de_2024_Santos et al.pdf}
}

@online{UnderstandingAffinityPropagation_2022_Yufeng,
  title = {Understanding {{Affinity Propagation Clustering And Implementation}} with {{Python}}},
  author = {{Yufeng}},
  date = {2022-09-02T16:08:26},
  url = {https://towardsdatascience.com/understanding-affinity-propagation-clustering-and-implementation-with-python-c1e78fe57cde},
  urldate = {2023-06-08},
  abstract = {One of the most used clustering methods, affinity propagation, is clearly explained together with the implementation with Python},
  langid = {english},
  organization = {Medium},
  keywords = {notion},
  file = {/home/baldoinov/Zotero/storage/IJ4VKJB3/understanding-affinity-propagation-clustering-and-implementation-with-python-c1e78fe57cde.html}
}

@article{UnderstandingConvolutionsGraphs_2021_DaigavaneEtAl,
  title = {Understanding {{Convolutions}} on {{Graphs}}},
  author = {Daigavane, Ameya and Ravindran, Balaraman and Aggarwal, Gaurav},
  date = {2021-09-02},
  journaltitle = {Distill},
  shortjournal = {Distill},
  volume = {6},
  number = {9},
  pages = {e32},
  issn = {2476-0757},
  doi = {10.23915/distill.00032},
  url = {https://distill.pub/2021/understanding-gnns},
  urldate = {2023-06-02},
  abstract = {Understanding the building blocks and design choices of graph neural networks.},
  langid = {english},
  keywords = {notion},
  file = {/home/baldoinov/Zotero/storage/RU9WIVRW/understanding-gnns.html}
}

@article{UnderstandingInteractionModels_2006_BramborEtAl,
  title = {Understanding {{Interaction Models}}: {{Improving Empirical Analyses}}},
  shorttitle = {Understanding {{Interaction Models}}},
  author = {Brambor, Thomas and Clark, William Roberts and Golder, Matt},
  date = {2006-01},
  journaltitle = {Political Analysis},
  volume = {14},
  number = {1},
  pages = {63--82},
  issn = {1047-1987, 1476-4989},
  doi = {10.1093/pan/mpi014},
  url = {https://www.cambridge.org/core/journals/political-analysis/article/abs/understanding-interaction-models-improving-empirical-analyses/9BA57B3720A303C61EBEC6DDFA40744B},
  urldate = {2025-04-21},
  abstract = {Multiplicative interaction models are common in the quantitative political science literature. This is so for good reason. Institutional arguments frequently imply that the relationship between political inputs and outcomes varies depending on the institutional context. Models of strategic interaction typically produce conditional hypotheses as well. Although conditional hypotheses are ubiquitous in political science and multiplicative interaction models have been found to capture their intuition quite well, a survey of the top three political science journals from 1998 to 2002 suggests that the execution of these models is often flawed and inferential errors are common. We believe that considerable progress in our understanding of the political world can occur if scholars follow the simple checklist of dos and don'ts for using multiplicative interaction models presented in this article. Only 10\% of the articles in our survey followed the checklist.},
  langid = {english}
}

@online{UnderstandingLLMsComprehensive_2024_LiuEtAl,
  title = {Understanding {{LLMs}}: {{A Comprehensive Overview}} from {{Training}} to {{Inference}}},
  shorttitle = {Understanding {{LLMs}}},
  author = {Liu, Yiheng and He, Hao and Han, Tianle and Zhang, Xu and Liu, Mengyuan and Tian, Jiaming and Zhang, Yutong and Wang, Jiaqi and Gao, Xiaohui and Zhong, Tianyang and Pan, Yi and Xu, Shaochen and Wu, Zihao and Liu, Zhengliang and Zhang, Xin and Zhang, Shu and Hu, Xintao and Zhang, Tuo and Qiang, Ning and Liu, Tianming and Ge, Bao},
  date = {2024-01-04},
  url = {https://arxiv.org/abs/2401.02038v1},
  urldate = {2024-01-08},
  abstract = {The introduction of ChatGPT has led to a significant increase in the utilization of Large Language Models (LLMs) for addressing downstream tasks. There's an increasing focus on cost-efficient training and deployment within this context. Low-cost training and deployment of LLMs represent the future development trend. This paper reviews the evolution of large language model training techniques and inference deployment technologies aligned with this emerging trend. The discussion on training includes various aspects, including data preprocessing, training architecture, pre-training tasks, parallel training, and relevant content related to model fine-tuning. On the inference side, the paper covers topics such as model compression, parallel computation, memory scheduling, and structural optimization. It also explores LLMs' utilization and provides insights into their future development.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Understanding LLMs_2024_Liu et al.pdf}
}

@online{UnderstandingRetrievalAugmentation_2023_ChenEtAl,
  title = {Understanding {{Retrieval Augmentation}} for {{Long-Form Question Answering}}},
  author = {Chen, Hung-Ting and Xu, Fangyuan and Arora, Shane A. and Choi, Eunsol},
  date = {2023-10-18},
  eprint = {2310.12150},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2310.12150},
  url = {http://arxiv.org/abs/2310.12150},
  urldate = {2023-10-19},
  abstract = {We present a study of retrieval-augmented language models (LMs) on long-form question answering. We analyze how retrieval augmentation impacts different LMs, by comparing answers generated from models while using the same evidence documents, and how differing quality of retrieval document set impacts the answers generated from the same LM. We study various attributes of generated answers (e.g., fluency, length, variance) with an emphasis on the attribution of generated long-form answers to in-context evidence documents. We collect human annotations of answer attribution and evaluate methods for automatically judging attribution. Our study provides new insights on how retrieval augmentation impacts long, knowledge-rich text generation of LMs. We further identify attribution patterns for long text generation and analyze the main culprits of attribution errors. Together, our analysis reveals how retrieval augmentation impacts long knowledge-rich text generation and provide directions for future work.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,notion},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Understanding Retrieval Augmentation for Long-Form Question Answering_2023_Chen et al.pdf;/home/baldoinov/Zotero/storage/5BUKZVQB/2310.html}
}

@article{unep_2022,
  title = {Annual Report 2022},
  author = {{UNEP}},
  date = {2022}
}

@inproceedings{UnifiedApproachInterpreting_2017_LundbergLee,
  title = {A {{Unified Approach}} to {{Interpreting Model Predictions}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Lundberg, Scott M and Lee, Su-In},
  date = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html},
  urldate = {2024-10-21},
  abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Recovery/Modelo de Incidencia de Acao Contra/A Unified Approach to Interpreting Model Predictions_2017_Lundberg et al.pdf}
}

@book{UnitedNationsTerrorism_2014_Blumenau,
  title = {The {{United Nations}} and {{Terrorism}}},
  author = {Blumenau, Bernhard},
  date = {2014},
  publisher = {Palgrave Macmillan UK},
  location = {London},
  doi = {10.1057/9781137391988},
  url = {http://link.springer.com/10.1057/9781137391988},
  urldate = {2024-05-29},
  isbn = {978-1-349-48315-0 978-1-137-39198-8},
  langid = {english}
}

@online{UnsupervisedCrosslingualRepresentation_2020_ConneauEtAl,
  title = {Unsupervised {{Cross-lingual Representation Learning}} at {{Scale}}},
  author = {Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzmán, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin},
  date = {2020-04-07},
  eprint = {1911.02116},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1911.02116},
  url = {http://arxiv.org/abs/1911.02116},
  urldate = {2023-10-03},
  abstract = {This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6\% average accuracy on XNLI, +13\% average F1 score on MLQA, and +2.4\% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7\% in XNLI accuracy for Swahili and 11.4\% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code, data and models publicly available.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,notion},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/projeto-heiwa/Unsupervised Cross-lingual Representation Learning at Scale_2020_Conneau et al.pdf;/home/baldoinov/Zotero/storage/CNG9BKEC/1911.html}
}

@online{UnsupervisedLearningData_2017_Mishra,
  title = {Unsupervised {{Learning}} and {{Data Clustering}}},
  author = {Mishra, Sanatan},
  date = {2017-05-21T01:00:34},
  url = {https://towardsdatascience.com/unsupervised-learning-and-data-clustering-eeecb78b422a},
  urldate = {2023-06-08},
  abstract = {A task involving machine learning may not be linear, but it has a number of well known steps:},
  langid = {english},
  organization = {Medium},
  keywords = {notion},
  file = {/home/baldoinov/Zotero/storage/5ARXJ6LQ/unsupervised-learning-and-data-clustering-eeecb78b422a.html}
}

@online{UpliftModelingPredict_2023_Mosca,
  title = {Uplift {{Modeling}}: Predict the Causal Effect of Marketing Communications},
  shorttitle = {Uplift {{Modeling}}},
  author = {Mosca, Andrea},
  date = {2023-11-30T16:17:22},
  url = {https://medium.com/data-reply-it-datatech/uplift-modeling-predict-the-causal-effect-of-marketing-communications-24385fb04f2e},
  urldate = {2025-08-17},
  abstract = {A complex task in marketing analytics is the skill to predict customers’ reactions to specific marketing communications.},
  langid = {english},
  organization = {Data Reply IT | DataTech}
}

@article{UsingDeepQlearning_2018_GoumagiasEtAl,
  title = {Using Deep {{Q-learning}} to Understand the Tax Evasion Behavior of Risk-Averse Firms},
  author = {Goumagias, Nikolaos D. and Hristu-Varsakelis, Dimitrios and Assael, Yannis M.},
  date = {2018-07-01},
  journaltitle = {Expert Systems with Applications},
  shortjournal = {Expert Systems with Applications},
  volume = {101},
  pages = {258--270},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2018.01.039},
  url = {https://www.sciencedirect.com/science/article/pii/S0957417418300538},
  urldate = {2024-06-25},
  abstract = {Designing tax policies that are effective in curbing tax evasion and maximize state revenues requires a rigorous understanding of taxpayer behavior. This work explores the problem of determining the strategy a self-interested, risk-averse tax entity is expected to follow, as it “navigates” – in the context of a Markov Decision Process – a government-controlled tax environment that includes random audits, penalties and occasional tax amnesties. Although simplified versions of this problem have been previously explored, the mere assumption of risk-aversion (as opposed to risk-neutrality) raises the complexity of finding the optimal policy well beyond the reach of analytical techniques. Here, we obtain approximate solutions via a combination of Q-learning and recent advances in Deep Reinforcement Learning. By doing so, we (i) determine the tax evasion behavior expected of the taxpayer entity, (ii) calculate the degree of risk aversion of the “average” entity given empirical estimates of tax evasion, and (iii) evaluate sample tax policies, in terms of expected revenues. Our model can be useful as a testbed for “in-vitro” testing of tax policies, while our results lead to various policy recommendations.},
  keywords = {Deep learning,Markov decision processes,Q-learning,Tax evasion},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Using deep Q-learning to understand the tax evasion behavior of risk-averse_2018_Goumagias et al.pdf;/home/baldoinov/Zotero/storage/IFB2AVKL/S0957417418300538.html}
}

@book{UsingIntroductoryEconometrics_2020_Heiss,
  title = {Using {{R}} for {{Introductory Econometrics}}},
  author = {Heiss, Florian},
  date = {2020-05-24},
  publisher = {Independently published},
  location = {Düsseldorf},
  abstract = {Introduces the popular, powerful and free programming language and software package RFocus: implementation of standard tools and methods used in econometricsCompatible with "Introductory Econometrics" by Jeffrey M. Wooldridge in terms of topics, organization, terminology and notationCompanion website with full text, all code for download and other goodies: http://urfie.netAlso check out Using Python for Introductory Econometrics: http://upfie.net/Praise: "A very nice resource for those wanting to use R in their introductory econometrics courses." (Jeffrey M. Wooldridge)Using R for Introductory Econometrics is a fabulous modern resource. I know I'm going to be using it with my students, and I recommend it to anyone who wants to learn about econometrics and R at the same time." (David E. Giles in his blog "Econometrics Beat")Topics:A gentle introduction to RSimple and multiple regression in matrix form and using black box routinesInference in small samples and asymptoticsMonte Carlo simulationsHeteroscedasticityTime series regressionPooled cross-sections and panel dataInstrumental variables and two-stage least squaresSimultaneous equation modelsLimited dependent variables: binary, count data, censoring, truncation, and sample selectionFormatted reports and research papers combining R with R Markdown or LaTeX},
  isbn = {9798648424364},
  langid = {english},
  pagetotal = {378},
  file = {/home/baldoinov/baldoinov/PDFs/Livros/Using R for Introductory Econometrics_2020_Heiss.pdf}
}

@article{UsingLLMLarge__AntuEtAl,
  title = {Using {{LLM}} ({{Large Language Model}}) to {{Improve Efficiency}} in {{Literature Review}} for {{Undergraduate Research}}},
  author = {Antu, Shouvik Ahmed and Chen, Haiyan and Richards, Cindy K},
  abstract = {The potential of artificial intelligence (AI) to streamline and improve the research process for academics is becoming increasingly evident as this technology develops. A promising avenue for conducting literature reviews is to employ artificial intelligence (AI). Using OpenAI's ChatGPT, this paper explores the utility of this tool in the context of academic literature reviews. Our study focuses on how ChatGPT can be used to support the literature review process for undergraduate students conducting research for their capstone courses. Furthermore, we will explore the possible drawbacks and limitations of relying on artificial intelligence to perform such research tasks. The aim is to provide a balanced and comprehensive view of the role and future potential of AI, and specifically ChatGPT, in literature review studies. In addition to evaluating the accuracy and relevance of the results generated by ChatGPT, we will also examine the quality of results generated by ChatGPT.},
  langid = {english},
  file = {/home/baldoinov/Zotero/storage/U96KMVSY/Antu et al. - Using LLM (Large Language Model) to Improve Effici.pdf}
}

@article{UsingMachineLearning_2023_AleEbrahimDehkordiEtAl,
  title = {Using {{Machine Learning}} for {{Agent Specifications}} in {{ABM}}:   {{A Critical Review}} and {{Guidelines}}},
  shorttitle = {Using {{Machine Learning}} for {{Agent Specifications}} in {{ABM}}},
  author = {Ale Ebrahim Dehkordi, Molood and Lechner, Jonas and Ghorbani, Amineh and Nikolic, Igor and Chappin, Émile and Herder, Paulien},
  date = {2023},
  journaltitle = {Journal of Artificial Societies and Social Simulation},
  shortjournal = {JASSS},
  volume = {26},
  number = {1},
  pages = {9},
  issn = {1460-7425},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Using Machine Learning for Agent Specifications in ABM_2023_Ale Ebrahim Dehkordi et al.pdf;/home/baldoinov/Zotero/storage/F99MMSD9/9.html}
}

@book{UsingPythonIntroductory_2020_HeissBrunner,
  title = {Using {{Python}} for {{Introductory Econometrics}}},
  author = {Heiss, Florian and Brunner, Daniel},
  date = {2020-05-25},
  publisher = {Independently published},
  location = {New York},
  abstract = {Introduces the popular, powerful and free programming language and software package PythonFocus: implementation of standard tools and methods used in econometricsCompatible with "Introductory Econometrics" by Jeffrey M. Wooldridge in terms of topics, organization, terminology and notationCompanion website with full text, all code for download and other goodiesTopics:A gentle introduction to PythonSimple and multiple regression in matrix form and using black box routinesInference in small samples and asymptoticsMonte Carlo simulationsHeteroscedasticityTime series regressionPooled cross-sections and panel dataInstrumental variables and two-stage least squaresSimultaneous equation modelsLimited dependent variables: binary, count data, censoring, truncation, and sample selectionFormatted reports using Jupyter Notebooks},
  isbn = {9798648436763},
  langid = {english},
  pagetotal = {428},
  file = {/home/baldoinov/baldoinov/PDFs/Livros/Using Python for Introductory Econometrics_2020_Heiss et al.pdf}
}

@article{USMortalityEconomic_1995_SorlieEtAl,
  title = {{{US}} Mortality by Economic, Demographic, and Social Characteristics: The {{National Longitudinal Mortality Study}}.},
  shorttitle = {{{US}} Mortality by Economic, Demographic, and Social Characteristics},
  author = {Sorlie, P D and Backlund, E and Keller, J B},
  date = {1995-07},
  journaltitle = {American Journal of Public Health},
  shortjournal = {Am J Public Health},
  volume = {85},
  number = {7},
  pages = {949--956},
  publisher = {American Public Health Association},
  issn = {0090-0036},
  doi = {10.2105/AJPH.85.7.949},
  url = {https://ajph.aphapublications.org/doi/abs/10.2105/AJPH.85.7.949},
  urldate = {2025-09-07},
  abstract = {OBJECTIVES. A large US sample was used to estimate the effects of race, employment status, income, education, occupation, marital status, and household size on mortality. METHODS. Approximately 530,000 persons 25 years of age or more were identified from selected Current Population Surveys between 1979 and 1985. These individuals were followed for mortality through use of the National Death Index for the years 1979 through 1989. RESULTS. Higher mortality was found in Blacks than in Whites less than 65 years of age; in persons not in the labor force, with lower incomes, with less education, and in service and other lower level occupations; and in persons not married and living alone. With occasional exceptions, in specific sex and age groups, these relationships were reduced but remained strong and statistically significant when each variable was adjusted for all of the other characteristics. The relationships were generally weaker in individuals 65 years of age or more. CONCLUSIONS. Employment status, income, education, occupation, race, and marital status have substantial net associations with mortality. This study identified segments of the population in need of public health attention and demonstrated the importance of including these variables in morbidity and mortality studies.}
}

@inproceedings{ValidationMethodologyAgentbased_2008_Klugl,
  title = {A Validation Methodology for Agent-Based Simulations},
  booktitle = {Proceedings of the 2008 {{ACM}} Symposium on {{Applied}} Computing},
  author = {Klügl, Franziska},
  date = {2008-03-16},
  series = {{{SAC}} '08},
  pages = {39--43},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/1363686.1363696},
  url = {https://dl.acm.org/doi/10.1145/1363686.1363696},
  urldate = {2023-09-19},
  abstract = {Validity forms the basic prerequisite for every simulation model, therefore also for reasonable usage of the agent-based simulation paradigm. However, models based on the multi-agent system metaphor tend to need some particular approaches. In this paper, I propose a process for validating agent-based simulation models that combines face validation, sensitivity analysis, calibration and statistical validation.},
  isbn = {978-1-59593-753-7},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/A validation methodology for agent-based simulations_2008_Klugl.pdf}
}

@article{ValidationVerificationTesting_1994_Balci,
  title = {Validation, Verification, and Testing Techniques throughout the Life Cycle of a Simulation Study},
  author = {Balci, Osman},
  date = {1994-12-01},
  journaltitle = {Annals of Operations Research},
  shortjournal = {Ann Oper Res},
  volume = {53},
  number = {1},
  pages = {121--173},
  issn = {1572-9338},
  doi = {10.1007/BF02136828},
  url = {https://doi.org/10.1007/BF02136828},
  urldate = {2024-06-23},
  abstract = {Life cycle validation, verification, and testing (VV\&T) is extremely important for the success of a simulation study. This paper surveys current software VV\&T techniques and current simulation model VV\&T techniques and describes how they can all be applied throughout the life cycle of a simulation study. The processes and credibility assessment stages of the life cycle are described and the applicability of the VV\&T techniques for each stage is stated. A glossary is provided to explicitly define important terms and VV\&T techniques.},
  langid = {english},
  keywords = {Current Simulation,Life Cycle,Simulation Model,Simulation Study,Testing Technique},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/Validation, verification, and testing techniques throughout the life cycle of a_1994_Balci.pdf}
}

@online{VisualGuideMamba_2023_Grootendorst,
  title = {A {{Visual Guide}} to {{Mamba}} and {{State Space Models}}},
  author = {Grootendorst, Maarten},
  date = {2023-11-13},
  url = {https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mamba-and-state?utm_medium=email},
  urldate = {2024-06-11},
  abstract = {An Alternative to Transformers for Language Modeling},
  langid = {english},
  file = {/home/baldoinov/Zotero/storage/HYPJ3AGQ/a-visual-guide-to-mamba-and-state.html}
}

@article{VisualizingClashesAlliances_2014_TavaresEtAl,
  title = {Visualizing {{Clashes}} and {{Alliances}} in {{Social Networks}} of {{Political Discussions}}},
  author = {Tavares, Rafael Lage and Pimentel, Mariano and family=Araujo, given=Renata Mendes, prefix=de, useprefix=false},
  date = {2014-02-27},
  journaltitle = {Social Networking},
  volume = {3},
  number = {2},
  pages = {94--101},
  publisher = {Scientific Research Publishing},
  doi = {10.4236/sn.2014.32012},
  url = {http://www.scirp.org/Journal/Paperabs.aspx?paperid=43326},
  urldate = {2023-04-19},
  abstract = {Political discussions are characterized by conflicts of interest, and decisions are made based on negotiations. In general, participants need to reinforce their opinions and influence other participants. In this context, it is important to know how allies and opponents are positioned, in order to understand the discussion dynamics and plan adequate actions. This paper suggests the use of social network visualizations to explicit oppositions and alliances in order to support the understanding and following of political discussions. A system which supports these visualizations was built. An experiment performed to test the proposed visualizations showed to which extent they can be more efficient in identifying information about clashes and alliances than an online discussion system can.},
  issue = {2},
  langid = {english},
  keywords = {notion},
  file = {/home/baldoinov/baldoinov/PDFs/Projetos/projeto-heiwa/Visualizing Clashes and Alliances in Social Networks of Political Discussions_2014_Tavares et al.pdf}
}

@article{wang-erlandsson_2022,
  title = {A Planetary Boundary for Green Water},
  author = {{Wang-Erlandsson} and {L.} and Tobian, A. and family=Ent, given=R.J., prefix=van der, useprefix=true and Fetzer, I. and family=Wierik, given=S., prefix=te, useprefix=true and Porkka, M. and Staal, A. and Jaramillo, F. and Dahlmann, H. and Singh, C. and Greve, P.},
  date = {2022},
  journaltitle = {Nature Reviews Earth \& Environment},
  volume = {3},
  number = {6},
  pages = {380--392}
}

@article{WarMilitarySpending_2006_Collier,
  title = {War and Military Spending in Developing Countries and Their Consequences for Development},
  author = {Collier, Paul},
  date = {2006-01-01},
  journaltitle = {The Economics of Peace and Security Journal},
  volume = {1},
  number = {1},
  issn = {1749-852X},
  doi = {10.15355/epsj.1.1.10},
  url = {https://www.epsjournal.org.uk/index.php/EPSJ/article/view/13},
  urldate = {2024-06-25},
  abstract = {That military expenditure and conflict have adverse consequences for development is unsurprising but important. The policy challenge is to reduce them. I have suggested that substantial components of military expenditure could be reduced without jeopardizing security interests. Military expenditure does not appear to be an effective deterrent of rebellion, and, if it is reduced in a coordinated manner across a region then external security interests would be unaffected. The resources released by reduced military expenditure could be used to increase growth rates, and this in turn would gradually but effectively reduce the risk of internal conflict. Development, not military deterrence, is the best strategy for a safer society.},
  issue = {1},
  langid = {english},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/War and military spending in developing countries and their consequences for_2006_Collier.pdf}
}

@online{WarpDriveExtremelyFast_2021_LanEtAl,
  title = {{{WarpDrive}}: {{Extremely Fast End-to-End Deep Multi-Agent Reinforcement Learning}} on a {{GPU}}},
  shorttitle = {{{WarpDrive}}},
  author = {Lan, Tian and Srinivasa, Sunil and Wang, Huan and Zheng, Stephan},
  date = {2021-10-08},
  eprint = {2108.13976},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2108.13976},
  url = {http://arxiv.org/abs/2108.13976},
  urldate = {2024-05-05},
  abstract = {Deep reinforcement learning (RL) is a powerful framework to train decision-making models in complex environments. However, RL can be slow as it requires repeated interaction with a simulation of the environment. In particular, there are key system engineering bottlenecks when using RL in complex environments that feature multiple agents with high-dimensional state, observation, or action spaces. We present WarpDrive, a flexible, lightweight, and easy-to-use open-source RL framework that implements end-to-end deep multi-agent RL on a single GPU (Graphics Processing Unit), built on PyCUDA and PyTorch. Using the extreme parallelization capability of GPUs, WarpDrive enables orders-of-magnitude faster RL compared to common implementations that blend CPU simulations and GPU models. Our design runs simulations and the agents in each simulation in parallel. It eliminates data copying between CPU and GPU. It also uses a single simulation data store on the GPU that is safely updated in-place. WarpDrive provides a lightweight Python interface and flexible environment wrappers that are easy to use and extend. Together, this allows the user to easily run thousands of concurrent multi-agent simulations and train on extremely large batches of experience. Through extensive experiments, we verify that WarpDrive provides high-throughput and scales almost linearly to many agents and parallel environments. For example, WarpDrive yields 2.9 million environment steps/second with 2000 environments and 1000 agents (at least 100x higher throughput compared to a CPU implementation) in a benchmark Tag simulation. As such, WarpDrive is a fast and extensible multi-agent RL platform to significantly accelerate research and development.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Multiagent Systems},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/WarpDrive_2021_Lan et al.pdf;/home/baldoinov/Zotero/storage/SXFIALL5/2108.html}
}

@online{WeightNormalizationSimple_2016_SalimansKingma,
  title = {Weight {{Normalization}}: {{A Simple Reparameterization}} to {{Accelerate Training}} of {{Deep Neural Networks}}},
  shorttitle = {Weight {{Normalization}}},
  author = {Salimans, Tim and Kingma, Diederik P.},
  date = {2016-06-03},
  eprint = {1602.07868},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1602.07868},
  url = {http://arxiv.org/abs/1602.07868},
  urldate = {2024-02-23},
  abstract = {We present weight normalization: a reparameterization of the weight vectors in a neural network that decouples the length of those weight vectors from their direction. By reparameterizing the weights in this way we improve the conditioning of the optimization problem and we speed up convergence of stochastic gradient descent. Our reparameterization is inspired by batch normalization but does not introduce any dependencies between the examples in a minibatch. This means that our method can also be applied successfully to recurrent models such as LSTMs and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited. Although our method is much simpler, it still provides much of the speed-up of full batch normalization. In addition, the computational overhead of our method is lower, permitting more optimization steps to be taken in the same amount of time. We demonstrate the usefulness of our method on applications in supervised image recognition, generative modelling, and deep reinforcement learning.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,notion},
  file = {/home/baldoinov/baldoinov/PDFs/Artigos-Cientificos/Weight Normalization_2016_Salimans et al.pdf;/home/baldoinov/Zotero/storage/HBHZFMLG/1602.html}
}

@article{whitesides_2020,
  title = {Learning from Success: {{Lessons}} in Science and Diplomacy from the {{Montreal Protocol}}},
  author = {Whitesides, G.},
  date = {2020},
  journaltitle = {Science \& Diplomacy},
  volume = {9},
  number = {2},
  pages = {1--13}
}

@online{WhyAgentBasedModeling_2022_mdmakowsky,
  title = {Why {{Agent-Based Modeling Never Happened}} in {{Economics}}},
  author = {{mdmakowsky}},
  date = {2022-03-14T14:00:00+00:00},
  url = {https://economistwritingeveryday.com/2022/03/14/why-agent-based-modeling-never-happened-in-economics/},
  urldate = {2024-06-10},
  abstract = {I had the title of this post sitting in “Drafts” for a couple months now, but Kris and Paul have given me good reason to actually write about it. These thoughts are largely off the cuff…},
  langid = {english},
  organization = {Economist Writing Every Day},
  file = {/home/baldoinov/Zotero/storage/9IRJKT6P/why-agent-based-modeling-never-happened-in-economics.html}
}

@article{WhyCivilResistance_2008_StephanChenoweth,
  title = {Why {{Civil Resistance Works}}: {{The Strategic Logic}} of {{Nonviolent Conflict}}},
  shorttitle = {Why {{Civil Resistance Works}}},
  author = {Stephan, Maria J. and Chenoweth, Erica},
  date = {2008},
  journaltitle = {International Security},
  volume = {33},
  number = {1},
  eprint = {40207100},
  eprinttype = {jstor},
  pages = {7--44},
  publisher = {The MIT Press},
  issn = {0162-2889},
  url = {https://www.jstor.org/stable/40207100},
  urldate = {2024-07-03},
  abstract = {The historical record indicates that nonviolent campaigns have been more successful than armed campaigns in achieving ultimate goals in political struggles, even when used against similar opponents and in the face of repression. Nonviolent campaigns are more likely to win legitimacy, attract widespread domestic and international support, neutralize the opponent's security forces, and compel loyalty shifts among erstwhile opponent supporters than are armed campaigns, which enjoin the active support of a relatively small number of people, offer the opponent a justification for violent counterattacks, and are less likely to prompt loyalty shifts and defections. An original, aggregate data set of all known major nonviolent and violent resistance campaigns from 1900 to 2006 is used to test these claims. These dynamics are further explored in case studies of resistance campaigns in Southeast Asia that have featured periods of both violent and nonviolent resistance.},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/avaliacao-de-politicas-sociais/Why Civil Resistance Works_2008_Stephan et al.pdf}
}

@book{WhyCivilResistance_2011_ChenowethStephan,
  title = {Why {{Civil Resistance Works}}: The Strategic Logic of Nonviolent Conflict},
  shorttitle = {Why Civil Resistance Works},
  author = {Chenoweth, Erica and Stephan, Maria J.},
  date = {2011},
  series = {Columbia Studies in Terrorism and Irregular Warfare},
  publisher = {Columbia University Press},
  location = {New York},
  isbn = {978-0-231-15682-0 978-0-231-52748-4},
  langid = {english}
}

@article{WOS:000592138500001,
  title = {A Survey on Multi-Agent Deep Reinforcement Learning: From the Perspective of Challenges and Applications},
  author = {Du, Wei and Ding, Shifei},
  date = {2021-06},
  journaltitle = {ARTIFICIAL INTELLIGENCE REVIEW},
  volume = {54},
  number = {5},
  pages = {3215--3238},
  issn = {0269-2821},
  doi = {10.1007/s10462-020-09938-y},
  abstract = {Deep reinforcement learning has proved to be a fruitful method in various tasks in the field of artificial intelligence during the last several years. Recent works have focused on deep reinforcement learning beyond single-agent scenarios, with more consideration of multi-agent settings. The main goal of this paper is to provide a detailed and systematic overview of multi-agent deep reinforcement learning methods in views of challenges and applications. Specifically, the preliminary knowledge is introduced first for a better understanding of this field. Then, a taxonomy of challenges is proposed and the corresponding structures and representative methods are introduced. Finally, some applications and interesting future opportunities for multi-agent deep reinforcement learning are given.},
  earlyaccessdate = {NOV 2020},
  eissn = {1573-7462},
  orcid-numbers = {Du, Wei/0000-0001-5954-3622},
  researcherid-numbers = {Du, Wei/AAI-1530-2019},
  unique-id = {WOS:000592138500001},
  file = {/home/baldoinov/baldoinov/PDFs/Feausp/monografia/A survey on multi-agent deep reinforcement learning_2021_Du et al.pdf}
}

@unpublished{YaRrrPiratesGuide__Phillips,
  title = {{{YaRrr}}! {{The Pirate}}'s {{Guide}} to {{R}}},
  author = {Phillips, Dr Nathaniel D},
  langid = {english},
  file = {/home/baldoinov/Zotero/storage/SIN44VLW/Phillips - YaRrr! The Pirate's Guide to R.pdf}
}

@online{YourOneStopDestination_2023_Das,
  title = {Your {{One-Stop Destination}} to {{Start}} Your {{NLP}} Journey with {{SpaCy}}},
  author = {Das, Akash},
  date = {2023-02-06T16:31:49+00:00},
  url = {https://www.analyticsvidhya.com/blog/2023/02/your-one-stop-destination-to-start-your-nlp-journey-with-spacy/},
  urldate = {2023-09-21},
  abstract = {This guide explains you all the steps which you have to perform while working on any NLP Project using SpaCy},
  langid = {english},
  organization = {Analytics Vidhya},
  keywords = {notion},
  file = {/home/baldoinov/Zotero/storage/7Q6BR6M4/your-one-stop-destination-to-start-your-nlp-journey-with-spacy.html}
}